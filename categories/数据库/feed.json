{
    "version": "https://jsonfeed.org/version/1",
    "title": "艾瑞可erik • All posts by \"数据库\" categories",
    "description": "一只PHP开发的程序猿，偶尔做做运维、Goland、Python、Java、摄影、画画、写作、顺便睡觉，反正整站都搞过。",
    "home_page_url": "https://erik.xyz",
    "items": [
        {
            "id": "https://erik.xyz/2024/11/11/mysql-redis-consistency/",
            "url": "https://erik.xyz/2024/11/11/mysql-redis-consistency/",
            "title": "如何下保证MySQL数据库与Redis缓存数据一致性？",
            "date_published": "2024-11-11T01:58:00.000Z",
            "content_html": "<p>有时候感觉MySQL我们懂了，Redis我们懂了，但是面试的时候一直答不好，经常被难住，问题在哪呢？</p>\n<p>答案是：面试官考的不是专项能力，而是多项技术结合应用能力。</p>\n<p>就拿<strong>并发场景下如何保证MySQL与Redis缓存一致性？</strong>这个面试官常见的拷打考点举例。</p>\n<p>对于读多写少并且要求高性能的业务逻辑，我们通常在应用服务器访问MySQL数据库的中间加上一层<strong>Redis缓存层</strong>，以提高数据的查询效率，减轻MySQL数据库的压力，避免在MySQL出现性能瓶颈。<br><span id=\"more\"></span><br><img src=\"/img/2024/2024111101.png\" alt=\"https://erik.xyz\"></p>\n<p>该问题，如果在数据存储后，只读场景下是不会出现MySQL与Redis缓存的一致性问题的，所以真正需要考虑的是<strong>并发读写场景</strong>下的数据一致性问题。</p>\n<p>如果我们不加分析，单独利用MySQL和Redis的知识进行回答并发场景下如何保证MySQL与Redis缓存一致性？很难把这个问题回答好，因为看起来很简单的方案实际上是漏洞百出的。</p>\n<h4 id=\"简单方案下的漏洞百出\"><a href=\"#简单方案下的漏洞百出\" class=\"headerlink\" title=\"简单方案下的漏洞百出\"></a>简单方案下的漏洞百出</h4><p>我们先看下简单的更新数据库、删除缓存和更新缓存方案下，会出现什么问题？</p>\n<p><img src=\"/img/2024/2024111102.png\" alt=\"https://erik.xyz\"></p>\n<h4 id=\"更新缓存，再更新数据库\"><a href=\"#更新缓存，再更新数据库\" class=\"headerlink\" title=\"更新缓存，再更新数据库\"></a>更新缓存，再更新数据库</h4><p>先说结论：不考虑。</p>\n<p>原因是更新缓存成功后，数据库可能更新失败，出现数据库为旧值，缓存为新值。导致后续的所有的读请求，在缓存未过期或缓存未重新正确更新的情况下，会一直保持了数据的完全不一致！并且当前数据库中的值为旧值，而业务数据的正确性应该以数据库的为准。</p>\n<p>那么如果更新缓存成功后，数据库可能更新失败，我们<strong>重新更新缓存</strong>是不是可以了？</p>\n<p><img src=\"/img/2024/2024111103.png\" alt=\"https://erik.xyz\"></p>\n<p>抛开需要重新更新缓存时，要单表或多表重新查询数据，再更新数据带来的性能问题，还可能期间有数据变更再次陷入脏数据的情况。实际上仍然还是会出现并发一致性问题。</p>\n<p>只要缓存进行了更新，后续的读请求<strong>在更新数据库前、更新数据库失败并准备更新缓存前</strong>，基本上都能命中缓存情况，而这时返回的数据都是未落库的脏数据。</p>\n<p><img src=\"/img/2024/2024111104.png\" alt=\"https://erik.xyz\"></p>\n<h4 id=\"更新数据库，再更新缓存\"><a href=\"#更新数据库，再更新缓存\" class=\"headerlink\" title=\"更新数据库，再更新缓存\"></a>更新数据库，再更新缓存</h4><p>不考虑。</p>\n<p>原因是当数据库更新成功后，缓存更新失败，出现数据库为最新值，缓存为旧值。导致后续的所有的读请求，在缓存未过期或缓存未重新正确更新的情况下，会一直保持了数据的完全不一致！</p>\n<p><img src=\"/img/2024/2024111105.png\" alt=\"https://erik.xyz\"></p>\n<p>该方案就算在更新数据库、更新缓存都成功的情况下，还是会存在并发引发的一致性问题，如下图所示（点击图片查看大图）：<br><img src=\"/img/2024/2024111106.png\" alt=\"https://erik.xyz\"></p>\n<p>可以看到在并发多写多读的场景下数据存在的不一致性问题。</p>\n<h4 id=\"先删除缓存，再更新数据库\"><a href=\"#先删除缓存，再更新数据库\" class=\"headerlink\" title=\"先删除缓存，再更新数据库\"></a>先删除缓存，再更新数据库</h4><p>不考虑，但是通过使用<strong>延时双删策略</strong>后可以考虑。</p>\n<p>采用“<strong>先删除缓存，再更新数据库</strong>”的方案是一种常见的方法来尝试解决这个问题的策略。</p>\n<p>这种方法逻辑较为简单，易于理解和实现，理论上删除旧缓存后，下次读取时将从数据库获取最新数据。</p>\n<p>但在并发的极端情况下，删除缓存成功后，如果再有大量的并发请求进来，那么便会直接请求到数据库中，对数据库造成巨大的压力。而且此方案还是可能会发生数据不一致性问题。</p>\n<p><img src=\"/img/2024/2024111107.png\" alt=\"https://erik.xyz\"></p>\n<p>通过上图发现在删除缓存后，如果有并发读请求1.1进来，那么查询缓存肯定是不存在，则去读取数据库，但因为此时更新数据库x=10的操作2.更新数据库还未完成，所以读取到的仍然是旧值x=5并设置缓存后，在2.更新数据库完成后，数据是新值10，而缓存是旧值，造成了数据不一致的问题。</p>\n<p>对此我们可以先进行一波的小优化，那就是<strong>延时双删策略</strong>。即在更新数据库之后，先延迟等待一下（等待时间参考该读请求的响应时间+几十毫秒），再继续删除缓存。这样做的目的是确保读请求结束（已经在1.2读库中读取到了旧数据，后续会在该请求中更新缓存），写请求可以删除读请求造成的缓存脏数据，保证再删除缓存之后的所有读请求都能读到最新值。</p>\n<p><img src=\"/img/2024/2024111108.png\" alt=\"https://erik.xyz\"></p>\n<p>可以看出此优化方案关键点在于等待多长时间后，再次删除缓存尤为重要，但是这个时间都是根据历史查询请求的响应时间判断的，实际情况会有浮动。这也导致如果等待的延时时间过短，则仍然会出现数据不一致的情况；等待延迟时间过长，则导致延迟期间出现数据不一致的时间变长。</p>\n<p>另外<strong>延时双删策略</strong>还需要考虑如果再次删除缓存失败的情况如何处理？</p>\n<p>因为删除失败将导致后续的所有的读请求，在缓存未过期或缓存未重新正确更新的情况下，会一直保持了数据的完全不一致！这个在下文的技术优化方案继续讨论。</p>\n<h4 id=\"先更新数据库，再删除缓存\"><a href=\"#先更新数据库，再删除缓存\" class=\"headerlink\" title=\"先更新数据库，再删除缓存\"></a>先更新数据库，再删除缓存</h4><p>比较推荐。</p>\n<p>采用的“先更新数据库，再删除缓存”策略，跟“先删除缓存，再更新数据库”中我们进行<strong>延时双删策略</strong>的小优化基本一样，仍然需要考虑删除缓存失败的情况如何处理。</p>\n<p>单纯从“先更新数据库，再删除缓存”和“先删除缓存，再更新数据库”对比起来。在大多数情况下，“先更新数据库，再删除缓存”被认为是一个更好的选择，原因如下：</p>\n<p>1.<strong>数据的一致性</strong>：这种方法更倾向于保持数据的最终一致性，即使缓存删除失败，也能保证数据的一致性不会长期受损。</p>\n<p>2.<strong>用户体验</strong>：在“先删除缓存，再更新数据库”的情况下，如果数据库更新失败，用户可能会一直看到旧数据，直到缓存过期。相比之下，“先更新数据库，再删除缓存”可以在某种程度上避免这种情况。</p>\n<p>但该方案同样也会出现数据不一致性问题，如下图所示。</p>\n<p><img src=\"/img/2024/2024111109.png\" alt=\"https://erik.xyz\"></p>\n<p>当数据库的数据被更新后，缓存也被删除。接下来的出现读请求3.1和写请求3.2同时进来。</p>\n<p>读请求先读了缓存发现缓存无命中，则查询数据库并在准备更新缓存时，3.2写请求已经完成了数据的更新和删除缓存的动作，之后3.1读请求才更新了缓存。最后导致了数据库中的值未新值，缓存中的值为旧值。</p>\n<h4 id=\"优化后方案\"><a href=\"#优化后方案\" class=\"headerlink\" title=\"优化后方案\"></a>优化后方案</h4><p>从上面的简单方案方案中，似乎没有一种方案真正能解决并发场景下MySQL数据与Redis缓存数据一致性的问题。</p>\n<p>这里有个说明下，如果业务要求必须要满足<strong>强一致性</strong>，那么不管如何优化缓存策略，都无法满足，而最好的办法是不用缓存。</p>\n<p>强一致性：它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大。</p>\n<p>解决方案是读写串行化，而此方案会大大增加系统的处理效率，吞吐量也会大大降低。</p>\n<p>另外在大型分布式系统中，其实分布式事务大多数情况都不会使用，因为维护成本太高了、复杂度也高。所以在分布式系统，我们一般都会推崇最终一致性，即这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态。</p>\n<p>现在我们接着继续优化..</p>\n<h4 id=\"延迟双删策略-重试机制\"><a href=\"#延迟双删策略-重试机制\" class=\"headerlink\" title=\"延迟双删策略+重试机制\"></a>延迟双删策略+重试机制</h4><p>从上面简单方案下的漏洞百出下的先删除缓存，再更新数据库中，我们可以看出来其实<strong>延迟双删策略</strong>，算是融合“先删除缓存，再更新数据库”和“先更新数据库，再删除缓存”的策略，可以解决大部分的数据一致性的业务逻辑处理问题。</p>\n<p>但我们前面还遗留了一个待解决的问题：如果再次<strong>删除缓存失败的情况如何处理</strong>？</p>\n<p>——-当然是补救去继续删除这个缓存Key了，而补救方法则是<strong>重试</strong>。</p>\n<p><strong>重试机制</strong>可以在当前中启动新协程（Golang中属于用户态的轻量级线程）中进行重试；也可以放到消息队列中进行重试；还可以是先启动新协程重试3次，重试失败后继续放到消息队列中重试，如下图展示的是放到消息队列中进行重试。</p>\n<p>新协程中进行重试需要注意的是使用的新上下文context.Background()，而不是当前请求的上下文。</p>\n<p>一般消息队列会支持高可靠性的队列，例如 RabbitMQ、Kafka 等。这些消息队列提供了非常强的消息传递、异步处理和持久化功能，可以有效地解决数据同步的问题。</p>\n<p><img src=\"/img/2024/2024111110.png\" alt=\"https://erik.xyz\"></p>\n<p>此方案仍然存在一些需要，如：选择合适的延迟等待时间进行删除缓存；协程中重试删除缓存次数、间隔时间；消息队列中删除失败缓存失败后是否需要重试等。</p>\n<h4 id=\"读取binlog异步删除缓存\"><a href=\"#读取binlog异步删除缓存\" class=\"headerlink\" title=\"读取binlog异步删除缓存\"></a>读取binlog异步删除缓存</h4><p>重试删除缓存机制还可以吧，就是会造成好多业务代码入侵。</p>\n<p>其实，还可以这样优化：</p>\n<p>1.通过Canal将binlog日志采集发送到MQ队列来异步淘汰key。</p>\n<p>2.删除缓存的应用程序通过ACK手动机制确认处理这条更新消息，删除缓存，保证数据缓存一致性。</p>\n<p><img src=\"/img/2024/2024111111.png\" alt=\"https://erik.xyz\"></p>\n<p>异步淘汰key相比于等新对比缓存数据并更新会简单一些，因为可能一份缓存数据涉及多张表的数据查询、聚合、排序等。</p>\n<p>尽管该方案看起来也不错了，但是因为引入额外的组件（如Canal、消息队列）复杂性增加了也不少，需要维护和监控这些组件的运行状态，保证组件运行正常。</p>\n<h4 id=\"定时任务\"><a href=\"#定时任务\" class=\"headerlink\" title=\"定时任务\"></a>定时任务</h4><p>在某些业务场景的需求下，也可以通过定时任务的方式进行 Redis 和 MySQL 的数据同步。</p>\n<p>具体做法是通过定时任务从 Redis 中读取数据，然后跟 MySQL 中的数据进行比对，如果 Redis 中数据有变化，则进行同步。</p>\n<p><img src=\"/img/2024/2024111112.png\" alt=\"https://erik.xyz\"></p>\n<p>这种方式虽然实现起来比较简单，但需要注意同步的时效性，如果时间间隔设置不当，可能会导致同步的数据丢失或者不准确。</p>\n<h4 id=\"双写一致性\"><a href=\"#双写一致性\" class=\"headerlink\" title=\"双写一致性\"></a>双写一致性</h4><p>在更新数据库的同时也更新缓存/删除缓存，即所谓的“<strong>双写</strong>”。</p>\n<p>这样可以确保在数据库更新后，缓存中的数据也是最新的，从而减少数据不一致的时间窗口。</p>\n<p><img src=\"/img/2024/2024111113.png\" alt=\"https://erik.xyz\"></p>\n<p><strong>并发控制</strong>：在高并发场景下，多个请求同时对同一个数据进行更新时，如果没有妥善处理并发控制，可能会导致数据不一致的问题。所以这里引入了分布式锁和事务操作：</p>\n<p><strong>使用分布式锁</strong>：在执行双写操作之前，获取一个分布式锁（如Zookeeper、Redis的SETNX命令等），确保同一时刻只有一个线程/进程能够执行双写操作。</p>\n<p><strong>事务处理</strong>：对于支持事务的缓存系统（如Redis的MULTI/EXEC命令）和MySQL事务，可以将Redis缓存和MySQL更新操作放入事务中，确保要么全部成功，要么全部失败。</p>\n<p>当然在“双写”的策略中，除了并发控制外，可以结合上面提到的重试、定时策略进行组合，以应对极端情况下的数据不一致性问题。</p>\n<p>另外也可以处理失败的逻辑上加入告警机制，及时通知开发和运维人员。</p>\n<p>转载自：<a href=\"https://mp.weixin.qq.com/s/sG7xDtLKLtlnu9ntpc5hdw\">皇子谈技术</a></p>\n",
            "tags": [
                "mysql",
                "redis",
                "数据一致性"
            ]
        },
        {
            "id": "https://erik.xyz/2020/03/17/nosql-db-presentation/",
            "url": "https://erik.xyz/2020/03/17/nosql-db-presentation/",
            "title": "nosql数据库的介绍",
            "date_published": "2020-03-16T17:35:00.000Z",
            "content_html": "<p>根据<a href=\"https://nosql-database.org\">NoSQL官网</a>查阅，结合网上资料做一个简单介绍。</p>\n<h4 id=\"键值数据库\"><a href=\"#键值数据库\" class=\"headerlink\" title=\"键值数据库\"></a>键值数据库</h4><ul>\n<li><p>目前使用较多的</p>\n<ul>\n<li><a href=\"https://redis.io\">redis</a>（开源）</li>\n<li><a href=\"https://memcached.org\">memcached</a>（开源）</li>\n</ul>\n</li>\n<li><p>特点：</p>\n<pre><code>  基于内存数据处理，相对速度最快；数据存储结构最简单，只有key-value形式；对值的查询统计功能支持很弱；由于基于内存数据处理，数据持久性相对较弱。但，redis具备大数据管理能力；事务处理能力弱。\n</code></pre><span id=\"more\"></span>\n<h4 id=\"文档数据库\"><a href=\"#文档数据库\" class=\"headerlink\" title=\"文档数据库\"></a>文档数据库</h4></li>\n<li><p>目前使用较多的</p>\n<ul>\n<li><a href=\"https://www.mongodb.com\">mongodb</a>（开源）</li>\n<li><a href=\"https://www.couchbase.com\">couchbase</a>（开源）</li>\n</ul>\n</li>\n<li><p>特点：</p>\n<pre><code>  mongodb基于硬盘数据处理，速度比sql数据库提高十几倍；couchbase基于内存处理；两者都具有很强的横向扩展能力；文档数据库的值具备复杂文档结构数据的处理能力，查询统计性能相对比键值数据库要强。具备大数据处理能力；无事务处理能力。\n</code></pre></li>\n</ul>\n<h4 id=\"列族数据库\"><a href=\"#列族数据库\" class=\"headerlink\" title=\"列族数据库\"></a>列族数据库</h4><ul>\n<li><p>目前使用较多的</p>\n<ul>\n<li><a href=\"https://cassandra.apache.org\">cassandra</a>（开源）</li>\n<li><a href=\"https://hbase.apache.org\">hbase</a>（开源）</li>\n</ul>\n</li>\n<li><p>特点：</p>\n<pre><code>  基于硬盘数据处理，主要用于大数据处理，写速度明显比读速度要快，整体读写速度较键值数据库、文档数据库要慢；有强大的数据查询统计功能；无事务处理能力。\n</code></pre></li>\n</ul>\n<h4 id=\"图数据库\"><a href=\"#图数据库\" class=\"headerlink\" title=\"图数据库\"></a>图数据库</h4><ul>\n<li><p>目前使用较多的</p>\n<ul>\n<li><a href=\"https://neo4j.com\">neo4j</a>（早期版本开源）</li>\n<li><a href=\"https://orientdb.com\">orientdb</a>（开源）</li>\n</ul>\n</li>\n<li><p>特点：</p>\n<pre><code>      基于硬盘的数据处理，侧重图（这里的图是数学里的“图论”，即一个包含若干个节点、节点之间存在边关系，节点和边可以附加相关属性的结合系统）数据查询计算。ACID事务\n</code></pre></li>\n</ul>\n<h4 id=\"多模式数据库\"><a href=\"#多模式数据库\" class=\"headerlink\" title=\"多模式数据库\"></a>多模式数据库</h4><ul>\n<li>目前使用较多的<ul>\n<li><a href=\"https://www.arangodb.com\">ArangoDB</a>（开源+商业）</li>\n<li><a href=\"http://virtuoso.openlinksw.com\">OpenLink Virtuoso</a>（开源+商业）</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"对象数据库\"><a href=\"#对象数据库\" class=\"headerlink\" title=\"对象数据库\"></a>对象数据库</h4><ul>\n<li>目前使用较多的<ul>\n<li><a href=\"http://db4o.com\">db4o</a>（开源）</li>\n<li><a href=\"http://www.versant.com\">versant</a>（商业）</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"网格和云数据库\"><a href=\"#网格和云数据库\" class=\"headerlink\" title=\"网格和云数据库\"></a>网格和云数据库</h4><ul>\n<li>目前使用较多的<ul>\n<li><a href=\"https://www.gridgain.com\">gridgain</a>（开源+商业）</li>\n<li><a href=\"https://crate.io\">cratedb</a>（开源） </li>\n</ul>\n</li>\n</ul>\n<h4 id=\"XML数据库\"><a href=\"#XML数据库\" class=\"headerlink\" title=\"XML数据库\"></a>XML数据库</h4><ul>\n<li>目前使用较多的<ul>\n<li><a href=\"https://www.delltechnologies.com\">EMC Documentum xDB</a>（商业）</li>\n<li><a href=\"http://exist-db.org\">eXist</a>（开源）</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"多维数据库\"><a href=\"#多维数据库\" class=\"headerlink\" title=\"多维数据库\"></a>多维数据库</h4><ul>\n<li>目前使用较多的<ul>\n<li><a href=\"http://globalsdb.org\">Globals</a>（商业）</li>\n<li><a href=\"http://www.intersystems.com\">Intersystems Cache</a>（商业） </li>\n</ul>\n</li>\n</ul>\n<h4 id=\"多值数据库\"><a href=\"#多值数据库\" class=\"headerlink\" title=\"多值数据库\"></a>多值数据库</h4><ul>\n<li>目前使用较多的<ul>\n<li><a href=\"http://www.revelation.com/index.php/features\">OpenInsight</a>（商业）</li>\n<li><a href=\"https://www.rocketsoftware.com/products/rocket-u2\">u2</a>（商业）</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"事件驱动数据库\"><a href=\"#事件驱动数据库\" class=\"headerlink\" title=\"事件驱动数据库\"></a>事件驱动数据库</h4><ul>\n<li>目前使用较多的<ul>\n<li><a href=\"https://eventstore.com\">event store</a>（开源）</li>\n<li><a href=\"https://github.com/NEventStore/NEventStore\">nevent store</a>（开源）</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"时间序列-流数据库\"><a href=\"#时间序列-流数据库\" class=\"headerlink\" title=\"时间序列/流数据库\"></a>时间序列/流数据库</h4><ul>\n<li>目前使用较多的<ul>\n<li><a href=\"http://axibase.com/products/axibase-time-series-database\">Axibase</a>（开源）</li>\n<li><a href=\"http://kx.com\">kdb+</a>（个人版开源）</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"科学、专业的数据库\"><a href=\"#科学、专业的数据库\" class=\"headerlink\" title=\"科学、专业的数据库\"></a>科学、专业的数据库</h4><ul>\n<li>目前使用较多的<ul>\n<li><a href=\"http://probcomp.csail.mit.edu/software/bayesdb\">bayesdb</a>（开源）</li>\n<li><a href=\"http://www.gpudb.com\">gpudb</a>（商业）</li>\n</ul>\n</li>\n</ul>\n",
            "tags": [
                "非关系型数据库"
            ]
        },
        {
            "id": "https://erik.xyz/2019/08/21/hbase-install/",
            "url": "https://erik.xyz/2019/08/21/hbase-install/",
            "title": "Hbase安装教程",
            "date_published": "2019-08-21T15:40:00.000Z",
            "content_html": "<p>hbase分布式数据库，很早就知道了，今天尝试在docker中搭建一下。<br>首先，安装配置hadoop环境。</p>\n<p><a href=\"https://jingyan.baidu.com/article/2d5afd69d5969585a2e28eaf.html\">hadoop安装教程</a></p>\n<p>这教程是我之前在百度经验分享的，版本是2.8，现在看了看了最新版hadoop，教程还是可以用的。</p>\n<p>安装好hadoop后别忘了配置ssh、java环境等。</p>\n<p>顺便创建用户和组为hadoop（也可以自己定义，这块不会的去预习一下linux用户和组的创建）<span id=\"more\"></span></p>\n<ul>\n<li><h4 id=\"单机配置\"><a href=\"#单机配置\" class=\"headerlink\" title=\"单机配置\"></a><a href=\"#单机配置\" title=\"单机配置\"></a>单机配置</h4><ul>\n<li>编辑hbase配置文件，在hbase目录下conf/hbase-site.xml添加</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/img/201908/2019-08-25_23-38.jpg\" alt=\"编辑配置文件\"></p>\n<p>注意哦：修改 ${user.name}为你自己的 hadoop 用户名</p>\n<ul>\n<li><p>进入/opt/hbase/conf目录下修改hbase-env.sh文件。去掉jdk指定地址注释，修改jdk配置地址<br><img src=\"/img/201908/2019-08-25_21-53.jpg\" alt=\"修改hbase-env.sh\"></p>\n<p>修改后的地址</p>\n</li>\n</ul>\n<p><img src=\"/img/201908/2019-08-25_21-54.jpg\" alt=\"修改后的地址\"></p>\n<ul>\n<li>进入hbase/bin目录执行./start-hbase.sh</li>\n</ul>\n<p><img src=\"/img/201908/2019-08-25_22-03.jpg\" alt=\"执行命令\"></p>\n<p>执行jps查看单机进程</p>\n<p>  <img src=\"/img/201908/2019-08-25_22-05.jpg\" alt=\"查看单机进程\"></p>\n<p> 单机模式已启动，进入命令界面<br> <img src=\"/img/201908/2019-08-25_22-23.jpg\" alt=\"进入命令\"></p>\n<hr>\n",
            "tags": [
                "数据库",
                "hbase安装教程",
                "hbase"
            ]
        },
        {
            "id": "https://erik.xyz/2019/04/23/mysql-fen-ku-fen-biao/",
            "url": "https://erik.xyz/2019/04/23/mysql-fen-ku-fen-biao/",
            "title": "mysql分库分表",
            "date_published": "2019-04-23T04:28:00.000Z",
            "content_html": "<p><strong>分库分表</strong></p>\n<blockquote>\n</blockquote>\n<p>php工作5年了,大部分场景都是业务层。说白了就是增删改查。也知道数据库大数据处理分库分表，一般是水平分表和垂直分表，实操是不可能的，今天看了一篇文章<a href=\"https://mp.weixin.qq.com/s/QFlUPS8X0errMwpxdBMHvg\">《分库分表？如何做到永不迁移数据和避免热点？》</a>，思路一下顺畅了。</p>\n<blockquote>\n</blockquote>\n<p> 一般来说mysql一个表大概数据量在100W查询速度跟不上了。<span id=\"more\"></span>如果每天的数据量有100W那数据这个块就要做处理了。我之前的思路是这样想的：</p>\n<blockquote>\n</blockquote>\n<p> 一个表固定分100w数据，每个表用数据id做范围分配。然后查询的时候根据id找表，但是这样有个问题，如果同一个用户数据在不同的表里，这时要查询用户的所有数据，就要连表查询，实际上分表查询是有时间和范围限制的，不存在一次性拿出所有数据的情况。（而我参加面试的时候有个面试官直接给了一个这样的问题：<br> “我这有一亿数据，一次性拿出来，用mysql如何实现。”我顿时懵逼了，没法实现，代码层可能实现，数据库查询我还真不知道。）</p>\n<blockquote>\n</blockquote>\n<p>  <strong>根据上面我看的文章介绍这是range范围方案</strong><br>   <img src=\"https://erik.xyz/wp-content/uploads/2019/2019-04-23_170042.jpg\" alt=\"\"></p>\n<p>   除了查询问题，还有一个热点问题。某个时间段数据量大增的情况，一个表就扛不住了。</p>\n<blockquote>\n</blockquote>\n<p>  <em>_文章中还介绍了hash取模法 </em>_<br>   <img src=\"https://erik.xyz/wp-content/uploads/2019/2019-04-23_171040.jpg\" alt=\"\"></p>\n<p>   hash取模法容易导致数据迁移问题，如果数据量大迁移的成本是比较高的。</p>\n<p>   <strong>根据文章介绍可以先做个范围分组，然后根据分组在用hash取模分表放数据</strong><br>   <img src=\"https://erik.xyz/wp-content/uploads/2019/2019-04-23_180531.jpg\" alt=\"\"></p>\n<p>   <em>_最终表的设计如 </em>_<br>   <img src=\"https://erik.xyz/wp-content/uploads/2019/2019-04-23_180824.jpg\" alt=\"\"></p>\n",
            "tags": [
                "mysql",
                "分库分表"
            ]
        },
        {
            "id": "https://erik.xyz/2016/03/01/mysql-ping-geng-de-ying-dui-cuo-shi/",
            "url": "https://erik.xyz/2016/03/01/mysql-ping-geng-de-ying-dui-cuo-shi/",
            "title": "mysql瓶颈的应对措施",
            "date_published": "2016-03-01T11:40:00.000Z",
            "content_html": "<p>mysql本身是存在瓶颈的，当数据量达到千万级别以上，无论mysql如何优化，其性能都显著降低（有专门团队开发并改进mysql的除外）。 那么就有以下几种办法解决。<br>1.增加mysql配置中buffer和cache的数值，增加服务器cpu数量和内存大小。<br>2.使用第三方引擎或衍生版本。例如：Percona、MariaDB、TokuDB 3.迁移到其他数据库。例如：PostgreSQL、Oracle</p>\n",
            "tags": [
                "mysql应对千万级",
                "mysql瓶颈",
                "mysql瓶颈解决办法"
            ]
        },
        {
            "id": "https://erik.xyz/2016/03/01/mysql-shu-ju-ku-ru-he-xuan-ze-cun-chu-yin-qing/",
            "url": "https://erik.xyz/2016/03/01/mysql-shu-ju-ku-ru-he-xuan-ze-cun-chu-yin-qing/",
            "title": "mysql数据库如何选择存储引擎",
            "date_published": "2016-03-01T09:50:00.000Z",
            "content_html": "<p>针对不同的业务需求来选择mysql存储引擎。 1.采用MyISAM引擎</p>\n<ul>\n<li>R/W&gt;100:1且updae相对较少。</li>\n<li>并发不高，不需要事务。</li>\n<li>表数据量小。</li>\n<li>硬件资源有限。</li>\n</ul>\n<p>2.采用InnoDB引擎<span id=\"more\"></span></p>\n<ul>\n<li>R/W比较小，频繁更新大字段。</li>\n<li>表数据量超过1000W,并发高。</li>\n<li>安全性和可用性要求高。</li>\n</ul>\n<p>3.采用Memory引擎</p>\n<ul>\n<li>有足够的内存。</li>\n<li>对数据一致性要求不高，如在线人数和Session等应用。</li>\n<li>需要定期归档的数据。</li>\n</ul>\n",
            "tags": [
                "mysql",
                "mysql存储",
                "mysql引擎",
                "mysql数据表设计选择"
            ]
        },
        {
            "id": "https://erik.xyz/2016/03/01/you-guan-mysql-you-hua-de-zhun-ze/",
            "url": "https://erik.xyz/2016/03/01/you-guan-mysql-you-hua-de-zhun-ze/",
            "title": "有关mysql优化的准则",
            "date_published": "2016-03-01T09:29:00.000Z",
            "content_html": "<p>前辈们总结的经验，学习、学习。 </p>\n<p>1.尽量避免在列上进行运算，这样会导致索引失效。 </p>\n<p>例如：SELECT <em> FROM t WHERE YEAR(d) &gt;=2011; 优化为：SELECT </em> FROM t WHERE d &gt;=’2011-01-01’;</p>\n<p>2.使用JOIN时，应该用小结果集驱动大结果集。</p>\n<p>同时把复杂的JOIN查询拆分成多个QUERY。因为JOIN多个 表时，可能导致更多的锁定和堵塞。 SELECT * FROM a JOIN b ON a.id=b.id LEFT JOIN c ON c.time=a.date LEFT JOIN d ON c.pid=d.aid LEFT JOIN e ON e.cid=a.did<br><span id=\"more\"></span></p>\n<p>3.LIKE模糊查询的使用，避免%% </p>\n<p>例如：SELECT <em> FROM t WHERE name LIKE ‘%de%’; 优化为：SELECT </em> FROM t WHERE name &gt;=’de’ AND name&lt;’df’; </p>\n<p>4.仅列出需要查询的字段，这对速度不会有明显影响，主要考虑节省内存。 </p>\n<p>5.使用批量插入语句节省交互 </p>\n<p>例如： INTO t (id,name) VALUES (1,’a’); INSERT INTO t (id,name) VALUES (2,’b’); INSERT INTO t (id,name) VALUES (3,’c’); 优化：INSERT INTO t (id,name) VALUES (1,’a’),(2,’b’),(3,’c’); </p>\n<p>6.limit的基数比较大时使用between </p>\n<p>SELECT <em> FROM article AS article ORDER BY id LIMIT 100000,10; 优化：SELECT </em> FROM article AS article WHERE id BETWEEN 100000 AND 100010 ORDER BY id; </p>\n<p>7.不要使用rand函数获取多条随机记录 </p>\n<p>SELECT <em> FROM table ORDER BY rand() LIMIT 20; 优化： SELECT </em> FROM ‘table’ AS t1 JOIN (SELECT ROUND (RAND() * ((SELECT MAX(id) FROM ‘table’)-(SELECT MIN(id) FROM ‘table’ )) + (SELECT MIN(id) FROM ‘table’ )) AS id) AS t2 WHERE t1.id&gt;=t2.id ORDER BY t1.id LIMIT 1; </p>\n<p>8.避免使用NULL </p>\n<p>9.不要使用count(id),而应该是count(*) </p>\n<p>10.不要做无谓的排序，而应尽可能在索引中完成排序。</p>\n",
            "tags": [
                "mysql",
                "mysql优化",
                "mysql读写优化",
                "数据库优化，mysql语句优化"
            ]
        },
        {
            "id": "https://erik.xyz/2015/01/09/wei-mysql-zeng-jia-http-rest-ke-hu-duan-mysql-udf-han-shu-mysql-udf-http-1-0-fa-bu/",
            "url": "https://erik.xyz/2015/01/09/wei-mysql-zeng-jia-http-rest-ke-hu-duan-mysql-udf-han-shu-mysql-udf-http-1-0-fa-bu/",
            "title": "为 MySQL 增加 HTTP/REST 客户端：MySQL UDF 函数 mysql-udf-http 1.0 发布",
            "date_published": "2015-01-09T14:38:00.000Z",
            "content_html": "<p>文章作者：张宴 Mysql-udf-http 是一款简单的MySQL用户自定义函数（UDF, User-Defined Functions），具有http_get()、http_post()、http_put()、http_delete()四个函数，可以在 MySQL数据库中利用HTTP协议进行REST相关操作。 项目网址：<a href=\"http://code.google.com/p/mysql-udf-http/\">http://code.google.com/p/mysql-udf-http/</a> 中文说明：<a href=\"http://blog.zyan.cc/mysql-udf-http/\">http://blog.zyan.cc/mysql-udf-http/</a> 使用环境：Linux操作系统，支持的MySQL版本：5.1.x 和 5.5.x。5.0.x未经测试。 软件作者：张宴<span id=\"more\"></span></p>\n<hr>\n<p><strong>一、REST架构风格：</strong> REST（Representational State Transfer）是一种轻量级的Web Service架构风格，其实现和操作明显比SOAP和XML-RPC更为简洁，可以完全通过HTTP协议实现，还可以利用缓存Cache来提高响应速 度，性能、效率和易用性上都优于SOAP协议。REST最早是由 Roy Thomas Fielding 博士2000年在论文《<a href=\"http://www.ics.uci.edu/%7Efielding/pubs/dissertation/top.htm\">Architectural Styles and the Design of Network-based Software Architectures</a>》中提出的，<a href=\"http://mysql-udf-http.googlecode.com/files/REST_cn.pdf\">中文译文全文PDF点此下载</a>。另外，<a href=\"http://www.infoq.com/cn/articles/rest-introduction\">有篇译文</a>对REST做了一个简化说明。 目前，REST架构风格的常见实现是基于HTTP协议及其四种基本方法（如POST、GET、PUT和DELETE）的。有人将HTTP协议的四种方法 与CRUD原则相对应，CRUD原则对于资源只需要四种行为：Create（创建）、Read（读取）、Update（更新）和Delete（删除）就可 以完成对其操作和处理。 </p>\n<p><a href=\"http://zyan.cc/attachment/201009/crud.png\"><img src=\"http://zyan.cc/attachment/201009/crud.png\" alt=\"点击在新窗口中浏览此图片\" title=\"点击在新窗口中浏览此图片\"></a></p>\n<p><a href=\"http://zyan.cc/attachment/201009/rest.jpg\"><img src=\"http://zyan.cc/attachment/201009/rest.jpg\" alt=\"点击在新窗口中浏览此图片\" title=\"点击在新窗口中浏览此图片\"></a> </p>\n<p>在Mysql-udf-http中，四个函数http_post()、http_get()、http_put()、http_delete()分别对应HTTP协议的POST、GET、PUT、DELETE四种方法。 REST是一种架构风格，而不是协议或标准。HTTP协议“POST、GET、PUT、DELET”四种方法与CRUD原则“Create、Read、 Update、Delete”四种行为的一一对应关系只是一种架构设计习惯，而不是规范。因此，POST方法也可以用来更新资源，PUT方法也可以用来创 建资源，这就要看具体应用程序作者的定义了。例如<a href=\"http://zyan.cc/post/362/\">Tokyo Tyrant</a>除了支持Memcached协议外，还支持REST方式存取，PUT代表创建和更新，GET代表读取，DELETE代表删除（<a href=\"http://zyan.cc/post/362/\">关于Tokyo Tyrant的安装使用请点击这儿</a>）。 目前国内外流行的Web 2.0应用API接口中，很多都支持REST架构风格。例如：<a href=\"http://open.t.sina.com.cn/wiki/index.php/API%E6%96%87%E6%A1%A3\">新浪微博开放平台</a>、<a href=\"http://wiki.dev.renren.com/wiki/API\">人人网API</a>、Google OpenID、Flickr、Twitter、eBay、Facebook、Last.fm、del.icio.us、Yahoo Search、Amazon S3、Amazon EC2、Digg、Microsoft Bing、FriendFeed、PayPal、Foursquare，<a href=\"http://www.programmableweb.com/apis/directory/1?protocol=REST\">更多…</a> 当记录数成百上千万条时，通常采用 MySQL 分表减低数据库压力。但是，全部数据按点击数、精华、积分排序显示等功能，在MySQL 分表中则无法实现。编写 Mysql-udf-http 的最初目的，是为了在项目开发中，将 MySQL 各分表的数据自动同步到我们的 <a href=\"http://zyan.cc/tcsql/\">TCSQL</a> 高速列表数据库，用来做列表查询、显示，内容页则根据ID直接查询各 MySQL 分表的内容。由于HTTP协议的通用性，通过 Mysql-udf-http 可以做更多的事情。 <strong>通过Mysql-udf-http，你可以在MySQL中利用触发器，将MySQL的数据同步到支持REST的应用上。</strong>例如你有一个独立博客，你可以在文章表创建MySQL触发器，这样，在发表文章时，就可以将文章标题、URL自动同步到新浪微博、Twitter。你想用 <a href=\"http://zyan.cc/post/362/\">Tokyo Tyrant</a> 做缓存，也可以利用MySQL触发器在发生增、删、改时，将数据自动同步到 <a href=\"http://zyan.cc/post/362/\">Tokyo Tyrant</a>。详细配置方法本文第4节中会有介绍。</p>\n<hr>\n<p><strong>二、Mysql-udf-http的安装与使用：</strong> <strong>1. 在Linux系统上安装Mysql-udf-http</strong> 注意：“/usr/local/webserver/mysql/”是你的MySQL安装路径，如果你的MySQL安装路径不同，请自行修改。</p>\n<pre><code>ulimit -SHn 65535 \n\nwget http://curl.haxx.se/download/curl-7.21.1.tar.gz\n\ntar zxvf curl-7.21.1.tar.gz cd curl-7.21.1/ \n\n./configure --prefix=/usr \n\nmake &amp;&amp; make install \n\ncd ../\n\necho &quot;/usr/local/webserver/mysql/lib/mysql/&quot; &gt; /etc/ld.so.conf.d/mysql.conf /sbin/ldconfig \n\nwget http://mysql-udf-http.googlecode.com/files/mysql-udf-http-1.0.tar.gz \n\ntar zxvf mysql-udf-http-1.0.tar.gz \n\ncd mysql-udf-http-1.0/\n\n./configure --prefix=/usr/local/webserver/mysql --with-mysql=/usr/local/webserver/mysql/bin/mysql_config \n\nmake &amp;&amp; make install cd ../\n</code></pre><hr>\n<p><strong>2. 通过命令行登陆进入MySQL</strong></p>\n<p>/usr/local/webserver/mysql/bin/mysql -S /tmp/mysql.sock</p>\n<hr>\n<p><strong>3. 创建MySQL自定义函数</strong> mysql&gt;</p>\n<p><a href=\"http://zyan.cc/#\">view plain</a><a href=\"http://zyan.cc/#\">print</a><a href=\"http://zyan.cc/#\">?</a></p>\n<pre><code>create function http_get returns string soname &#39;mysql-udf-http.so&#39;;\ncreate function http_post returns string soname &#39;mysql-udf-http.so&#39;;\ncreate function http_put returns string soname &#39;mysql-udf-http.so&#39;;\ncreate function http_delete returns string soname &#39;mysql-udf-http.so&#39;;\n</code></pre><hr>\n<p><strong>4. 使用方法</strong> <strong>I. 函数描述：</strong> mysql&gt;</p>\n<p><a href=\"http://zyan.cc/#\">view plain</a><a href=\"http://zyan.cc/#\">print</a><a href=\"http://zyan.cc/#\">?</a></p>\n<pre><code>SELECT http_get(&#39;&lt;url&gt;&#39;);\nSELECT http_post(&#39;&lt;url&gt;&#39;, &#39;&lt;data&gt;&#39;);\nSELECT http_put(&#39;&lt;url&gt;&#39;, &#39;&lt;data&gt;&#39;);\nSELECT http_delete(&#39;&lt;url&gt;&#39;);\n</code></pre><p><strong>II. 示例 A：</strong> mysql&gt;</p>\n<p><a href=\"http://zyan.cc/#\">view plain</a><a href=\"http://zyan.cc/#\">print</a><a href=\"http://zyan.cc/#\">?</a></p>\n<pre><code>/* HTTP GET、POST方式提交关键词“xoyo”到百度移动搜索 */\nSELECT http_get(&#39;http://m.baidu.com/s?word=xoyo&amp;pn=0&#39;);\nSELECT http_post(&#39;http://m.baidu.com/s&#39;,&#39;word=xoyo&amp;pn=0&#39;);\n\n/* 新浪微博开放平台：获取新浪用户ID为103500的最近一条微博内容 */\nSELECT http_get(&#39;http://api.t.sina.com.cn/statuses/user_timeline/103500.json?count=1&amp;source=1561596835&#39;) AS data;\n/* 新浪微博开放平台：发表一条微博 */\nSELECT http_post(&#39;http://your\\_sina\\_uid:your_password@api.t.sina.com.cn/statuses/update.xml?source=1561596835&#39;, &#39;status=Thins is sina weibo test information&#39;);\n\n/* Tokyo Tyrant 写入、读取、删除操作 */\nSELECT http_put(&#39;http://192.168.8.34:1978/key&#39;, &#39;This is value&#39;);\nSELECT http_get(&#39;http://192.168.8.34:1978/key&#39;);\nSELECT http_delete(&#39;http://192.168.8.34:1978/key&#39;);\n</code></pre><p><strong>III. 示例</strong> <strong>通过MySQL触发器，利用mysql-udf-http和第三方UDF函数lib_mysqludf_json，自动同步数据到 Tokyo Tyrant。</strong> <strong>(1). 下载安装 lib_mysqludf_json 修改版：</strong> 以下安装包适合32位Linux操作系统：</p>\n<pre><code>wget http://mysql-udf-http.googlecode.com/files/lib_mysqludf_json-i386.tar.gz\n\ntar zxvf lib_mysqludf_json-i386.tar.gz \ncd lib_mysqludf_json-i386/ \n\n# 如果你的MySQL安装路径不是/usr/local/webserver/mysql/，请修改以下路径。 \n\ncp -f lib_mysqludf_json.so  /usr/local/webserver/mysql/lib/mysql/plugin/lib_mysqludf_json.so\n\ncd ../\n</code></pre><p>以下安装包适合64位Linux操作系统：</p>\n<pre><code>wget http://mysql-udf-http.googlecode.com/files/lib_mysqludf_json-x86_64.tar.gz\n\ntar zxvf lib_mysqludf_json-x86_64.tar.gz cd lib_mysqludf_json-x86_64/ \n\n# 如果你的MySQL安装路径不是/usr/local/webserver/mysql/，请修改以下路径。\n\ncp -f lib_mysqludf_json.so /usr/local/webserver/mysql/lib/mysql/plugin/lib_mysqludf_json.so \n\ncd ../\n\n# 通过命令行登陆进入MySQL：\n\n/usr/local/webserver/mysql/bin/mysql -S /tmp/mysql.sock\n</code></pre><p>mysql&gt;</p>\n<p><a href=\"http://zyan.cc/#\">view plain</a><a href=\"http://zyan.cc/#\">print</a><a href=\"http://zyan.cc/#\">?</a></p>\n<pre><code>create function lib\\_mysqludf\\_json_info returns string soname &#39;lib\\_mysqludf\\_json.so&#39;;\ncreate function json_array returns string soname &#39;lib\\_mysqludf\\_json.so&#39;;\ncreate function json_members returns string soname &#39;lib\\_mysqludf\\_json.so&#39;;\ncreate function json_object returns string soname &#39;lib\\_mysqludf\\_json.so&#39;;\ncreate function json_values returns string soname &#39;lib\\_mysqludf\\_json.so&#39;;\n</code></pre><p>lib_mysqludf_json的详细用法请访问：<a href=\"http://www.mysqludf.org/lib_mysqludf_json/\">http://www.mysqludf.org/lib_mysqludf_json/</a> <strong>(2). 创建测试表</strong> mysql&gt;</p>\n<p><a href=\"http://zyan.cc/#\">view plain</a><a href=\"http://zyan.cc/#\">print</a><a href=\"http://zyan.cc/#\">?</a></p>\n<pre><code>SET NAMES UTF8;\nUSE test;\nCREATE TABLE IF NOT EXISTS `mytable` (\n  `id` int(10) NOT NULL AUTO_INCREMENT,\n  `addtime` int(10) NOT NULL,\n  `title` varchar(255) CHARACTER SET utf8 NOT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=MyISAM DEFAULT CHARSET=utf8 AUTO_INCREMENT=1;\n</code></pre><p><strong>(3). 为测试表创建触发器：</strong> mysql&gt;</p>\n<p><a href=\"http://zyan.cc/#\">view plain</a><a href=\"http://zyan.cc/#\">print</a><a href=\"http://zyan.cc/#\">?</a></p>\n<pre><code>  /* INSERT插入操作的触发器 */\n  DELIMITER |\n  DROP TRIGGER IF EXISTS mytable_insert;\n  CREATE TRIGGER mytable_insert\n  AFTER INSERT ON mytable\n  FOR EACH ROW BEGIN\n      SET @tt_json = (SELECT json_object(id,addtime,title) FROM mytable WHERE id = NEW.id LIMIT 1);\n      SET @tt_resu = (SELECT http_put(CONCAT(&#39;http://192.168.8.34:1978/&#39;, NEW.id), @tt_json));\n  END |\n  DELIMITER ;\n\n  /* UPDATE更新操作的触发器 */\n DELIMITER |\n  DROP TRIGGER IF EXISTS mytable_update;\n  CREATE TRIGGER mytable_update\n  AFTER UPDATE ON mytable\n  FOR EACH ROW BEGIN\n     SET @tt_json = (SELECT json_object(id,addtime,title) FROM mytable WHERE id = OLD.id LIMIT 1);\n      SET @tt_resu = (SELECT http_put(CONCAT(&#39;http://192.168.8.34:1978/&#39;, OLD.id), @tt_json));\n  END |\n  DELIMITER ;\n\n  /* DELETE删除操作的触发器 */\n  DELIMITER |\n  DROP TRIGGER IF EXISTS mytable_delete;\n  CREATE TRIGGER mytable_delete\n  AFTER DELETE ON mytable\n  FOR EACH ROW BEGIN\n      SET @tt_resu = (SELECT http_delete(CONCAT(&#39;http://192.168.8.34:1978/&#39;, OLD.id)));\n  END |\n  DELIMITER ;\n</code></pre><p><strong>(4). 将 MySQL 表和 Tokyo Tyrant 关联进行查询：</strong> mysql&gt;</p>\n<p><a href=\"http://zyan.cc/#\">view plain</a><a href=\"http://zyan.cc/#\">print</a><a href=\"http://zyan.cc/#\">?</a></p>\n<pre><code> SELECT id,addtime,title,http_get(CONCAT(&#39;http://192.168.8.34:1978/&#39;,id)) AS tt FROM mytable ORDER BY id DESC LIMIT 0,5;\n</code></pre><hr>\n<p><strong>5. 如何删除mysql-udf-http UDF函数：</strong> mysql&gt;</p>\n<p><a href=\"http://zyan.cc/#\">view plain</a><a href=\"http://zyan.cc/#\">print</a><a href=\"http://zyan.cc/#\">?</a></p>\n<pre><code>drop function http_get;\ndrop function http_post;\ndrop function http_put;\ndrop function http_delete;\n</code></pre><p>原文链接：<a href=\"http://blog.zyan.cc/mysql-udf-http/\">http://blog.zyan.cc/mysql-udf-http/</a>]function http_delete;</p>\n",
            "tags": [
                "数据库",
                "mysql",
                "数据表"
            ]
        }
    ]
}